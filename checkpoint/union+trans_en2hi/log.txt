=> creating model
CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
No gpu will be used
log_inv_t: 1.0
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 41286, warmup steps: 400
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 400,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": false,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 100,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 1,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
=> creating model
CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
No gpu will be used
log_inv_t: 1.0
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 41286, warmup steps: 400
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 400,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": false,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 100,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 1,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
=> creating model
CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 41286, warmup steps: 400
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 400,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 100,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
=> creating model
CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 2064, warmup steps: 206
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 206,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 5,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
=> creating model
CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 2064, warmup steps: 206
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 206,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 5,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
=> creating model
CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 2064, warmup steps: 206
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 206,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 5,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
=> creating model
CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 1651, warmup steps: 165
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 165,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 4,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
=> creating model
CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 1651, warmup steps: 165
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 165,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 4,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
Epoch: [0][  0/412]	Loss 12.23 (12.23)	InvT  20.00 ( 20.00)	Acc@1   0.78 (  0.78)	Acc@3   1.95 (  1.95)
Epoch: [0][ 20/412]	Loss 10.42 (11.6)	InvT  20.00 ( 20.00)	Acc@1   1.56 (  0.73)	Acc@3   8.20 (  4.32)
Epoch: [0][ 40/412]	Loss 9.169 (10.76)	InvT  20.00 ( 20.00)	Acc@1   2.34 (  0.90)	Acc@3  11.33 (  5.92)
Epoch: [0][ 60/412]	Loss 8.253 (10.04)	InvT  20.00 ( 20.00)	Acc@1   3.52 (  1.86)	Acc@3  18.36 (  9.20)
Epoch: [0][ 80/412]	Loss 7.49 (9.473)	InvT  19.99 ( 20.00)	Acc@1  12.11 (  3.40)	Acc@3  23.05 ( 12.64)
Epoch: [0][100/412]	Loss 7.09 (9.025)	InvT  19.99 ( 20.00)	Acc@1  10.94 (  5.05)	Acc@3  30.47 ( 15.81)
Epoch: [0][120/412]	Loss 6.775 (8.657)	InvT  19.98 ( 19.99)	Acc@1  18.75 (  6.76)	Acc@3  34.77 ( 18.90)
Epoch: [0][140/412]	Loss 6.13 (8.339)	InvT  19.98 ( 19.99)	Acc@1  17.97 (  8.25)	Acc@3  42.58 ( 21.77)
Epoch: [0][160/412]	Loss 5.956 (8.066)	InvT  19.97 ( 19.99)	Acc@1  23.83 (  9.94)	Acc@3  48.44 ( 24.43)
Epoch: [0][180/412]	Loss 5.836 (7.831)	InvT  19.97 ( 19.99)	Acc@1  31.25 ( 11.47)	Acc@3  46.09 ( 26.69)
Epoch: [0][200/412]	Loss 5.42 (7.623)	InvT  19.96 ( 19.99)	Acc@1  31.25 ( 12.99)	Acc@3  54.30 ( 28.72)
Epoch: [0][220/412]	Loss 4.959 (7.434)	InvT  19.95 ( 19.98)	Acc@1  31.25 ( 14.31)	Acc@3  53.91 ( 30.64)
Epoch: [0][240/412]	Loss 5.164 (7.261)	InvT  19.95 ( 19.98)	Acc@1  31.25 ( 15.66)	Acc@3  55.86 ( 32.44)
Epoch: [0][260/412]	Loss 5.22 (7.111)	InvT  19.94 ( 19.98)	Acc@1  31.25 ( 16.84)	Acc@3  53.12 ( 33.99)
Epoch: [0][280/412]	Loss 5.079 (6.97)	InvT  19.93 ( 19.97)	Acc@1  32.42 ( 17.93)	Acc@3  53.52 ( 35.43)
Epoch: [0][300/412]	Loss 5.267 (6.843)	InvT  19.93 ( 19.97)	Acc@1  33.98 ( 18.99)	Acc@3  53.12 ( 36.80)
Epoch: [0][320/412]	Loss 5.102 (6.729)	InvT  19.92 ( 19.97)	Acc@1  33.98 ( 19.95)	Acc@3  51.17 ( 38.07)
Epoch: [0][340/412]	Loss 4.979 (6.621)	InvT  19.91 ( 19.97)	Acc@1  32.03 ( 20.77)	Acc@3  55.86 ( 39.21)
Epoch: [0][360/412]	Loss 4.913 (6.523)	InvT  19.91 ( 19.96)	Acc@1  32.81 ( 21.54)	Acc@3  57.42 ( 40.27)
Epoch: [0][380/412]	Loss 4.934 (6.427)	InvT  19.90 ( 19.96)	Acc@1  33.59 ( 22.38)	Acc@3  58.59 ( 41.26)
Epoch: [0][400/412]	Loss 4.858 (6.344)	InvT  19.90 ( 19.96)	Acc@1  36.33 ( 23.08)	Acc@3  56.64 ( 42.15)
Learning rate: 2.5013458950201888e-05
Epoch 0, valid metric: {"Acc@1": 21.2, "Acc@3": 34.0, "loss": 3.847}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch0.mdl
Epoch: [1][  0/412]	Loss 4.005 (4.005)	InvT  19.89 ( 19.89)	Acc@1  46.09 ( 46.09)	Acc@3  70.31 ( 70.31)
Epoch: [1][ 20/412]	Loss 4.031 (4.21)	InvT  19.89 ( 19.89)	Acc@1  44.53 ( 42.69)	Acc@3  68.75 ( 65.53)
Epoch: [1][ 40/412]	Loss 4.02 (4.215)	InvT  19.89 ( 19.89)	Acc@1  48.05 ( 42.55)	Acc@3  65.62 ( 65.32)
Epoch: [1][ 60/412]	Loss 4.237 (4.199)	InvT  19.88 ( 19.89)	Acc@1  44.92 ( 42.56)	Acc@3  61.33 ( 65.20)
Epoch: [1][ 80/412]	Loss 3.932 (4.164)	InvT  19.88 ( 19.89)	Acc@1  47.66 ( 42.95)	Acc@3  69.53 ( 65.65)
Epoch: [1][100/412]	Loss 4.099 (4.139)	InvT  19.88 ( 19.88)	Acc@1  41.41 ( 43.13)	Acc@3  67.58 ( 65.93)
Epoch: [1][120/412]	Loss 4.314 (4.119)	InvT  19.87 ( 19.88)	Acc@1  40.62 ( 43.45)	Acc@3  61.33 ( 66.15)
Epoch: [1][140/412]	Loss 3.712 (4.101)	InvT  19.87 ( 19.88)	Acc@1  51.56 ( 43.65)	Acc@3  70.31 ( 66.26)
Epoch: [1][160/412]	Loss 3.785 (4.092)	InvT  19.87 ( 19.88)	Acc@1  48.83 ( 43.88)	Acc@3  69.92 ( 66.45)
Epoch: [1][180/412]	Loss 4.477 (4.086)	InvT  19.86 ( 19.88)	Acc@1  43.36 ( 43.96)	Acc@3  63.67 ( 66.51)
Epoch: [1][200/412]	Loss 3.813 (4.076)	InvT  19.86 ( 19.88)	Acc@1  48.05 ( 44.17)	Acc@3  71.48 ( 66.58)
Epoch: [1][220/412]	Loss 4.088 (4.06)	InvT  19.86 ( 19.87)	Acc@1  43.75 ( 44.39)	Acc@3  65.62 ( 66.71)
Epoch: [1][240/412]	Loss 4.238 (4.05)	InvT  19.85 ( 19.87)	Acc@1  39.45 ( 44.55)	Acc@3  64.45 ( 66.86)
Epoch: [1][260/412]	Loss 3.627 (4.045)	InvT  19.85 ( 19.87)	Acc@1  50.00 ( 44.64)	Acc@3  71.09 ( 66.90)
Epoch: [1][280/412]	Loss 4.11 (4.034)	InvT  19.85 ( 19.87)	Acc@1  45.31 ( 44.76)	Acc@3  67.58 ( 67.02)
Epoch: [1][300/412]	Loss 3.748 (4.021)	InvT  19.84 ( 19.87)	Acc@1  46.88 ( 44.98)	Acc@3  69.14 ( 67.16)
Epoch: [1][320/412]	Loss 3.678 (4.002)	InvT  19.84 ( 19.87)	Acc@1  47.27 ( 45.16)	Acc@3  69.92 ( 67.38)
Epoch: [1][340/412]	Loss 3.874 (3.992)	InvT  19.84 ( 19.86)	Acc@1  47.27 ( 45.27)	Acc@3  68.36 ( 67.51)
Epoch: [1][360/412]	Loss 3.525 (3.989)	InvT  19.84 ( 19.86)	Acc@1  49.61 ( 45.26)	Acc@3  74.22 ( 67.49)
Epoch: [1][380/412]	Loss 3.521 (3.975)	InvT  19.83 ( 19.86)	Acc@1  51.95 ( 45.40)	Acc@3  73.44 ( 67.66)
Epoch: [1][400/412]	Loss 4.144 (3.967)	InvT  19.83 ( 19.86)	Acc@1  43.36 ( 45.48)	Acc@3  66.02 ( 67.72)
Learning rate: 1.6695827725437415e-05
Epoch 1, valid metric: {"Acc@1": 26.3, "Acc@3": 41.6, "loss": 3.547}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch1.mdl
Epoch: [2][  0/412]	Loss 3.318 (3.318)	InvT  19.83 ( 19.83)	Acc@1  53.91 ( 53.91)	Acc@3  73.83 ( 73.83)
Epoch: [2][ 20/412]	Loss 2.964 (3.19)	InvT  19.83 ( 19.83)	Acc@1  52.34 ( 53.87)	Acc@3  80.08 ( 76.38)
Epoch: [2][ 40/412]	Loss 3.069 (3.194)	InvT  19.83 ( 19.83)	Acc@1  54.30 ( 54.27)	Acc@3  76.17 ( 76.48)
Epoch: [2][ 60/412]	Loss 3.199 (3.196)	InvT  19.83 ( 19.83)	Acc@1  54.30 ( 53.95)	Acc@3  73.44 ( 76.29)
Epoch: [2][ 80/412]	Loss 2.922 (3.201)	InvT  19.83 ( 19.83)	Acc@1  59.38 ( 54.02)	Acc@3  78.12 ( 76.15)
Epoch: [2][100/412]	Loss 3.254 (3.193)	InvT  19.84 ( 19.83)	Acc@1  53.12 ( 54.12)	Acc@3  71.48 ( 76.26)
Epoch: [2][120/412]	Loss 3.379 (3.195)	InvT  19.84 ( 19.83)	Acc@1  49.61 ( 54.08)	Acc@3  75.39 ( 76.27)
Epoch: [2][140/412]	Loss 3.252 (3.182)	InvT  19.84 ( 19.83)	Acc@1  52.34 ( 54.14)	Acc@3  73.83 ( 76.32)
Epoch: [2][160/412]	Loss 3.256 (3.193)	InvT  19.84 ( 19.83)	Acc@1  58.20 ( 54.13)	Acc@3  74.61 ( 76.17)
Epoch: [2][180/412]	Loss 3.203 (3.182)	InvT  19.84 ( 19.84)	Acc@1  55.47 ( 54.33)	Acc@3  75.39 ( 76.25)
Epoch: [2][200/412]	Loss 3.273 (3.18)	InvT  19.84 ( 19.84)	Acc@1  52.34 ( 54.34)	Acc@3  75.78 ( 76.30)
Epoch: [2][220/412]	Loss 3.21 (3.181)	InvT  19.84 ( 19.84)	Acc@1  52.73 ( 54.39)	Acc@3  75.00 ( 76.24)
Epoch: [2][240/412]	Loss 3.266 (3.175)	InvT  19.84 ( 19.84)	Acc@1  55.08 ( 54.49)	Acc@3  74.22 ( 76.28)
Epoch: [2][260/412]	Loss 3.184 (3.175)	InvT  19.84 ( 19.84)	Acc@1  55.08 ( 54.50)	Acc@3  76.56 ( 76.31)
Epoch: [2][280/412]	Loss 2.824 (3.17)	InvT  19.84 ( 19.84)	Acc@1  55.47 ( 54.58)	Acc@3  78.91 ( 76.32)
Epoch: [2][300/412]	Loss 3.048 (3.172)	InvT  19.85 ( 19.84)	Acc@1  53.12 ( 54.51)	Acc@3  78.12 ( 76.34)
Epoch: [2][320/412]	Loss 3.031 (3.172)	InvT  19.85 ( 19.84)	Acc@1  57.81 ( 54.56)	Acc@3  77.73 ( 76.30)
Epoch: [2][340/412]	Loss 3.583 (3.172)	InvT  19.85 ( 19.84)	Acc@1  52.34 ( 54.56)	Acc@3  72.27 ( 76.35)
Epoch: [2][360/412]	Loss 3.182 (3.165)	InvT  19.85 ( 19.84)	Acc@1  60.16 ( 54.61)	Acc@3  73.83 ( 76.41)
Epoch: [2][380/412]	Loss 3.267 (3.163)	InvT  19.85 ( 19.84)	Acc@1  55.08 ( 54.65)	Acc@3  76.17 ( 76.42)
Epoch: [2][400/412]	Loss 3.191 (3.16)	InvT  19.85 ( 19.84)	Acc@1  53.12 ( 54.71)	Acc@3  75.00 ( 76.43)
Learning rate: 8.378196500672948e-06
Epoch 2, valid metric: {"Acc@1": 27.2, "Acc@3": 43.1, "loss": 3.451}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch2.mdl
Epoch: [3][  0/412]	Loss 3.063 (3.063)	InvT  19.85 ( 19.85)	Acc@1  55.47 ( 55.47)	Acc@3  78.91 ( 78.91)
Epoch: [3][ 20/412]	Loss 2.656 (2.739)	InvT  19.85 ( 19.85)	Acc@1  62.11 ( 59.75)	Acc@3  79.69 ( 81.25)
Epoch: [3][ 40/412]	Loss 2.86 (2.72)	InvT  19.85 ( 19.85)	Acc@1  57.42 ( 59.83)	Acc@3  82.03 ( 81.15)
Epoch: [3][ 60/412]	Loss 2.703 (2.735)	InvT  19.86 ( 19.85)	Acc@1  59.77 ( 59.74)	Acc@3  83.20 ( 81.06)
Epoch: [3][ 80/412]	Loss 2.589 (2.73)	InvT  19.86 ( 19.85)	Acc@1  56.64 ( 59.76)	Acc@3  82.81 ( 81.17)
Epoch: [3][100/412]	Loss 2.712 (2.723)	InvT  19.86 ( 19.85)	Acc@1  63.28 ( 59.76)	Acc@3  80.08 ( 81.20)
Epoch: [3][120/412]	Loss 3.017 (2.725)	InvT  19.86 ( 19.85)	Acc@1  55.47 ( 59.80)	Acc@3  77.73 ( 81.13)
Epoch: [3][140/412]	Loss 3.197 (2.738)	InvT  19.86 ( 19.86)	Acc@1  55.47 ( 59.70)	Acc@3  75.78 ( 80.95)
Epoch: [3][160/412]	Loss 2.33 (2.734)	InvT  19.86 ( 19.86)	Acc@1  64.84 ( 59.76)	Acc@3  84.77 ( 81.02)
Epoch: [3][180/412]	Loss 2.569 (2.729)	InvT  19.87 ( 19.86)	Acc@1  57.42 ( 59.85)	Acc@3  84.38 ( 81.11)
Epoch: [3][200/412]	Loss 2.721 (2.732)	InvT  19.87 ( 19.86)	Acc@1  59.38 ( 59.81)	Acc@3  82.03 ( 81.06)
Epoch: [3][220/412]	Loss 2.947 (2.733)	InvT  19.87 ( 19.86)	Acc@1  55.08 ( 59.83)	Acc@3  78.12 ( 81.06)
Epoch: [3][240/412]	Loss 3.072 (2.731)	InvT  19.87 ( 19.86)	Acc@1  53.52 ( 59.91)	Acc@3  76.95 ( 81.04)
Epoch: [3][260/412]	Loss 2.803 (2.724)	InvT  19.87 ( 19.86)	Acc@1  56.64 ( 60.01)	Acc@3  80.47 ( 81.05)
Epoch: [3][280/412]	Loss 2.712 (2.723)	InvT  19.87 ( 19.86)	Acc@1  60.16 ( 60.03)	Acc@3  80.08 ( 81.06)
Epoch: [3][300/412]	Loss 2.761 (2.721)	InvT  19.87 ( 19.86)	Acc@1  57.03 ( 60.02)	Acc@3  82.03 ( 81.09)
Epoch: [3][320/412]	Loss 2.232 (2.72)	InvT  19.87 ( 19.86)	Acc@1  64.06 ( 60.03)	Acc@3  84.38 ( 81.08)
Epoch: [3][340/412]	Loss 2.506 (2.72)	InvT  19.87 ( 19.86)	Acc@1  66.02 ( 60.01)	Acc@3  86.33 ( 81.09)
Epoch: [3][360/412]	Loss 2.468 (2.717)	InvT  19.87 ( 19.86)	Acc@1  63.67 ( 60.09)	Acc@3  83.98 ( 81.13)
Epoch: [3][380/412]	Loss 2.627 (2.716)	InvT  19.87 ( 19.86)	Acc@1  64.06 ( 60.15)	Acc@3  80.47 ( 81.14)
Epoch: [3][400/412]	Loss 2.593 (2.711)	InvT  19.87 ( 19.86)	Acc@1  62.50 ( 60.23)	Acc@3  81.64 ( 81.20)
Learning rate: 6.056527590847914e-08
Epoch 3, valid metric: {"Acc@1": 27.0, "Acc@3": 44.0, "loss": 3.421}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch3.mdl
=> creating model
=> creating model
=> creating model
=> creating model
CustomBertModel(
  (dropout): Dropout(p=0.5, inplace=False)
  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
layer_norm.weight: 768
layer_norm.bias: 768
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 1651, warmup steps: 165
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 165,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.5,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 4,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
Epoch: [0][  0/412]	Loss 3.731e+03 (3.731e+03)	InvT  20.00 ( 20.00)	Acc@1   1.17 (  1.17)	Acc@3   3.52 (  3.52)
Epoch: [0][ 20/412]	Loss 3.05e+03 (3.786e+03)	InvT  20.00 ( 20.00)	Acc@1   3.52 (  1.17)	Acc@3   6.64 (  4.30)
Epoch: [0][ 40/412]	Loss 2.264e+03 (3.248e+03)	InvT  20.00 ( 20.00)	Acc@1   0.39 (  1.34)	Acc@3   3.91 (  4.61)
Epoch: [0][ 60/412]	Loss 1.614e+03 (2.797e+03)	InvT  19.99 ( 20.00)	Acc@1   1.95 (  1.67)	Acc@3   3.12 (  5.04)
Epoch: [0][ 80/412]	Loss 910.7 (2.401e+03)	InvT  19.99 ( 20.00)	Acc@1   1.56 (  1.68)	Acc@3   5.86 (  4.88)
Epoch: [0][100/412]	Loss 503.3 (2.06e+03)	InvT  19.98 ( 19.99)	Acc@1   0.78 (  1.52)	Acc@3   1.95 (  4.38)
Epoch: [0][120/412]	Loss 392.9 (1.793e+03)	InvT  19.98 ( 19.99)	Acc@1   0.00 (  1.35)	Acc@3   1.95 (  3.95)
Epoch: [0][140/412]	Loss 331.1 (1.59e+03)	InvT  19.97 ( 19.99)	Acc@1   1.95 (  1.21)	Acc@3   2.73 (  3.61)
Epoch: [0][160/412]	Loss 298.8 (1.43e+03)	InvT  19.97 ( 19.99)	Acc@1   0.39 (  1.12)	Acc@3   1.56 (  3.36)
Epoch: [0][180/412]	Loss 318.2 (1.304e+03)	InvT  19.96 ( 19.99)	Acc@1   0.00 (  1.04)	Acc@3   0.00 (  3.11)
Epoch: [0][200/412]	Loss 224.4 (1.199e+03)	InvT  19.96 ( 19.98)	Acc@1   0.39 (  0.99)	Acc@3   1.95 (  2.94)
Epoch: [0][220/412]	Loss 220.8 (1.111e+03)	InvT  19.95 ( 19.98)	Acc@1   0.00 (  0.94)	Acc@3   0.39 (  2.77)
Epoch: [0][240/412]	Loss 219.8 (1.036e+03)	InvT  19.94 ( 19.98)	Acc@1   0.39 (  0.89)	Acc@3   0.78 (  2.65)
Epoch: [0][260/412]	Loss 186.6 (971.9)	InvT  19.94 ( 19.97)	Acc@1   0.39 (  0.88)	Acc@3   1.56 (  2.59)
Epoch: [0][280/412]	Loss 180.8 (916.6)	InvT  19.93 ( 19.97)	Acc@1   1.17 (  0.84)	Acc@3   3.12 (  2.51)
Epoch: [0][300/412]	Loss 167.0 (867.5)	InvT  19.92 ( 19.97)	Acc@1   0.00 (  0.82)	Acc@3   0.00 (  2.43)
Epoch: [0][320/412]	Loss 169.0 (824.0)	InvT  19.92 ( 19.97)	Acc@1   1.17 (  0.80)	Acc@3   2.73 (  2.38)
Epoch: [0][340/412]	Loss 160.6 (785.0)	InvT  19.91 ( 19.96)	Acc@1   0.00 (  0.79)	Acc@3   1.95 (  2.35)
Epoch: [0][360/412]	Loss 156.7 (749.9)	InvT  19.90 ( 19.96)	Acc@1   0.00 (  0.78)	Acc@3   0.39 (  2.31)
Epoch: [0][380/412]	Loss 141.9 (718.1)	InvT  19.90 ( 19.96)	Acc@1   0.78 (  0.76)	Acc@3   2.73 (  2.28)
Epoch: [0][400/412]	Loss 138.9 (689.3)	InvT  19.89 ( 19.95)	Acc@1   0.39 (  0.76)	Acc@3   1.95 (  2.26)
Learning rate: 2.5013458950201888e-05
Epoch 0, valid metric: {"Acc@1": 0.2, "Acc@3": 0.6, "loss": 26.039}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch0.mdl
Epoch: [1][  0/412]	Loss 138.9 (138.9)	InvT  19.89 ( 19.89)	Acc@1   0.00 (  0.00)	Acc@3   3.12 (  3.12)
Epoch: [1][ 20/412]	Loss 130.5 (133.4)	InvT  19.88 ( 19.88)	Acc@1   1.17 (  0.45)	Acc@3   3.91 (  2.10)
Epoch: [1][ 40/412]	Loss 122.7 (131.8)	InvT  19.87 ( 19.88)	Acc@1   1.17 (  0.55)	Acc@3   1.95 (  1.99)
Epoch: [1][ 60/412]	Loss 121.7 (129.8)	InvT  19.87 ( 19.88)	Acc@1   1.17 (  0.58)	Acc@3   2.34 (  1.85)
Epoch: [1][ 80/412]	Loss 119.0 (127.8)	InvT  19.86 ( 19.87)	Acc@1   0.78 (  0.62)	Acc@3   2.34 (  1.93)
Epoch: [1][100/412]	Loss 117.9 (125.7)	InvT  19.85 ( 19.87)	Acc@1   0.39 (  0.67)	Acc@3   2.34 (  2.00)
Epoch: [1][120/412]	Loss 108.8 (123.7)	InvT  19.84 ( 19.87)	Acc@1   1.17 (  0.69)	Acc@3   2.73 (  1.99)
Epoch: [1][140/412]	Loss 111.0 (122.3)	InvT  19.84 ( 19.86)	Acc@1   1.56 (  0.67)	Acc@3   2.73 (  1.98)
Epoch: [1][160/412]	Loss 106.0 (120.7)	InvT  19.83 ( 19.86)	Acc@1   0.39 (  0.65)	Acc@3   1.56 (  1.95)
Epoch: [1][180/412]	Loss 106.9 (119.2)	InvT  19.83 ( 19.86)	Acc@1   0.78 (  0.65)	Acc@3   1.56 (  1.96)
Epoch: [1][200/412]	Loss 106.8 (118.0)	InvT  19.82 ( 19.85)	Acc@1   0.00 (  0.64)	Acc@3   1.17 (  1.95)
Epoch: [1][220/412]	Loss 102.0 (116.6)	InvT  19.81 ( 19.85)	Acc@1   1.56 (  0.66)	Acc@3   2.34 (  1.98)
Epoch: [1][240/412]	Loss 101.0 (115.5)	InvT  19.81 ( 19.85)	Acc@1   0.39 (  0.65)	Acc@3   2.34 (  2.01)
Epoch: [1][260/412]	Loss 97.83 (114.2)	InvT  19.80 ( 19.84)	Acc@1   0.39 (  0.67)	Acc@3   2.34 (  2.02)
Epoch: [1][280/412]	Loss 100.3 (113.1)	InvT  19.79 ( 19.84)	Acc@1   1.56 (  0.68)	Acc@3   2.73 (  2.01)
Epoch: [1][300/412]	Loss 92.96 (111.9)	InvT  19.79 ( 19.84)	Acc@1   1.17 (  0.69)	Acc@3   1.95 (  2.02)
Epoch: [1][320/412]	Loss 91.53 (110.8)	InvT  19.78 ( 19.83)	Acc@1   1.17 (  0.69)	Acc@3   1.95 (  2.03)
Epoch: [1][340/412]	Loss 88.64 (109.8)	InvT  19.78 ( 19.83)	Acc@1   0.39 (  0.69)	Acc@3   2.34 (  2.03)
Epoch: [1][360/412]	Loss 92.1 (108.8)	InvT  19.77 ( 19.83)	Acc@1   0.39 (  0.68)	Acc@3   1.56 (  2.01)
Epoch: [1][380/412]	Loss 90.23 (107.8)	InvT  19.76 ( 19.82)	Acc@1   1.17 (  0.68)	Acc@3   1.95 (  2.01)
Epoch: [1][400/412]	Loss 87.44 (107.0)	InvT  19.76 ( 19.82)	Acc@1   0.78 (  0.69)	Acc@3   1.56 (  2.02)
Learning rate: 1.6695827725437415e-05
Epoch 1, valid metric: {"Acc@1": 0.2, "Acc@3": 0.6, "loss": 15.338}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch1.mdl
Epoch: [2][  0/412]	Loss 92.04 (92.04)	InvT  19.76 ( 19.76)	Acc@1   0.39 (  0.39)	Acc@3   1.56 (  1.56)
Epoch: [2][ 20/412]	Loss 90.32 (88.32)	InvT  19.75 ( 19.75)	Acc@1   0.39 (  0.58)	Acc@3   1.95 (  1.90)
Epoch: [2][ 40/412]	Loss 88.88 (87.85)	InvT  19.75 ( 19.75)	Acc@1   0.78 (  0.61)	Acc@3   1.56 (  2.00)
Epoch: [2][ 60/412]	Loss 82.95 (87.38)	InvT  19.74 ( 19.75)	Acc@1   0.39 (  0.69)	Acc@3   1.17 (  2.02)
Epoch: [2][ 80/412]	Loss 86.36 (86.89)	InvT  19.74 ( 19.75)	Acc@1   0.39 (  0.65)	Acc@3   1.56 (  1.99)
Epoch: [2][100/412]	Loss 84.45 (86.5)	InvT  19.73 ( 19.74)	Acc@1   1.17 (  0.66)	Acc@3   2.34 (  1.99)
Epoch: [2][120/412]	Loss 80.23 (86.23)	InvT  19.73 ( 19.74)	Acc@1   1.17 (  0.73)	Acc@3   1.56 (  2.10)
Epoch: [2][140/412]	Loss 84.41 (85.75)	InvT  19.72 ( 19.74)	Acc@1   0.78 (  0.71)	Acc@3   3.52 (  2.09)
Epoch: [2][160/412]	Loss 80.41 (85.25)	InvT  19.72 ( 19.74)	Acc@1   1.17 (  0.72)	Acc@3   1.56 (  2.08)
Epoch: [2][180/412]	Loss 83.07 (84.68)	InvT  19.71 ( 19.73)	Acc@1   1.17 (  0.73)	Acc@3   1.56 (  2.08)
Epoch: [2][200/412]	Loss 83.97 (84.32)	InvT  19.71 ( 19.73)	Acc@1   0.78 (  0.72)	Acc@3   3.12 (  2.06)
Epoch: [2][220/412]	Loss 76.14 (83.94)	InvT  19.70 ( 19.73)	Acc@1   1.56 (  0.72)	Acc@3   4.30 (  2.09)
Epoch: [2][240/412]	Loss 80.97 (83.55)	InvT  19.70 ( 19.73)	Acc@1   0.78 (  0.72)	Acc@3   1.56 (  2.07)
Epoch: [2][260/412]	Loss 76.48 (83.13)	InvT  19.69 ( 19.72)	Acc@1   0.00 (  0.71)	Acc@3   2.73 (  2.07)
Epoch: [2][280/412]	Loss 76.9 (82.7)	InvT  19.69 ( 19.72)	Acc@1   0.00 (  0.71)	Acc@3   1.56 (  2.04)
Epoch: [2][300/412]	Loss 73.99 (82.26)	InvT  19.69 ( 19.72)	Acc@1   1.95 (  0.70)	Acc@3   3.52 (  2.06)
Epoch: [2][320/412]	Loss 81.33 (81.9)	InvT  19.68 ( 19.72)	Acc@1   0.39 (  0.70)	Acc@3   1.17 (  2.04)
Epoch: [2][340/412]	Loss 72.4 (81.49)	InvT  19.68 ( 19.72)	Acc@1   1.17 (  0.71)	Acc@3   1.95 (  2.03)
Epoch: [2][360/412]	Loss 78.55 (81.12)	InvT  19.68 ( 19.71)	Acc@1   0.39 (  0.71)	Acc@3   1.95 (  2.04)
Epoch: [2][380/412]	Loss 73.89 (80.73)	InvT  19.67 ( 19.71)	Acc@1   0.78 (  0.71)	Acc@3   2.73 (  2.05)
Epoch: [2][400/412]	Loss 70.2 (80.35)	InvT  19.67 ( 19.71)	Acc@1   0.39 (  0.70)	Acc@3   1.17 (  2.03)
Learning rate: 8.378196500672948e-06
Epoch 2, valid metric: {"Acc@1": 0.2, "Acc@3": 0.6, "loss": 12.955}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch2.mdl
Epoch: [3][  0/412]	Loss 73.4 (73.4)	InvT  19.67 ( 19.67)	Acc@1   0.39 (  0.39)	Acc@3   2.73 (  2.73)
Epoch: [3][ 20/412]	Loss 74.16 (73.33)	InvT  19.67 ( 19.67)	Acc@1   0.39 (  0.71)	Acc@3   1.56 (  2.19)
Epoch: [3][ 40/412]	Loss 66.22 (72.47)	InvT  19.66 ( 19.67)	Acc@1   0.78 (  0.78)	Acc@3   1.95 (  2.24)
Epoch: [3][ 60/412]	Loss 73.6 (72.44)	InvT  19.66 ( 19.66)	Acc@1   0.78 (  0.72)	Acc@3   1.56 (  2.12)
Epoch: [3][ 80/412]	Loss 66.82 (71.74)	InvT  19.66 ( 19.66)	Acc@1   0.78 (  0.72)	Acc@3   2.73 (  2.20)
Epoch: [3][100/412]	Loss 72.86 (71.74)	InvT  19.66 ( 19.66)	Acc@1   0.78 (  0.71)	Acc@3   2.73 (  2.16)
Epoch: [3][120/412]	Loss 68.73 (71.65)	InvT  19.65 ( 19.66)	Acc@1   0.39 (  0.70)	Acc@3   0.78 (  2.14)
Epoch: [3][140/412]	Loss 68.43 (71.5)	InvT  19.65 ( 19.66)	Acc@1   0.39 (  0.68)	Acc@3   2.73 (  2.12)
Epoch: [3][160/412]	Loss 65.77 (71.23)	InvT  19.65 ( 19.66)	Acc@1   0.78 (  0.68)	Acc@3   2.73 (  2.12)
Epoch: [3][180/412]	Loss 70.76 (71.08)	InvT  19.65 ( 19.66)	Acc@1   0.39 (  0.67)	Acc@3   1.56 (  2.11)
Epoch: [3][200/412]	Loss 65.76 (70.81)	InvT  19.65 ( 19.66)	Acc@1   1.17 (  0.66)	Acc@3   1.95 (  2.09)
Epoch: [3][220/412]	Loss 68.01 (70.54)	InvT  19.64 ( 19.65)	Acc@1   0.78 (  0.66)	Acc@3   1.17 (  2.08)
Epoch: [3][240/412]	Loss 70.75 (70.34)	InvT  19.64 ( 19.65)	Acc@1   0.39 (  0.66)	Acc@3   1.56 (  2.06)
Epoch: [3][260/412]	Loss 65.52 (70.1)	InvT  19.64 ( 19.65)	Acc@1   0.39 (  0.66)	Acc@3   0.78 (  2.08)
Epoch: [3][280/412]	Loss 70.01 (69.93)	InvT  19.64 ( 19.65)	Acc@1   0.00 (  0.65)	Acc@3   1.56 (  2.07)
Epoch: [3][300/412]	Loss 69.5 (69.73)	InvT  19.64 ( 19.65)	Acc@1   1.56 (  0.65)	Acc@3   3.52 (  2.07)
Epoch: [3][320/412]	Loss 64.97 (69.53)	InvT  19.64 ( 19.65)	Acc@1   0.78 (  0.65)	Acc@3   2.34 (  2.06)
Epoch: [3][340/412]	Loss 63.04 (69.28)	InvT  19.64 ( 19.65)	Acc@1   0.39 (  0.67)	Acc@3   2.73 (  2.09)
Epoch: [3][360/412]	Loss 70.45 (69.11)	InvT  19.64 ( 19.65)	Acc@1   0.00 (  0.67)	Acc@3   0.39 (  2.10)
Epoch: [3][380/412]	Loss 64.27 (68.95)	InvT  19.64 ( 19.65)	Acc@1   1.17 (  0.67)	Acc@3   3.12 (  2.10)
Epoch: [3][400/412]	Loss 62.32 (68.74)	InvT  19.64 ( 19.65)	Acc@1   0.78 (  0.67)	Acc@3   1.95 (  2.10)
Learning rate: 6.056527590847914e-08
Epoch 3, valid metric: {"Acc@1": 0.2, "Acc@3": 0.6, "loss": 13.245}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch3.mdl
=> creating model
CustomBertModel(
  (dropout): Dropout(p=0.5, inplace=False)
  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
layer_norm.weight: 768
layer_norm.bias: 768
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 4128, warmup steps: 400
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 400,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.5,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 10,
    "epochs": 10,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
Epoch: [0][  0/412]	Loss 3.731e+03 (3.731e+03)	InvT  20.00 ( 20.00)	Acc@1   1.17 (  1.17)	Acc@3   3.52 (  3.52)
Epoch: [0][ 20/412]	Loss 3.45e+03 (3.836e+03)	InvT  20.00 ( 20.00)	Acc@1   3.12 (  1.10)	Acc@3   6.64 (  4.20)
Epoch: [0][ 40/412]	Loss 2.799e+03 (3.515e+03)	InvT  20.00 ( 20.00)	Acc@1   1.17 (  1.27)	Acc@3   3.52 (  4.54)
Epoch: [0][ 60/412]	Loss 2.235e+03 (3.161e+03)	InvT  20.00 ( 20.00)	Acc@1   1.17 (  1.50)	Acc@3   3.12 (  4.91)
Epoch: [0][ 80/412]	Loss 1.875e+03 (2.854e+03)	InvT  20.00 ( 20.00)	Acc@1   2.34 (  1.69)	Acc@3   8.20 (  5.15)
Epoch: [0][100/412]	Loss 1.188e+03 (2.599e+03)	InvT  19.99 ( 20.00)	Acc@1   1.56 (  1.71)	Acc@3   4.30 (  5.10)
Epoch: [0][120/412]	Loss 794.6 (2.366e+03)	InvT  19.99 ( 20.00)	Acc@1   0.78 (  1.66)	Acc@3   4.69 (  4.88)
Epoch: [0][140/412]	Loss 604.9 (2.14e+03)	InvT  19.99 ( 20.00)	Acc@1   1.17 (  1.56)	Acc@3   3.52 (  4.63)
Epoch: [0][160/412]	Loss 454.5 (1.941e+03)	InvT  19.99 ( 19.99)	Acc@1   0.39 (  1.47)	Acc@3   1.17 (  4.33)
Epoch: [0][180/412]	Loss 406.7 (1.773e+03)	InvT  19.98 ( 19.99)	Acc@1   0.78 (  1.36)	Acc@3   1.56 (  3.99)
Epoch: [0][200/412]	Loss 345.4 (1.632e+03)	InvT  19.98 ( 19.99)	Acc@1   0.00 (  1.27)	Acc@3   0.78 (  3.76)
Epoch: [0][220/412]	Loss 314.5 (1.514e+03)	InvT  19.98 ( 19.99)	Acc@1   0.00 (  1.19)	Acc@3   1.17 (  3.52)
Epoch: [0][240/412]	Loss 292.7 (1.412e+03)	InvT  19.97 ( 19.99)	Acc@1   0.39 (  1.12)	Acc@3   0.39 (  3.34)
Epoch: [0][260/412]	Loss 280.2 (1.327e+03)	InvT  19.97 ( 19.99)	Acc@1   0.39 (  1.07)	Acc@3   1.95 (  3.17)
Epoch: [0][280/412]	Loss 244.5 (1.251e+03)	InvT  19.97 ( 19.99)	Acc@1   1.17 (  1.03)	Acc@3   2.73 (  3.06)
Epoch: [0][300/412]	Loss 246.0 (1.184e+03)	InvT  19.96 ( 19.99)	Acc@1   0.00 (  1.00)	Acc@3   0.78 (  2.95)
Epoch: [0][320/412]	Loss 227.2 (1.124e+03)	InvT  19.96 ( 19.98)	Acc@1   0.78 (  0.98)	Acc@3   2.73 (  2.88)
Epoch: [0][340/412]	Loss 211.3 (1.071e+03)	InvT  19.95 ( 19.98)	Acc@1   0.39 (  0.93)	Acc@3   1.56 (  2.79)
Epoch: [0][360/412]	Loss 209.8 (1.023e+03)	InvT  19.95 ( 19.98)	Acc@1   0.78 (  0.92)	Acc@3   0.78 (  2.71)
Epoch: [0][380/412]	Loss 179.0 (979.4)	InvT  19.94 ( 19.98)	Acc@1   0.00 (  0.90)	Acc@3   1.17 (  2.67)
Epoch: [0][400/412]	Loss 176.1 (939.8)	InvT  19.93 ( 19.98)	Acc@1   0.39 (  0.89)	Acc@3   0.39 (  2.60)
Learning rate: 2.990343347639485e-05
Epoch 0, valid metric: {"Acc@1": 0.2, "Acc@3": 0.6, "loss": 29.418}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch0.mdl
Epoch: [1][  0/412]	Loss 188.8 (188.8)	InvT  19.93 ( 19.93)	Acc@1   1.56 (  1.56)	Acc@3   2.34 (  2.34)
Epoch: [1][ 20/412]	Loss 168.5 (173.0)	InvT  19.92 ( 19.93)	Acc@1   0.78 (  0.67)	Acc@3   1.56 (  1.90)
Epoch: [1][ 40/412]	Loss 158.1 (169.0)	InvT  19.92 ( 19.92)	Acc@1   0.00 (  0.58)	Acc@3   1.95 (  1.79)
Epoch: [1][ 60/412]	Loss 151.8 (164.8)	InvT  19.91 ( 19.92)	Acc@1   0.39 (  0.59)	Acc@3   1.56 (  1.81)
Epoch: [1][ 80/412]	Loss 147.3 (161.2)	InvT  19.90 ( 19.92)	Acc@1   0.00 (  0.57)	Acc@3   0.78 (  1.76)
Epoch: [1][100/412]	Loss 133.9 (157.3)	InvT  19.89 ( 19.91)	Acc@1   0.39 (  0.59)	Acc@3   2.73 (  1.79)
Epoch: [1][120/412]	Loss 129.1 (153.7)	InvT  19.89 ( 19.91)	Acc@1   1.17 (  0.58)	Acc@3   2.73 (  1.78)
Epoch: [1][140/412]	Loss 136.1 (150.9)	InvT  19.88 ( 19.90)	Acc@1   0.00 (  0.58)	Acc@3   2.34 (  1.79)
Epoch: [1][160/412]	Loss 121.0 (148.2)	InvT  19.87 ( 19.90)	Acc@1   1.17 (  0.59)	Acc@3   1.95 (  1.78)
Epoch: [1][180/412]	Loss 122.4 (145.5)	InvT  19.86 ( 19.90)	Acc@1   0.00 (  0.58)	Acc@3   0.78 (  1.79)
Epoch: [1][200/412]	Loss 122.9 (143.0)	InvT  19.85 ( 19.89)	Acc@1   0.39 (  0.58)	Acc@3   1.17 (  1.80)
Epoch: [1][220/412]	Loss 116.3 (140.7)	InvT  19.85 ( 19.89)	Acc@1   0.00 (  0.61)	Acc@3   0.78 (  1.82)
Epoch: [1][240/412]	Loss 116.4 (138.6)	InvT  19.84 ( 19.88)	Acc@1   1.56 (  0.61)	Acc@3   4.69 (  1.85)
Epoch: [1][260/412]	Loss 111.3 (136.5)	InvT  19.83 ( 19.88)	Acc@1   0.39 (  0.62)	Acc@3   1.56 (  1.84)
Epoch: [1][280/412]	Loss 105.1 (134.6)	InvT  19.82 ( 19.88)	Acc@1   0.78 (  0.63)	Acc@3   2.73 (  1.87)
Epoch: [1][300/412]	Loss 108.3 (132.7)	InvT  19.81 ( 19.87)	Acc@1   0.00 (  0.63)	Acc@3   1.17 (  1.89)
Epoch: [1][320/412]	Loss 106.1 (131.1)	InvT  19.81 ( 19.87)	Acc@1   0.78 (  0.62)	Acc@3   2.34 (  1.87)
Epoch: [1][340/412]	Loss 97.44 (129.4)	InvT  19.80 ( 19.87)	Acc@1   0.78 (  0.63)	Acc@3   1.95 (  1.88)
Epoch: [1][360/412]	Loss 101.4 (127.9)	InvT  19.79 ( 19.86)	Acc@1   0.78 (  0.63)	Acc@3   2.34 (  1.89)
Epoch: [1][380/412]	Loss 96.01 (126.2)	InvT  19.78 ( 19.86)	Acc@1   0.78 (  0.63)	Acc@3   1.95 (  1.89)
Epoch: [1][400/412]	Loss 96.23 (124.8)	InvT  19.77 ( 19.85)	Acc@1   0.39 (  0.62)	Acc@3   1.17 (  1.90)
Learning rate: 2.6587982832618026e-05
Epoch 1, valid metric: {"Acc@1": 0.2, "Acc@3": 0.6, "loss": 15.352}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch1.mdl
Epoch: [2][  0/412]	Loss 95.63 (95.63)	InvT  19.77 ( 19.77)	Acc@1   0.39 (  0.39)	Acc@3   2.34 (  2.34)
Epoch: [2][ 20/412]	Loss 92.98 (94.34)	InvT  19.76 ( 19.77)	Acc@1   0.39 (  0.73)	Acc@3   1.95 (  2.16)
Epoch: [2][ 40/412]	Loss 92.76 (93.87)	InvT  19.75 ( 19.76)	Acc@1   1.56 (  0.73)	Acc@3   3.52 (  2.13)
Epoch: [2][ 60/412]	Loss 85.92 (93.13)	InvT  19.74 ( 19.76)	Acc@1   0.00 (  0.77)	Acc@3   0.78 (  2.18)
Epoch: [2][ 80/412]	Loss 91.54 (92.28)	InvT  19.74 ( 19.75)	Acc@1   1.17 (  0.71)	Acc@3   1.95 (  2.08)
Epoch: [2][100/412]	Loss 89.53 (91.94)	InvT  19.73 ( 19.75)	Acc@1   1.56 (  0.68)	Acc@3   3.12 (  2.11)
Epoch: [2][120/412]	Loss 79.85 (91.28)	InvT  19.72 ( 19.74)	Acc@1   0.78 (  0.69)	Acc@3   1.95 (  2.13)
Epoch: [2][140/412]	Loss 87.81 (90.57)	InvT  19.71 ( 19.74)	Acc@1   0.39 (  0.66)	Acc@3   4.30 (  2.12)
Epoch: [2][160/412]	Loss 82.73 (89.96)	InvT  19.70 ( 19.74)	Acc@1   0.39 (  0.66)	Acc@3   1.56 (  2.08)
Epoch: [2][180/412]	Loss 80.8 (89.11)	InvT  19.69 ( 19.73)	Acc@1   1.17 (  0.66)	Acc@3   2.73 (  2.08)
Epoch: [2][200/412]	Loss 83.94 (88.53)	InvT  19.69 ( 19.73)	Acc@1   1.56 (  0.64)	Acc@3   2.34 (  2.08)
Epoch: [2][220/412]	Loss 76.77 (88.04)	InvT  19.68 ( 19.72)	Acc@1   1.56 (  0.65)	Acc@3   3.91 (  2.07)
Epoch: [2][240/412]	Loss 80.97 (87.43)	InvT  19.67 ( 19.72)	Acc@1   1.56 (  0.66)	Acc@3   1.56 (  2.06)
Epoch: [2][260/412]	Loss 82.11 (86.83)	InvT  19.66 ( 19.72)	Acc@1   0.78 (  0.65)	Acc@3   3.12 (  2.04)
Epoch: [2][280/412]	Loss 76.35 (86.36)	InvT  19.65 ( 19.71)	Acc@1   0.39 (  0.64)	Acc@3   3.12 (  2.03)
Epoch: [2][300/412]	Loss 77.28 (85.77)	InvT  19.64 ( 19.71)	Acc@1   1.56 (  0.65)	Acc@3   2.73 (  2.04)
Epoch: [2][320/412]	Loss 76.61 (85.22)	InvT  19.64 ( 19.70)	Acc@1   0.00 (  0.64)	Acc@3   1.17 (  2.03)
Epoch: [2][340/412]	Loss 75.69 (84.72)	InvT  19.63 ( 19.70)	Acc@1   0.39 (  0.66)	Acc@3   1.56 (  2.06)
Epoch: [2][360/412]	Loss 78.37 (84.28)	InvT  19.62 ( 19.69)	Acc@1   0.00 (  0.66)	Acc@3   1.56 (  2.06)
Epoch: [2][380/412]	Loss 75.05 (83.78)	InvT  19.61 ( 19.69)	Acc@1   0.78 (  0.66)	Acc@3   1.56 (  2.07)
Epoch: [2][400/412]	Loss 73.08 (83.29)	InvT  19.60 ( 19.69)	Acc@1   0.00 (  0.66)	Acc@3   0.00 (  2.06)
Learning rate: 2.32725321888412e-05
Epoch 2, valid metric: {"Acc@1": 0.2, "Acc@3": 0.6, "loss": 11.197}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch2.mdl
Epoch: [3][  0/412]	Loss 71.66 (71.66)	InvT  19.60 ( 19.60)	Acc@1   0.39 (  0.39)	Acc@3   1.56 (  1.56)
Epoch: [3][ 20/412]	Loss 74.5 (73.22)	InvT  19.59 ( 19.60)	Acc@1   0.78 (  0.61)	Acc@3   1.56 (  2.08)
Epoch: [3][ 40/412]	Loss 65.47 (72.82)	InvT  19.58 ( 19.59)	Acc@1   0.39 (  0.67)	Acc@3   1.17 (  2.24)
Epoch: [3][ 60/412]	Loss 73.47 (72.66)	InvT  19.58 ( 19.59)	Acc@1   0.39 (  0.64)	Acc@3   1.56 (  2.18)
Epoch: [3][ 80/412]	Loss 66.02 (71.79)	InvT  19.57 ( 19.58)	Acc@1   1.17 (  0.67)	Acc@3   2.34 (  2.15)
Epoch: [3][100/412]	Loss 71.42 (71.71)	InvT  19.56 ( 19.58)	Acc@1   0.00 (  0.65)	Acc@3   1.95 (  2.11)
Epoch: [3][120/412]	Loss 65.3 (71.52)	InvT  19.55 ( 19.58)	Acc@1   0.78 (  0.65)	Acc@3   2.73 (  2.11)
Epoch: [3][140/412]	Loss 71.67 (71.38)	InvT  19.54 ( 19.57)	Acc@1   0.00 (  0.65)	Acc@3   2.34 (  2.09)
Epoch: [3][160/412]	Loss 66.32 (71.2)	InvT  19.54 ( 19.57)	Acc@1   0.78 (  0.64)	Acc@3   2.73 (  2.05)
Epoch: [3][180/412]	Loss 67.56 (70.94)	InvT  19.53 ( 19.56)	Acc@1   0.39 (  0.65)	Acc@3   0.78 (  2.03)
Epoch: [3][200/412]	Loss 67.4 (70.64)	InvT  19.52 ( 19.56)	Acc@1   1.17 (  0.64)	Acc@3   1.56 (  2.03)
Epoch: [3][220/412]	Loss 66.66 (70.31)	InvT  19.51 ( 19.56)	Acc@1   0.39 (  0.65)	Acc@3   1.95 (  2.03)
Epoch: [3][240/412]	Loss 71.29 (69.97)	InvT  19.51 ( 19.55)	Acc@1   0.39 (  0.64)	Acc@3   0.78 (  2.04)
Epoch: [3][260/412]	Loss 66.66 (69.7)	InvT  19.50 ( 19.55)	Acc@1   0.39 (  0.65)	Acc@3   1.56 (  2.03)
Epoch: [3][280/412]	Loss 66.53 (69.44)	InvT  19.49 ( 19.54)	Acc@1   0.78 (  0.65)	Acc@3   1.95 (  2.06)
Epoch: [3][300/412]	Loss 66.14 (69.11)	InvT  19.48 ( 19.54)	Acc@1   0.00 (  0.65)	Acc@3   1.56 (  2.06)
Epoch: [3][320/412]	Loss 63.83 (68.82)	InvT  19.48 ( 19.54)	Acc@1   0.78 (  0.65)	Acc@3   2.34 (  2.06)
Epoch: [3][340/412]	Loss 61.99 (68.54)	InvT  19.47 ( 19.53)	Acc@1   1.17 (  0.66)	Acc@3   2.34 (  2.06)
Epoch: [3][360/412]	Loss 66.38 (68.28)	InvT  19.46 ( 19.53)	Acc@1   0.39 (  0.66)	Acc@3   1.56 (  2.04)
Epoch: [3][380/412]	Loss 62.84 (68.05)	InvT  19.46 ( 19.53)	Acc@1   1.56 (  0.66)	Acc@3   1.95 (  2.03)
Epoch: [3][400/412]	Loss 58.62 (67.69)	InvT  19.45 ( 19.52)	Acc@1   0.78 (  0.67)	Acc@3   2.34 (  2.05)
Learning rate: 1.9957081545064377e-05
Epoch 3, valid metric: {"Acc@1": 0.2, "Acc@3": 0.8, "loss": 9.521}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch3.mdl
Epoch: [4][  0/412]	Loss 64.99 (64.99)	InvT  19.44 ( 19.44)	Acc@1   0.39 (  0.39)	Acc@3   1.95 (  1.95)
Epoch: [4][ 20/412]	Loss 59.97 (63.2)	InvT  19.44 ( 19.44)	Acc@1   0.78 (  0.65)	Acc@3   1.95 (  2.36)
Epoch: [4][ 40/412]	Loss 64.28 (63.22)	InvT  19.43 ( 19.44)	Acc@1   0.39 (  0.57)	Acc@3   0.78 (  2.07)
Epoch: [4][ 60/412]	Loss 65.21 (62.72)	InvT  19.42 ( 19.43)	Acc@1   0.00 (  0.64)	Acc@3   1.95 (  2.11)
Epoch: [4][ 80/412]	Loss 62.42 (62.36)	InvT  19.42 ( 19.43)	Acc@1   0.39 (  0.67)	Acc@3   1.56 (  2.11)
Epoch: [4][100/412]	Loss 56.24 (62.11)	InvT  19.41 ( 19.43)	Acc@1   1.56 (  0.72)	Acc@3   3.52 (  2.18)
Epoch: [4][120/412]	Loss 60.23 (61.7)	InvT  19.40 ( 19.42)	Acc@1   0.78 (  0.68)	Acc@3   3.12 (  2.10)
Epoch: [4][140/412]	Loss 60.47 (61.56)	InvT  19.40 ( 19.42)	Acc@1   0.00 (  0.67)	Acc@3   1.95 (  2.09)
Epoch: [4][160/412]	Loss 60.31 (61.29)	InvT  19.39 ( 19.42)	Acc@1   1.17 (  0.65)	Acc@3   3.91 (  2.11)
Epoch: [4][180/412]	Loss 57.62 (61.11)	InvT  19.38 ( 19.41)	Acc@1   0.78 (  0.66)	Acc@3   1.95 (  2.11)
Epoch: [4][200/412]	Loss 58.27 (60.8)	InvT  19.37 ( 19.41)	Acc@1   0.00 (  0.65)	Acc@3   1.56 (  2.12)
Epoch: [4][220/412]	Loss 58.44 (60.56)	InvT  19.37 ( 19.41)	Acc@1   0.39 (  0.67)	Acc@3   3.12 (  2.11)
Epoch: [4][240/412]	Loss 58.04 (60.34)	InvT  19.36 ( 19.40)	Acc@1   1.17 (  0.67)	Acc@3   2.73 (  2.09)
Epoch: [4][260/412]	Loss 62.36 (60.14)	InvT  19.35 ( 19.40)	Acc@1   0.78 (  0.66)	Acc@3   2.34 (  2.08)
Epoch: [4][280/412]	Loss 57.87 (59.92)	InvT  19.35 ( 19.40)	Acc@1   0.78 (  0.67)	Acc@3   1.95 (  2.07)
Epoch: [4][300/412]	Loss 53.2 (59.71)	InvT  19.34 ( 19.39)	Acc@1   1.17 (  0.67)	Acc@3   2.73 (  2.06)
Epoch: [4][320/412]	Loss 60.55 (59.54)	InvT  19.34 ( 19.39)	Acc@1   0.78 (  0.67)	Acc@3   3.12 (  2.07)
Epoch: [4][340/412]	Loss 56.74 (59.37)	InvT  19.33 ( 19.39)	Acc@1   0.78 (  0.66)	Acc@3   1.17 (  2.06)
Epoch: [4][360/412]	Loss 58.44 (59.19)	InvT  19.32 ( 19.38)	Acc@1   0.39 (  0.66)	Acc@3   1.95 (  2.06)
Epoch: [4][380/412]	Loss 54.77 (58.96)	InvT  19.32 ( 19.38)	Acc@1   0.78 (  0.67)	Acc@3   2.34 (  2.07)
Epoch: [4][400/412]	Loss 53.0 (58.77)	InvT  19.31 ( 19.38)	Acc@1   1.95 (  0.68)	Acc@3   3.12 (  2.09)
Learning rate: 1.6641630901287554e-05
Epoch 4, valid metric: {"Acc@1": 0.2, "Acc@3": 0.6, "loss": 8.459}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch4.mdl
Epoch: [5][  0/412]	Loss 54.28 (54.28)	InvT  19.31 ( 19.31)	Acc@1   1.56 (  1.56)	Acc@3   4.30 (  4.30)
Epoch: [5][ 20/412]	Loss 52.69 (56.09)	InvT  19.30 ( 19.30)	Acc@1   0.39 (  0.73)	Acc@3   1.17 (  2.23)
Epoch: [5][ 40/412]	Loss 58.46 (55.64)	InvT  19.29 ( 19.30)	Acc@1   1.17 (  0.68)	Acc@3   2.34 (  2.27)
Epoch: [5][ 60/412]	Loss 52.67 (55.39)	InvT  19.29 ( 19.30)	Acc@1   0.00 (  0.68)	Acc@3   1.56 (  2.25)
Epoch: [5][ 80/412]	Loss 51.48 (54.87)	InvT  19.28 ( 19.30)	Acc@1   0.78 (  0.66)	Acc@3   2.34 (  2.13)
Epoch: [5][100/412]	Loss 56.95 (54.55)	InvT  19.28 ( 19.29)	Acc@1   0.78 (  0.68)	Acc@3   2.34 (  2.13)
Epoch: [5][120/412]	Loss 53.3 (54.36)	InvT  19.27 ( 19.29)	Acc@1   0.39 (  0.69)	Acc@3   2.73 (  2.11)
Epoch: [5][140/412]	Loss 49.11 (54.23)	InvT  19.27 ( 19.29)	Acc@1   0.39 (  0.69)	Acc@3   1.95 (  2.09)
Epoch: [5][160/412]	Loss 53.54 (54.03)	InvT  19.26 ( 19.28)	Acc@1   0.39 (  0.68)	Acc@3   1.56 (  2.06)
Epoch: [5][180/412]	Loss 55.63 (53.85)	InvT  19.26 ( 19.28)	Acc@1   0.78 (  0.67)	Acc@3   0.78 (  2.05)
Epoch: [5][200/412]	Loss 53.83 (53.64)	InvT  19.25 ( 19.28)	Acc@1   1.17 (  0.67)	Acc@3   1.56 (  2.06)
Epoch: [5][220/412]	Loss 54.88 (53.63)	InvT  19.24 ( 19.27)	Acc@1   0.39 (  0.68)	Acc@3   3.12 (  2.07)
Epoch: [5][240/412]	Loss 50.13 (53.5)	InvT  19.24 ( 19.27)	Acc@1   1.17 (  0.69)	Acc@3   1.56 (  2.06)
Epoch: [5][260/412]	Loss 50.34 (53.43)	InvT  19.23 ( 19.27)	Acc@1   1.56 (  0.69)	Acc@3   1.95 (  2.06)
Epoch: [5][280/412]	Loss 50.9 (53.28)	InvT  19.23 ( 19.27)	Acc@1   0.39 (  0.71)	Acc@3   1.95 (  2.09)
Epoch: [5][300/412]	Loss 50.97 (53.12)	InvT  19.22 ( 19.26)	Acc@1   0.00 (  0.71)	Acc@3   1.95 (  2.11)
Epoch: [5][320/412]	Loss 50.27 (53.02)	InvT  19.22 ( 19.26)	Acc@1   1.17 (  0.72)	Acc@3   3.52 (  2.11)
Epoch: [5][340/412]	Loss 51.54 (52.97)	InvT  19.21 ( 19.26)	Acc@1   0.39 (  0.72)	Acc@3   3.91 (  2.11)
Epoch: [5][360/412]	Loss 44.59 (52.86)	InvT  19.21 ( 19.26)	Acc@1   1.17 (  0.70)	Acc@3   1.56 (  2.10)
Epoch: [5][380/412]	Loss 51.08 (52.74)	InvT  19.20 ( 19.25)	Acc@1   0.78 (  0.70)	Acc@3   2.73 (  2.11)
Epoch: [5][400/412]	Loss 52.2 (52.63)	InvT  19.20 ( 19.25)	Acc@1   1.17 (  0.69)	Acc@3   2.34 (  2.10)
Learning rate: 1.332618025751073e-05
Epoch 5, valid metric: {"Acc@1": 0.2, "Acc@3": 0.6, "loss": 8.705}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch5.mdl
Epoch: [6][  0/412]	Loss 52.81 (52.81)	InvT  19.20 ( 19.20)	Acc@1   0.78 (  0.78)	Acc@3   1.56 (  1.56)
Epoch: [6][ 20/412]	Loss 53.59 (49.36)	InvT  19.19 ( 19.19)	Acc@1   0.78 (  0.80)	Acc@3   3.12 (  2.40)
Epoch: [6][ 40/412]	Loss 51.58 (49.88)	InvT  19.19 ( 19.19)	Acc@1   0.00 (  0.65)	Acc@3   0.78 (  2.36)
Epoch: [6][ 60/412]	Loss 47.88 (50.17)	InvT  19.18 ( 19.19)	Acc@1   0.78 (  0.63)	Acc@3   1.95 (  2.23)
Epoch: [6][ 80/412]	Loss 53.53 (50.22)	InvT  19.18 ( 19.19)	Acc@1   0.39 (  0.69)	Acc@3   2.34 (  2.22)
Epoch: [6][100/412]	Loss 50.05 (50.12)	InvT  19.17 ( 19.18)	Acc@1   0.39 (  0.70)	Acc@3   3.12 (  2.27)
Epoch: [6][120/412]	Loss 49.5 (49.89)	InvT  19.17 ( 19.18)	Acc@1   0.78 (  0.70)	Acc@3   2.34 (  2.22)
Epoch: [6][140/412]	Loss 48.73 (49.84)	InvT  19.16 ( 19.18)	Acc@1   0.00 (  0.69)	Acc@3   2.34 (  2.20)
Epoch: [6][160/412]	Loss 54.63 (49.86)	InvT  19.16 ( 19.18)	Acc@1   1.17 (  0.71)	Acc@3   3.52 (  2.20)
Epoch: [6][180/412]	Loss 52.16 (49.8)	InvT  19.15 ( 19.17)	Acc@1   0.78 (  0.71)	Acc@3   1.95 (  2.18)
Epoch: [6][200/412]	Loss 52.23 (49.74)	InvT  19.15 ( 19.17)	Acc@1   1.17 (  0.71)	Acc@3   1.56 (  2.18)
Epoch: [6][220/412]	Loss 49.62 (49.59)	InvT  19.15 ( 19.17)	Acc@1   0.39 (  0.73)	Acc@3   1.56 (  2.19)
Epoch: [6][240/412]	Loss 48.24 (49.57)	InvT  19.14 ( 19.17)	Acc@1   0.78 (  0.73)	Acc@3   3.91 (  2.19)
Epoch: [6][260/412]	Loss 49.42 (49.5)	InvT  19.14 ( 19.17)	Acc@1   0.78 (  0.72)	Acc@3   1.17 (  2.19)
Epoch: [6][280/412]	Loss 50.48 (49.38)	InvT  19.13 ( 19.16)	Acc@1   1.17 (  0.72)	Acc@3   3.12 (  2.18)
Epoch: [6][300/412]	Loss 47.0 (49.29)	InvT  19.13 ( 19.16)	Acc@1   0.39 (  0.73)	Acc@3   1.56 (  2.19)
Epoch: [6][320/412]	Loss 48.69 (49.2)	InvT  19.13 ( 19.16)	Acc@1   1.17 (  0.73)	Acc@3   3.52 (  2.20)
Epoch: [6][340/412]	Loss 42.22 (49.17)	InvT  19.12 ( 19.16)	Acc@1   0.00 (  0.74)	Acc@3   2.34 (  2.20)
Epoch: [6][360/412]	Loss 49.15 (49.09)	InvT  19.12 ( 19.15)	Acc@1   0.78 (  0.74)	Acc@3   2.34 (  2.21)
Epoch: [6][380/412]	Loss 48.67 (49.01)	InvT  19.11 ( 19.15)	Acc@1   0.00 (  0.74)	Acc@3   1.95 (  2.21)
Epoch: [6][400/412]	Loss 44.26 (48.95)	InvT  19.11 ( 19.15)	Acc@1   0.39 (  0.74)	Acc@3   1.95 (  2.20)
Learning rate: 1.0010729613733907e-05
Epoch 6, valid metric: {"Acc@1": 0.2, "Acc@3": 0.6, "loss": 7.945}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch6.mdl
Epoch: [7][  0/412]	Loss 48.27 (48.27)	InvT  19.11 ( 19.11)	Acc@1   0.39 (  0.39)	Acc@3   2.73 (  2.73)
Epoch: [7][ 20/412]	Loss 47.93 (47.52)	InvT  19.10 ( 19.11)	Acc@1   0.78 (  0.74)	Acc@3   3.12 (  2.19)
Epoch: [7][ 40/412]	Loss 42.23 (47.41)	InvT  19.10 ( 19.10)	Acc@1   0.78 (  0.75)	Acc@3   1.95 (  2.17)
Epoch: [7][ 60/412]	Loss 49.99 (47.14)	InvT  19.10 ( 19.10)	Acc@1   0.39 (  0.74)	Acc@3   1.95 (  2.24)
Epoch: [7][ 80/412]	Loss 46.94 (47.13)	InvT  19.09 ( 19.10)	Acc@1   0.78 (  0.73)	Acc@3   1.95 (  2.26)
Epoch: [7][100/412]	Loss 48.06 (47.09)	InvT  19.09 ( 19.10)	Acc@1   0.39 (  0.75)	Acc@3   1.56 (  2.27)
Epoch: [7][120/412]	Loss 45.62 (47.1)	InvT  19.09 ( 19.10)	Acc@1   1.17 (  0.72)	Acc@3   1.95 (  2.21)
Epoch: [7][140/412]	Loss 42.61 (46.85)	InvT  19.08 ( 19.10)	Acc@1   1.17 (  0.76)	Acc@3   3.52 (  2.22)
Epoch: [7][160/412]	Loss 46.08 (46.77)	InvT  19.08 ( 19.09)	Acc@1   0.39 (  0.77)	Acc@3   0.78 (  2.20)
Epoch: [7][180/412]	Loss 44.57 (46.77)	InvT  19.08 ( 19.09)	Acc@1   0.78 (  0.77)	Acc@3   1.95 (  2.20)
Epoch: [7][200/412]	Loss 46.43 (46.73)	InvT  19.07 ( 19.09)	Acc@1   0.39 (  0.77)	Acc@3   1.95 (  2.22)
Epoch: [7][220/412]	Loss 45.76 (46.61)	InvT  19.07 ( 19.09)	Acc@1   0.00 (  0.76)	Acc@3   2.34 (  2.23)
Epoch: [7][240/412]	Loss 47.2 (46.55)	InvT  19.07 ( 19.09)	Acc@1   1.17 (  0.75)	Acc@3   2.34 (  2.22)
Epoch: [7][260/412]	Loss 43.5 (46.48)	InvT  19.07 ( 19.09)	Acc@1   1.56 (  0.74)	Acc@3   3.12 (  2.20)
Epoch: [7][280/412]	Loss 46.72 (46.45)	InvT  19.06 ( 19.08)	Acc@1   0.78 (  0.74)	Acc@3   3.52 (  2.22)
Epoch: [7][300/412]	Loss 46.47 (46.38)	InvT  19.06 ( 19.08)	Acc@1   1.17 (  0.74)	Acc@3   2.34 (  2.22)
Epoch: [7][320/412]	Loss 47.03 (46.24)	InvT  19.06 ( 19.08)	Acc@1   0.39 (  0.75)	Acc@3   3.52 (  2.24)
Epoch: [7][340/412]	Loss 51.67 (46.18)	InvT  19.05 ( 19.08)	Acc@1   0.78 (  0.74)	Acc@3   1.95 (  2.23)
Epoch: [7][360/412]	Loss 41.33 (46.08)	InvT  19.05 ( 19.08)	Acc@1   0.78 (  0.75)	Acc@3   2.73 (  2.22)
Epoch: [7][380/412]	Loss 44.78 (46.03)	InvT  19.05 ( 19.08)	Acc@1   0.78 (  0.76)	Acc@3   1.95 (  2.22)
Epoch: [7][400/412]	Loss 40.84 (45.93)	InvT  19.05 ( 19.07)	Acc@1   0.39 (  0.76)	Acc@3   1.17 (  2.21)
Learning rate: 6.695278969957082e-06
Epoch 7, valid metric: {"Acc@1": 0.2, "Acc@3": 0.6, "loss": 7.606}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch7.mdl
Epoch: [8][  0/412]	Loss 46.23 (46.23)	InvT  19.04 ( 19.04)	Acc@1   0.78 (  0.78)	Acc@3   1.95 (  1.95)
Epoch: [8][ 20/412]	Loss 45.24 (44.41)	InvT  19.04 ( 19.04)	Acc@1   1.17 (  0.89)	Acc@3   3.52 (  2.27)
Epoch: [8][ 40/412]	Loss 44.3 (44.62)	InvT  19.04 ( 19.04)	Acc@1   0.39 (  0.79)	Acc@3   1.56 (  2.31)
Epoch: [8][ 60/412]	Loss 43.5 (44.48)	InvT  19.04 ( 19.04)	Acc@1   1.17 (  0.80)	Acc@3   1.95 (  2.38)
Epoch: [8][ 80/412]	Loss 42.36 (44.42)	InvT  19.04 ( 19.04)	Acc@1   1.56 (  0.82)	Acc@3   2.73 (  2.36)
Epoch: [8][100/412]	Loss 45.0 (44.52)	InvT  19.03 ( 19.04)	Acc@1   0.78 (  0.81)	Acc@3   2.73 (  2.30)
Epoch: [8][120/412]	Loss 43.99 (44.51)	InvT  19.03 ( 19.04)	Acc@1   0.39 (  0.80)	Acc@3   3.12 (  2.30)
Epoch: [8][140/412]	Loss 44.71 (44.35)	InvT  19.03 ( 19.04)	Acc@1   0.39 (  0.80)	Acc@3   1.95 (  2.32)
Epoch: [8][160/412]	Loss 39.79 (44.26)	InvT  19.03 ( 19.04)	Acc@1   0.78 (  0.79)	Acc@3   0.78 (  2.31)
Epoch: [8][180/412]	Loss 40.36 (44.15)	InvT  19.02 ( 19.03)	Acc@1   1.17 (  0.78)	Acc@3   3.12 (  2.30)
Epoch: [8][200/412]	Loss 43.99 (44.1)	InvT  19.02 ( 19.03)	Acc@1   0.00 (  0.78)	Acc@3   1.17 (  2.29)
Epoch: [8][220/412]	Loss 45.7 (44.05)	InvT  19.02 ( 19.03)	Acc@1   1.95 (  0.79)	Acc@3   3.12 (  2.30)
Epoch: [8][240/412]	Loss 43.78 (44.03)	InvT  19.02 ( 19.03)	Acc@1   0.00 (  0.78)	Acc@3   1.17 (  2.28)
Epoch: [8][260/412]	Loss 43.37 (43.99)	InvT  19.02 ( 19.03)	Acc@1   1.17 (  0.78)	Acc@3   1.95 (  2.30)
Epoch: [8][280/412]	Loss 38.76 (43.89)	InvT  19.02 ( 19.03)	Acc@1   1.95 (  0.79)	Acc@3   2.34 (  2.29)
Epoch: [8][300/412]	Loss 40.52 (43.8)	InvT  19.01 ( 19.03)	Acc@1   1.17 (  0.79)	Acc@3   3.91 (  2.31)
Epoch: [8][320/412]	Loss 40.83 (43.73)	InvT  19.01 ( 19.03)	Acc@1   0.39 (  0.80)	Acc@3   1.56 (  2.31)
Epoch: [8][340/412]	Loss 37.75 (43.68)	InvT  19.01 ( 19.03)	Acc@1   0.39 (  0.79)	Acc@3   1.17 (  2.30)
Epoch: [8][360/412]	Loss 39.99 (43.59)	InvT  19.01 ( 19.03)	Acc@1   1.56 (  0.80)	Acc@3   2.34 (  2.31)
Epoch: [8][380/412]	Loss 42.61 (43.6)	InvT  19.01 ( 19.02)	Acc@1   0.00 (  0.80)	Acc@3   1.17 (  2.30)
Epoch: [8][400/412]	Loss 42.21 (43.53)	InvT  19.01 ( 19.02)	Acc@1   0.78 (  0.80)	Acc@3   3.12 (  2.32)
Learning rate: 3.379828326180258e-06
Epoch 8, valid metric: {"Acc@1": 0.2, "Acc@3": 0.5, "loss": 7.677}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch8.mdl
Epoch: [9][  0/412]	Loss 41.62 (41.62)	InvT  19.01 ( 19.01)	Acc@1   0.78 (  0.78)	Acc@3   2.34 (  2.34)
Epoch: [9][ 20/412]	Loss 42.93 (43.4)	InvT  19.00 ( 19.01)	Acc@1   1.56 (  0.78)	Acc@3   2.73 (  2.44)
Epoch: [9][ 40/412]	Loss 43.39 (43.19)	InvT  19.00 ( 19.00)	Acc@1   0.39 (  0.75)	Acc@3   2.34 (  2.41)
Epoch: [9][ 60/412]	Loss 42.55 (42.65)	InvT  19.00 ( 19.00)	Acc@1   0.78 (  0.77)	Acc@3   1.95 (  2.38)
Epoch: [9][ 80/412]	Loss 41.64 (42.79)	InvT  19.00 ( 19.00)	Acc@1   1.17 (  0.74)	Acc@3   2.34 (  2.40)
Epoch: [9][100/412]	Loss 44.27 (42.78)	InvT  19.00 ( 19.00)	Acc@1   0.78 (  0.75)	Acc@3   2.34 (  2.43)
Epoch: [9][120/412]	Loss 40.65 (42.69)	InvT  19.00 ( 19.00)	Acc@1   0.39 (  0.77)	Acc@3   1.56 (  2.38)
Epoch: [9][140/412]	Loss 44.13 (42.67)	InvT  19.00 ( 19.00)	Acc@1   0.00 (  0.77)	Acc@3   2.34 (  2.38)
Epoch: [9][160/412]	Loss 44.52 (42.61)	InvT  19.00 ( 19.00)	Acc@1   0.78 (  0.75)	Acc@3   1.56 (  2.32)
Epoch: [9][180/412]	Loss 42.24 (42.52)	InvT  19.00 ( 19.00)	Acc@1   0.78 (  0.76)	Acc@3   1.95 (  2.33)
Epoch: [9][200/412]	Loss 39.91 (42.43)	InvT  19.00 ( 19.00)	Acc@1   1.95 (  0.77)	Acc@3   2.73 (  2.31)
Epoch: [9][220/412]	Loss 43.76 (42.47)	InvT  19.00 ( 19.00)	Acc@1   3.12 (  0.77)	Acc@3   5.86 (  2.33)
Epoch: [9][240/412]	Loss 40.15 (42.37)	InvT  19.00 ( 19.00)	Acc@1   1.17 (  0.80)	Acc@3   3.12 (  2.35)
Epoch: [9][260/412]	Loss 44.69 (42.36)	InvT  18.99 ( 19.00)	Acc@1   0.39 (  0.82)	Acc@3   1.95 (  2.38)
Epoch: [9][280/412]	Loss 42.18 (42.37)	InvT  18.99 ( 19.00)	Acc@1   1.17 (  0.82)	Acc@3   3.91 (  2.40)
Epoch: [9][300/412]	Loss 40.18 (42.35)	InvT  18.99 ( 19.00)	Acc@1   0.78 (  0.82)	Acc@3   4.30 (  2.41)
Epoch: [9][320/412]	Loss 42.54 (42.34)	InvT  18.99 ( 19.00)	Acc@1   1.56 (  0.82)	Acc@3   3.91 (  2.40)
Epoch: [9][340/412]	Loss 39.47 (42.29)	InvT  18.99 ( 19.00)	Acc@1   1.56 (  0.84)	Acc@3   5.47 (  2.43)
Epoch: [9][360/412]	Loss 41.85 (42.26)	InvT  18.99 ( 19.00)	Acc@1   0.78 (  0.84)	Acc@3   3.91 (  2.44)
Epoch: [9][380/412]	Loss 42.5 (42.26)	InvT  18.99 ( 19.00)	Acc@1   0.78 (  0.85)	Acc@3   1.56 (  2.43)
Epoch: [9][400/412]	Loss 44.72 (42.25)	InvT  18.99 ( 19.00)	Acc@1   0.78 (  0.86)	Acc@3   1.95 (  2.45)
Learning rate: 6.437768240343348e-08
Epoch 9, valid metric: {"Acc@1": 0.2, "Acc@3": 0.6, "loss": 7.548}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch9.mdl
=> creating model
CustomBertModel(
  (dropout): Dropout(p=0.3, inplace=False)
  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
layer_norm.weight: 768
layer_norm.bias: 768
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 412, warmup steps: 41
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 41,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.3,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 1,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
=> creating model
CustomBertModel(
  (dropout): Dropout(p=0.3, inplace=False)
  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
layer_norm.weight: 768
layer_norm.bias: 768
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 412, warmup steps: 41
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 41,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.3,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 1,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
=> creating model
CustomBertModel(
  (dropout): Dropout(p=0.3, inplace=False)
  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
layer_norm.weight: 768
layer_norm.bias: 768
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 412, warmup steps: 41
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 41,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.3,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 1,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
=> creating model
CustomBertModel(
  (dropout): Dropout(p=0.3, inplace=False)
  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
layer_norm.weight: 768
layer_norm.bias: 768
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 412, warmup steps: 41
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 41,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.3,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 1,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
=> creating model
CustomBertModel(
  (dropout): Dropout(p=0.3, inplace=False)
  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
layer_norm.weight: 768
layer_norm.bias: 768
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 6605, warmup steps: 400
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 400,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.3,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 1,
    "batch_size": 16,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
Epoch: [0][   0/6605]	Loss 3.338e+03 (3.338e+03)	InvT  20.00 ( 20.00)	Acc@1   6.25 (  6.25)	Acc@3  18.75 ( 18.75)
=> creating model
CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 8257, warmup steps: 400
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 400,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 20,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
Epoch: [0][  0/412]	Loss 12.23 (12.23)	InvT  20.00 ( 20.00)	Acc@1   0.78 (  0.78)	Acc@3   1.95 (  1.95)
Epoch: [0][ 20/412]	Loss 10.93 (11.9)	InvT  20.00 ( 20.00)	Acc@1   2.34 (  0.73)	Acc@3   6.25 (  3.61)
Epoch: [0][ 40/412]	Loss 10.22 (11.36)	InvT  20.00 ( 20.00)	Acc@1   1.17 (  0.76)	Acc@3   7.42 (  4.54)
Epoch: [0][ 60/412]	Loss 9.41 (10.81)	InvT  20.00 ( 20.00)	Acc@1   0.78 (  0.86)	Acc@3   9.38 (  6.09)
Epoch: [0][ 80/412]	Loss 8.398 (10.31)	InvT  20.00 ( 20.00)	Acc@1   4.30 (  1.39)	Acc@3  15.62 (  7.97)
Epoch: [0][100/412]	Loss 7.88 (9.858)	InvT  19.99 ( 20.00)	Acc@1   7.03 (  2.32)	Acc@3  23.05 ( 10.32)
Epoch: [0][120/412]	Loss 7.331 (9.479)	InvT  19.99 ( 20.00)	Acc@1  10.55 (  3.48)	Acc@3  25.00 ( 12.70)
Epoch: [0][140/412]	Loss 6.984 (9.155)	InvT  19.99 ( 20.00)	Acc@1  10.94 (  4.61)	Acc@3  32.42 ( 15.12)
Epoch: [0][160/412]	Loss 6.748 (8.876)	InvT  19.99 ( 20.00)	Acc@1  18.36 (  5.84)	Acc@3  35.16 ( 17.42)
Epoch: [0][180/412]	Loss 6.594 (8.632)	InvT  19.98 ( 19.99)	Acc@1  20.70 (  7.03)	Acc@3  37.89 ( 19.51)
Epoch: [0][200/412]	Loss 6.166 (8.412)	InvT  19.98 ( 19.99)	Acc@1  21.09 (  8.28)	Acc@3  42.58 ( 21.45)
Epoch: [0][220/412]	Loss 5.524 (8.208)	InvT  19.98 ( 19.99)	Acc@1  27.34 (  9.49)	Acc@3  48.83 ( 23.36)
Epoch: [0][240/412]	Loss 5.703 (8.016)	InvT  19.97 ( 19.99)	Acc@1  26.95 ( 10.70)	Acc@3  51.17 ( 25.25)
Epoch: [0][260/412]	Loss 5.622 (7.847)	InvT  19.97 ( 19.99)	Acc@1  23.44 ( 11.81)	Acc@3  47.27 ( 26.86)
Epoch: [0][280/412]	Loss 5.475 (7.689)	InvT  19.97 ( 19.99)	Acc@1  28.91 ( 12.85)	Acc@3  50.00 ( 28.37)
Epoch: [0][300/412]	Loss 5.653 (7.544)	InvT  19.96 ( 19.99)	Acc@1  26.56 ( 13.81)	Acc@3  48.44 ( 29.82)
Epoch: [0][320/412]	Loss 5.483 (7.413)	InvT  19.96 ( 19.98)	Acc@1  30.08 ( 14.75)	Acc@3  48.44 ( 31.16)
Epoch: [0][340/412]	Loss 5.348 (7.288)	InvT  19.95 ( 19.98)	Acc@1  26.17 ( 15.63)	Acc@3  52.34 ( 32.43)
Epoch: [0][360/412]	Loss 5.252 (7.173)	InvT  19.95 ( 19.98)	Acc@1  27.73 ( 16.48)	Acc@3  55.47 ( 33.57)
Epoch: [0][380/412]	Loss 5.341 (7.061)	InvT  19.94 ( 19.98)	Acc@1  28.91 ( 17.39)	Acc@3  52.34 ( 34.72)
Epoch: [0][400/412]	Loss 5.147 (6.962)	InvT  19.93 ( 19.98)	Acc@1  32.42 ( 18.18)	Acc@3  55.08 ( 35.77)
Learning rate: 2.9954180985108822e-05
Epoch 0, valid metric: {"Acc@1": 19.2, "Acc@3": 31.3, "loss": 4.019}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch0.mdl
Epoch: [1][  0/412]	Loss 4.489 (4.489)	InvT  19.93 ( 19.93)	Acc@1  39.84 ( 39.84)	Acc@3  63.28 ( 63.28)
Epoch: [1][ 20/412]	Loss 4.455 (4.635)	InvT  19.92 ( 19.92)	Acc@1  39.45 ( 37.98)	Acc@3  66.80 ( 60.51)
Epoch: [1][ 40/412]	Loss 4.396 (4.639)	InvT  19.92 ( 19.92)	Acc@1  41.41 ( 37.79)	Acc@3  64.06 ( 60.63)
Epoch: [1][ 60/412]	Loss 4.633 (4.615)	InvT  19.91 ( 19.92)	Acc@1  35.94 ( 38.06)	Acc@3  60.16 ( 60.80)
Epoch: [1][ 80/412]	Loss 4.345 (4.569)	InvT  19.91 ( 19.92)	Acc@1  40.62 ( 38.53)	Acc@3  66.02 ( 61.29)
Epoch: [1][100/412]	Loss 4.478 (4.537)	InvT  19.90 ( 19.91)	Acc@1  36.72 ( 38.83)	Acc@3  62.50 ( 61.63)
Epoch: [1][120/412]	Loss 4.597 (4.51)	InvT  19.90 ( 19.91)	Acc@1  36.33 ( 39.13)	Acc@3  58.20 ( 61.91)
Epoch: [1][140/412]	Loss 4.093 (4.484)	InvT  19.89 ( 19.91)	Acc@1  45.70 ( 39.46)	Acc@3  67.97 ( 62.22)
Epoch: [1][160/412]	Loss 4.077 (4.468)	InvT  19.88 ( 19.91)	Acc@1  46.48 ( 39.73)	Acc@3  66.02 ( 62.42)
Epoch: [1][180/412]	Loss 4.779 (4.455)	InvT  19.88 ( 19.90)	Acc@1  38.67 ( 39.89)	Acc@3  59.77 ( 62.55)
Epoch: [1][200/412]	Loss 4.149 (4.438)	InvT  19.87 ( 19.90)	Acc@1  44.53 ( 40.16)	Acc@3  68.75 ( 62.71)
Epoch: [1][220/412]	Loss 4.329 (4.414)	InvT  19.87 ( 19.90)	Acc@1  41.41 ( 40.46)	Acc@3  62.11 ( 62.91)
Epoch: [1][240/412]	Loss 4.51 (4.396)	InvT  19.86 ( 19.90)	Acc@1  38.67 ( 40.66)	Acc@3  62.89 ( 63.12)
Epoch: [1][260/412]	Loss 3.975 (4.386)	InvT  19.86 ( 19.89)	Acc@1  45.70 ( 40.81)	Acc@3  67.19 ( 63.24)
Epoch: [1][280/412]	Loss 4.325 (4.37)	InvT  19.85 ( 19.89)	Acc@1  43.75 ( 41.02)	Acc@3  66.41 ( 63.42)
Epoch: [1][300/412]	Loss 3.961 (4.35)	InvT  19.84 ( 19.89)	Acc@1  43.75 ( 41.26)	Acc@3  69.14 ( 63.63)
Epoch: [1][320/412]	Loss 3.906 (4.327)	InvT  19.84 ( 19.88)	Acc@1  44.53 ( 41.55)	Acc@3  64.06 ( 63.87)
Epoch: [1][340/412]	Loss 4.07 (4.313)	InvT  19.83 ( 19.88)	Acc@1  46.09 ( 41.73)	Acc@3  65.23 ( 64.05)
Epoch: [1][360/412]	Loss 3.787 (4.305)	InvT  19.83 ( 19.88)	Acc@1  48.83 ( 41.82)	Acc@3  69.92 ( 64.10)
Epoch: [1][380/412]	Loss 3.749 (4.287)	InvT  19.82 ( 19.88)	Acc@1  49.61 ( 42.00)	Acc@3  70.70 ( 64.29)
Epoch: [1][400/412]	Loss 4.403 (4.273)	InvT  19.82 ( 19.87)	Acc@1  41.80 ( 42.15)	Acc@3  63.28 ( 64.42)
Learning rate: 2.838106147384498e-05
Epoch 1, valid metric: {"Acc@1": 24.3, "Acc@3": 39.7, "loss": 3.633}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch1.mdl
Epoch: [2][  0/412]	Loss 3.363 (3.363)	InvT  19.81 ( 19.81)	Acc@1  54.69 ( 54.69)	Acc@3  71.09 ( 71.09)
Epoch: [2][ 20/412]	Loss 2.991 (3.282)	InvT  19.81 ( 19.81)	Acc@1  53.12 ( 53.05)	Acc@3  78.91 ( 75.26)
Epoch: [2][ 40/412]	Loss 3.11 (3.288)	InvT  19.81 ( 19.81)	Acc@1  54.69 ( 53.22)	Acc@3  75.78 ( 75.30)
Epoch: [2][ 60/412]	Loss 3.272 (3.29)	InvT  19.82 ( 19.81)	Acc@1  52.34 ( 52.90)	Acc@3  74.22 ( 75.21)
Epoch: [2][ 80/412]	Loss 2.996 (3.293)	InvT  19.82 ( 19.82)	Acc@1  57.03 ( 52.89)	Acc@3  77.73 ( 75.02)
Epoch: [2][100/412]	Loss 3.404 (3.29)	InvT  19.82 ( 19.82)	Acc@1  51.95 ( 53.06)	Acc@3  72.27 ( 75.08)
Epoch: [2][120/412]	Loss 3.442 (3.293)	InvT  19.82 ( 19.82)	Acc@1  50.39 ( 52.86)	Acc@3  72.66 ( 75.08)
Epoch: [2][140/412]	Loss 3.359 (3.284)	InvT  19.82 ( 19.82)	Acc@1  50.78 ( 52.93)	Acc@3  73.44 ( 75.13)
Epoch: [2][160/412]	Loss 3.377 (3.297)	InvT  19.82 ( 19.82)	Acc@1  55.47 ( 52.94)	Acc@3  74.22 ( 74.99)
Epoch: [2][180/412]	Loss 3.308 (3.285)	InvT  19.82 ( 19.82)	Acc@1  53.91 ( 53.09)	Acc@3  70.31 ( 75.07)
Epoch: [2][200/412]	Loss 3.321 (3.283)	InvT  19.82 ( 19.82)	Acc@1  53.52 ( 53.21)	Acc@3  72.66 ( 75.08)
Epoch: [2][220/412]	Loss 3.33 (3.284)	InvT  19.82 ( 19.82)	Acc@1  52.34 ( 53.27)	Acc@3  73.83 ( 75.04)
Epoch: [2][240/412]	Loss 3.423 (3.279)	InvT  19.82 ( 19.82)	Acc@1  53.91 ( 53.37)	Acc@3  72.27 ( 75.06)
Epoch: [2][260/412]	Loss 3.253 (3.279)	InvT  19.82 ( 19.82)	Acc@1  53.91 ( 53.35)	Acc@3  75.00 ( 75.04)
Epoch: [2][280/412]	Loss 2.946 (3.273)	InvT  19.82 ( 19.82)	Acc@1  55.86 ( 53.46)	Acc@3  78.91 ( 75.13)
Epoch: [2][300/412]	Loss 3.226 (3.275)	InvT  19.82 ( 19.82)	Acc@1  52.73 ( 53.38)	Acc@3  74.61 ( 75.14)
Epoch: [2][320/412]	Loss 3.069 (3.276)	InvT  19.82 ( 19.82)	Acc@1  55.86 ( 53.42)	Acc@3  75.78 ( 75.10)
Epoch: [2][340/412]	Loss 3.672 (3.275)	InvT  19.82 ( 19.82)	Acc@1  50.78 ( 53.44)	Acc@3  71.09 ( 75.14)
Epoch: [2][360/412]	Loss 3.366 (3.268)	InvT  19.82 ( 19.82)	Acc@1  55.86 ( 53.54)	Acc@3  72.27 ( 75.20)
Epoch: [2][380/412]	Loss 3.318 (3.266)	InvT  19.82 ( 19.82)	Acc@1  55.08 ( 53.57)	Acc@3  75.00 ( 75.21)
Epoch: [2][400/412]	Loss 3.374 (3.262)	InvT  19.82 ( 19.82)	Acc@1  53.52 ( 53.63)	Acc@3  77.34 ( 75.25)
Learning rate: 2.680794196258114e-05
Epoch 2, valid metric: {"Acc@1": 27.4, "Acc@3": 42.9, "loss": 3.461}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch2.mdl
Epoch: [3][  0/412]	Loss 2.821 (2.821)	InvT  19.82 ( 19.82)	Acc@1  57.42 ( 57.42)	Acc@3  82.81 ( 82.81)
Epoch: [3][ 20/412]	Loss 2.562 (2.58)	InvT  19.83 ( 19.82)	Acc@1  63.67 ( 62.22)	Acc@3  80.47 ( 82.44)
Epoch: [3][ 40/412]	Loss 2.68 (2.569)	InvT  19.84 ( 19.83)	Acc@1  60.16 ( 62.32)	Acc@3  85.16 ( 82.56)
Epoch: [3][ 60/412]	Loss 2.615 (2.583)	InvT  19.85 ( 19.83)	Acc@1  63.67 ( 62.26)	Acc@3  80.86 ( 82.29)
Epoch: [3][ 80/412]	Loss 2.52 (2.579)	InvT  19.85 ( 19.84)	Acc@1  60.94 ( 62.23)	Acc@3  79.69 ( 82.37)
Epoch: [3][100/412]	Loss 2.553 (2.58)	InvT  19.86 ( 19.84)	Acc@1  62.11 ( 62.26)	Acc@3  83.20 ( 82.41)
Epoch: [3][120/412]	Loss 2.877 (2.584)	InvT  19.87 ( 19.85)	Acc@1  58.98 ( 62.20)	Acc@3  79.30 ( 82.36)
Epoch: [3][140/412]	Loss 3.044 (2.601)	InvT  19.88 ( 19.85)	Acc@1  57.42 ( 61.94)	Acc@3  77.73 ( 82.16)
Epoch: [3][160/412]	Loss 2.239 (2.599)	InvT  19.88 ( 19.85)	Acc@1  67.19 ( 61.99)	Acc@3  84.77 ( 82.24)
Epoch: [3][180/412]	Loss 2.505 (2.596)	InvT  19.89 ( 19.86)	Acc@1  59.77 ( 62.02)	Acc@3  85.94 ( 82.32)
Epoch: [3][200/412]	Loss 2.583 (2.6)	InvT  19.90 ( 19.86)	Acc@1  63.28 ( 61.92)	Acc@3  82.81 ( 82.24)
Epoch: [3][220/412]	Loss 2.873 (2.604)	InvT  19.90 ( 19.86)	Acc@1  56.64 ( 61.84)	Acc@3  78.52 ( 82.22)
Epoch: [3][240/412]	Loss 2.959 (2.606)	InvT  19.91 ( 19.87)	Acc@1  57.03 ( 61.90)	Acc@3  76.56 ( 82.16)
Epoch: [3][260/412]	Loss 2.683 (2.6)	InvT  19.92 ( 19.87)	Acc@1  57.81 ( 61.94)	Acc@3  82.03 ( 82.20)
Epoch: [3][280/412]	Loss 2.671 (2.601)	InvT  19.92 ( 19.87)	Acc@1  59.38 ( 61.93)	Acc@3  80.08 ( 82.19)
Epoch: [3][300/412]	Loss 2.728 (2.601)	InvT  19.93 ( 19.88)	Acc@1  58.98 ( 61.92)	Acc@3  83.20 ( 82.19)
Epoch: [3][320/412]	Loss 2.164 (2.602)	InvT  19.93 ( 19.88)	Acc@1  68.36 ( 61.92)	Acc@3  87.89 ( 82.15)
Epoch: [3][340/412]	Loss 2.379 (2.604)	InvT  19.94 ( 19.88)	Acc@1  65.62 ( 61.88)	Acc@3  87.11 ( 82.16)
Epoch: [3][360/412]	Loss 2.359 (2.602)	InvT  19.95 ( 19.89)	Acc@1  65.62 ( 61.94)	Acc@3  83.59 ( 82.19)
Epoch: [3][380/412]	Loss 2.576 (2.603)	InvT  19.95 ( 19.89)	Acc@1  63.67 ( 61.96)	Acc@3  81.64 ( 82.21)
Epoch: [3][400/412]	Loss 2.496 (2.598)	InvT  19.96 ( 19.89)	Acc@1  63.67 ( 62.00)	Acc@3  82.03 ( 82.23)
Learning rate: 2.5234822451317296e-05
Epoch 3, valid metric: {"Acc@1": 29.5, "Acc@3": 45.6, "loss": 3.383}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch3.mdl
Epoch: [4][  0/412]	Loss 1.954 (1.954)	InvT  19.96 ( 19.96)	Acc@1  69.53 ( 69.53)	Acc@3  88.67 ( 88.67)
Epoch: [4][ 20/412]	Loss 1.894 (2.023)	InvT  19.97 ( 19.97)	Acc@1  73.44 ( 69.20)	Acc@3  85.94 ( 87.87)
Epoch: [4][ 40/412]	Loss 2.332 (2.039)	InvT  19.99 ( 19.97)	Acc@1  66.02 ( 68.92)	Acc@3  83.59 ( 87.84)
Epoch: [4][ 60/412]	Loss 2.116 (2.042)	InvT  20.00 ( 19.98)	Acc@1  67.58 ( 68.92)	Acc@3  88.28 ( 87.93)
Epoch: [4][ 80/412]	Loss 1.722 (2.046)	InvT  20.02 ( 19.99)	Acc@1  75.00 ( 68.99)	Acc@3  91.41 ( 87.89)
Epoch: [4][100/412]	Loss 2.087 (2.051)	InvT  20.03 ( 20.00)	Acc@1  69.53 ( 69.02)	Acc@3  87.11 ( 87.81)
Epoch: [4][120/412]	Loss 2.23 (2.049)	InvT  20.04 ( 20.00)	Acc@1  67.58 ( 69.11)	Acc@3  84.38 ( 87.83)
Epoch: [4][140/412]	Loss 2.121 (2.061)	InvT  20.05 ( 20.01)	Acc@1  67.97 ( 68.94)	Acc@3  86.72 ( 87.73)
Epoch: [4][160/412]	Loss 2.088 (2.065)	InvT  20.06 ( 20.01)	Acc@1  68.36 ( 68.85)	Acc@3  85.94 ( 87.66)
Epoch: [4][180/412]	Loss 2.117 (2.067)	InvT  20.07 ( 20.02)	Acc@1  67.97 ( 68.70)	Acc@3  88.28 ( 87.63)
Epoch: [4][200/412]	Loss 1.957 (2.072)	InvT  20.08 ( 20.03)	Acc@1  71.88 ( 68.67)	Acc@3  89.45 ( 87.56)
Epoch: [4][220/412]	Loss 2.034 (2.078)	InvT  20.09 ( 20.03)	Acc@1  69.14 ( 68.56)	Acc@3  89.84 ( 87.54)
Epoch: [4][240/412]	Loss 2.334 (2.091)	InvT  20.10 ( 20.04)	Acc@1  66.80 ( 68.43)	Acc@3  85.55 ( 87.43)
Epoch: [4][260/412]	Loss 2.21 (2.098)	InvT  20.11 ( 20.04)	Acc@1  65.62 ( 68.37)	Acc@3  86.72 ( 87.40)
Epoch: [4][280/412]	Loss 2.117 (2.101)	InvT  20.12 ( 20.05)	Acc@1  69.53 ( 68.36)	Acc@3  88.67 ( 87.35)
Epoch: [4][300/412]	Loss 2.245 (2.099)	InvT  20.13 ( 20.05)	Acc@1  65.23 ( 68.32)	Acc@3  86.72 ( 87.36)
Epoch: [4][320/412]	Loss 2.172 (2.101)	InvT  20.14 ( 20.06)	Acc@1  66.02 ( 68.24)	Acc@3  87.11 ( 87.32)
Epoch: [4][340/412]	Loss 1.97 (2.108)	InvT  20.14 ( 20.06)	Acc@1  70.70 ( 68.16)	Acc@3  87.50 ( 87.24)
Epoch: [4][360/412]	Loss 2.05 (2.106)	InvT  20.15 ( 20.07)	Acc@1  70.70 ( 68.22)	Acc@3  86.33 ( 87.25)
Epoch: [4][380/412]	Loss 2.288 (2.109)	InvT  20.16 ( 20.07)	Acc@1  68.36 ( 68.21)	Acc@3  86.72 ( 87.24)
Epoch: [4][400/412]	Loss 1.821 (2.109)	InvT  20.17 ( 20.08)	Acc@1  73.44 ( 68.21)	Acc@3  87.89 ( 87.21)
Learning rate: 2.3661702940053455e-05
Epoch 4, valid metric: {"Acc@1": 28.6, "Acc@3": 46.7, "loss": 3.341}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch4.mdl
Epoch: [5][  0/412]	Loss 1.68 (1.68)	InvT  20.18 ( 20.18)	Acc@1  73.05 ( 73.05)	Acc@3  92.58 ( 92.58)
Epoch: [5][ 20/412]	Loss 1.543 (1.671)	InvT  20.19 ( 20.18)	Acc@1  75.39 ( 73.64)	Acc@3  92.58 ( 91.80)
Epoch: [5][ 40/412]	Loss 1.639 (1.638)	InvT  20.21 ( 20.19)	Acc@1  77.34 ( 75.07)	Acc@3  92.58 ( 91.76)
Epoch: [5][ 60/412]	Loss 1.642 (1.651)	InvT  20.22 ( 20.20)	Acc@1  75.78 ( 74.98)	Acc@3  94.14 ( 91.52)
Epoch: [5][ 80/412]	Loss 1.661 (1.668)	InvT  20.23 ( 20.20)	Acc@1  74.22 ( 74.66)	Acc@3  92.19 ( 91.39)
Epoch: [5][100/412]	Loss 1.644 (1.663)	InvT  20.25 ( 20.21)	Acc@1  73.05 ( 74.74)	Acc@3  93.36 ( 91.47)
Epoch: [5][120/412]	Loss 1.652 (1.657)	InvT  20.26 ( 20.22)	Acc@1  72.27 ( 74.73)	Acc@3  92.97 ( 91.52)
Epoch: [5][140/412]	Loss 1.604 (1.66)	InvT  20.27 ( 20.23)	Acc@1  74.61 ( 74.53)	Acc@3  91.80 ( 91.48)
Epoch: [5][160/412]	Loss 1.832 (1.67)	InvT  20.29 ( 20.23)	Acc@1  72.66 ( 74.35)	Acc@3  88.67 ( 91.36)
Epoch: [5][180/412]	Loss 1.915 (1.68)	InvT  20.30 ( 20.24)	Acc@1  71.48 ( 74.20)	Acc@3  89.45 ( 91.27)
Epoch: [5][200/412]	Loss 1.803 (1.685)	InvT  20.31 ( 20.25)	Acc@1  70.70 ( 74.15)	Acc@3  88.28 ( 91.23)
Epoch: [5][220/412]	Loss 1.922 (1.687)	InvT  20.32 ( 20.25)	Acc@1  70.70 ( 74.11)	Acc@3  88.67 ( 91.21)
Epoch: [5][240/412]	Loss 1.75 (1.692)	InvT  20.33 ( 20.26)	Acc@1  73.83 ( 74.01)	Acc@3  91.02 ( 91.18)
Epoch: [5][260/412]	Loss 1.875 (1.699)	InvT  20.34 ( 20.26)	Acc@1  72.27 ( 73.94)	Acc@3  90.23 ( 91.10)
Epoch: [5][280/412]	Loss 1.731 (1.702)	InvT  20.35 ( 20.27)	Acc@1  75.78 ( 73.91)	Acc@3  89.06 ( 91.02)
Epoch: [5][300/412]	Loss 1.787 (1.704)	InvT  20.36 ( 20.28)	Acc@1  75.78 ( 73.91)	Acc@3  91.02 ( 90.99)
Epoch: [5][320/412]	Loss 1.894 (1.706)	InvT  20.37 ( 20.28)	Acc@1  69.14 ( 73.90)	Acc@3  90.62 ( 90.96)
Epoch: [5][340/412]	Loss 1.623 (1.707)	InvT  20.38 ( 20.29)	Acc@1  74.61 ( 73.89)	Acc@3  91.41 ( 90.94)
Epoch: [5][360/412]	Loss 1.578 (1.708)	InvT  20.39 ( 20.29)	Acc@1  76.56 ( 73.88)	Acc@3  91.80 ( 90.93)
Epoch: [5][380/412]	Loss 1.561 (1.709)	InvT  20.40 ( 20.30)	Acc@1  76.95 ( 73.88)	Acc@3  92.19 ( 90.90)
Epoch: [5][400/412]	Loss 1.664 (1.711)	InvT  20.41 ( 20.30)	Acc@1  76.95 ( 73.87)	Acc@3  91.02 ( 90.87)
Learning rate: 2.2088583428789617e-05
Epoch 5, valid metric: {"Acc@1": 32.4, "Acc@3": 48.8, "loss": 3.322}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch5.mdl
Epoch: [6][  0/412]	Loss 1.262 (1.262)	InvT  20.41 ( 20.41)	Acc@1  81.64 ( 81.64)	Acc@3  94.53 ( 94.53)
Epoch: [6][ 20/412]	Loss 1.347 (1.342)	InvT  20.43 ( 20.42)	Acc@1  80.86 ( 79.48)	Acc@3  92.58 ( 93.71)
Epoch: [6][ 40/412]	Loss 1.32 (1.326)	InvT  20.44 ( 20.43)	Acc@1  80.47 ( 79.53)	Acc@3  94.53 ( 94.02)
Epoch: [6][ 60/412]	Loss 1.369 (1.341)	InvT  20.45 ( 20.43)	Acc@1  78.12 ( 79.20)	Acc@3  93.36 ( 93.81)
Epoch: [6][ 80/412]	Loss 1.317 (1.338)	InvT  20.47 ( 20.44)	Acc@1  81.25 ( 79.16)	Acc@3  93.75 ( 93.81)
Epoch: [6][100/412]	Loss 1.357 (1.344)	InvT  20.48 ( 20.45)	Acc@1  78.12 ( 79.19)	Acc@3  93.36 ( 93.86)
Epoch: [6][120/412]	Loss 1.26 (1.35)	InvT  20.49 ( 20.45)	Acc@1  77.34 ( 79.08)	Acc@3  95.31 ( 93.90)
Epoch: [6][140/412]	Loss 1.649 (1.36)	InvT  20.50 ( 20.46)	Acc@1  71.48 ( 78.88)	Acc@3  93.36 ( 93.87)
Epoch: [6][160/412]	Loss 1.637 (1.368)	InvT  20.51 ( 20.47)	Acc@1  74.61 ( 78.74)	Acc@3  93.36 ( 93.86)
Epoch: [6][180/412]	Loss 1.473 (1.375)	InvT  20.52 ( 20.47)	Acc@1  76.17 ( 78.61)	Acc@3  93.36 ( 93.81)
Epoch: [6][200/412]	Loss 1.409 (1.376)	InvT  20.54 ( 20.48)	Acc@1  79.30 ( 78.54)	Acc@3  93.75 ( 93.82)
Epoch: [6][220/412]	Loss 1.481 (1.38)	InvT  20.55 ( 20.48)	Acc@1  79.69 ( 78.46)	Acc@3  91.02 ( 93.76)
Epoch: [6][240/412]	Loss 1.416 (1.383)	InvT  20.56 ( 20.49)	Acc@1  76.95 ( 78.38)	Acc@3  92.19 ( 93.71)
Epoch: [6][260/412]	Loss 1.484 (1.386)	InvT  20.57 ( 20.49)	Acc@1  78.12 ( 78.28)	Acc@3  95.70 ( 93.70)
Epoch: [6][280/412]	Loss 1.337 (1.392)	InvT  20.57 ( 20.50)	Acc@1  79.30 ( 78.20)	Acc@3  94.53 ( 93.62)
Epoch: [6][300/412]	Loss 1.367 (1.396)	InvT  20.58 ( 20.50)	Acc@1  80.86 ( 78.15)	Acc@3  95.70 ( 93.61)
Epoch: [6][320/412]	Loss 1.299 (1.398)	InvT  20.59 ( 20.51)	Acc@1  79.69 ( 78.13)	Acc@3  95.31 ( 93.60)
Epoch: [6][340/412]	Loss 1.772 (1.403)	InvT  20.60 ( 20.52)	Acc@1  75.39 ( 78.09)	Acc@3  89.45 ( 93.55)
Epoch: [6][360/412]	Loss 1.456 (1.404)	InvT  20.61 ( 20.52)	Acc@1  76.95 ( 78.08)	Acc@3  93.75 ( 93.52)
Epoch: [6][380/412]	Loss 1.288 (1.406)	InvT  20.62 ( 20.53)	Acc@1  78.91 ( 78.04)	Acc@3  92.97 ( 93.50)
Epoch: [6][400/412]	Loss 1.487 (1.406)	InvT  20.63 ( 20.53)	Acc@1  77.34 ( 78.01)	Acc@3  92.97 ( 93.52)
Learning rate: 2.0515463917525773e-05
Epoch 6, valid metric: {"Acc@1": 31.4, "Acc@3": 48.7, "loss": 3.345}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch6.mdl
Epoch: [7][  0/412]	Loss 1.158 (1.158)	InvT  20.64 ( 20.64)	Acc@1  80.47 ( 80.47)	Acc@3  95.70 ( 95.70)
Epoch: [7][ 20/412]	Loss 1.042 (1.133)	InvT  20.65 ( 20.64)	Acc@1  85.94 ( 82.22)	Acc@3  96.09 ( 96.00)
Epoch: [7][ 40/412]	Loss 1.173 (1.142)	InvT  20.66 ( 20.65)	Acc@1  83.20 ( 82.04)	Acc@3  96.09 ( 95.79)
Epoch: [7][ 60/412]	Loss 1.345 (1.136)	InvT  20.67 ( 20.65)	Acc@1  77.73 ( 82.38)	Acc@3  93.75 ( 95.81)
Epoch: [7][ 80/412]	Loss 1.213 (1.124)	InvT  20.68 ( 20.66)	Acc@1  80.86 ( 82.58)	Acc@3  96.09 ( 95.92)
Epoch: [7][100/412]	Loss 1.043 (1.127)	InvT  20.69 ( 20.66)	Acc@1  85.16 ( 82.46)	Acc@3  96.09 ( 95.94)
Epoch: [7][120/412]	Loss 1.226 (1.135)	InvT  20.70 ( 20.67)	Acc@1  79.30 ( 82.32)	Acc@3  96.09 ( 95.85)
Epoch: [7][140/412]	Loss 1.458 (1.137)	InvT  20.71 ( 20.68)	Acc@1  77.34 ( 82.21)	Acc@3  92.97 ( 95.85)
Epoch: [7][160/412]	Loss 1.076 (1.137)	InvT  20.72 ( 20.68)	Acc@1  82.81 ( 82.23)	Acc@3  96.88 ( 95.83)
Epoch: [7][180/412]	Loss 1.352 (1.141)	InvT  20.73 ( 20.69)	Acc@1  78.91 ( 82.15)	Acc@3  94.14 ( 95.79)
Epoch: [7][200/412]	Loss 1.098 (1.145)	InvT  20.74 ( 20.69)	Acc@1  83.20 ( 82.06)	Acc@3  96.09 ( 95.75)
Epoch: [7][220/412]	Loss 1.125 (1.147)	InvT  20.75 ( 20.70)	Acc@1  78.91 ( 82.01)	Acc@3  96.48 ( 95.75)
Epoch: [7][240/412]	Loss 1.178 (1.148)	InvT  20.76 ( 20.70)	Acc@1  81.64 ( 82.02)	Acc@3  95.70 ( 95.75)
Epoch: [7][260/412]	Loss 1.149 (1.149)	InvT  20.77 ( 20.71)	Acc@1  82.03 ( 82.00)	Acc@3  94.14 ( 95.75)
Epoch: [7][280/412]	Loss 1.372 (1.151)	InvT  20.78 ( 20.71)	Acc@1  80.08 ( 81.98)	Acc@3  93.36 ( 95.73)
Epoch: [7][300/412]	Loss 0.9559 (1.155)	InvT  20.79 ( 20.72)	Acc@1  86.33 ( 81.84)	Acc@3  96.88 ( 95.69)
Epoch: [7][320/412]	Loss 1.308 (1.158)	InvT  20.80 ( 20.72)	Acc@1  82.03 ( 81.81)	Acc@3  93.75 ( 95.66)
Epoch: [7][340/412]	Loss 1.124 (1.158)	InvT  20.81 ( 20.73)	Acc@1  82.42 ( 81.82)	Acc@3  95.70 ( 95.66)
Epoch: [7][360/412]	Loss 0.9804 (1.157)	InvT  20.82 ( 20.73)	Acc@1  84.38 ( 81.84)	Acc@3  97.27 ( 95.66)
Epoch: [7][380/412]	Loss 1.06 (1.159)	InvT  20.83 ( 20.74)	Acc@1  80.86 ( 81.82)	Acc@3  98.05 ( 95.63)
Epoch: [7][400/412]	Loss 1.179 (1.162)	InvT  20.84 ( 20.74)	Acc@1  78.52 ( 81.76)	Acc@3  96.48 ( 95.62)
Learning rate: 1.894234440626193e-05
Epoch 7, valid metric: {"Acc@1": 34.1, "Acc@3": 50.0, "loss": 3.312}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch7.mdl
Epoch: [8][  0/412]	Loss 0.9494 (0.9494)	InvT  20.84 ( 20.84)	Acc@1  85.16 ( 85.16)	Acc@3  96.48 ( 96.48)
Epoch: [8][ 20/412]	Loss 0.8824 (0.9236)	InvT  20.85 ( 20.85)	Acc@1  86.33 ( 85.60)	Acc@3  97.66 ( 97.23)
Epoch: [8][ 40/412]	Loss 0.8739 (0.9205)	InvT  20.86 ( 20.85)	Acc@1  86.72 ( 85.58)	Acc@3  98.44 ( 97.23)
Epoch: [8][ 60/412]	Loss 1.035 (0.937)	InvT  20.87 ( 20.86)	Acc@1  83.98 ( 85.43)	Acc@3  96.88 ( 97.16)
Epoch: [8][ 80/412]	Loss 0.8896 (0.9339)	InvT  20.88 ( 20.86)	Acc@1  83.59 ( 85.44)	Acc@3  98.05 ( 97.20)
Epoch: [8][100/412]	Loss 0.8742 (0.9352)	InvT  20.89 ( 20.87)	Acc@1  87.89 ( 85.34)	Acc@3  96.09 ( 97.17)
Epoch: [8][120/412]	Loss 1.008 (0.9362)	InvT  20.90 ( 20.87)	Acc@1  85.94 ( 85.34)	Acc@3  96.48 ( 97.15)
Epoch: [8][140/412]	Loss 0.8259 (0.9372)	InvT  20.91 ( 20.88)	Acc@1  88.67 ( 85.30)	Acc@3  97.66 ( 97.15)
Epoch: [8][160/412]	Loss 0.8815 (0.9431)	InvT  20.92 ( 20.88)	Acc@1  87.50 ( 85.17)	Acc@3  96.88 ( 97.13)
Epoch: [8][180/412]	Loss 0.8861 (0.9388)	InvT  20.93 ( 20.89)	Acc@1  85.55 ( 85.23)	Acc@3  98.05 ( 97.17)
Epoch: [8][200/412]	Loss 0.8284 (0.9444)	InvT  20.94 ( 20.89)	Acc@1  84.77 ( 85.12)	Acc@3  98.44 ( 97.16)
Epoch: [8][220/412]	Loss 0.8951 (0.9461)	InvT  20.95 ( 20.90)	Acc@1  86.72 ( 85.10)	Acc@3  97.27 ( 97.12)
Epoch: [8][240/412]	Loss 0.9817 (0.95)	InvT  20.96 ( 20.90)	Acc@1  82.42 ( 84.99)	Acc@3  94.53 ( 97.07)
Epoch: [8][260/412]	Loss 0.8249 (0.9531)	InvT  20.96 ( 20.90)	Acc@1  88.28 ( 84.95)	Acc@3  98.05 ( 97.04)
Epoch: [8][280/412]	Loss 0.8552 (0.954)	InvT  20.97 ( 20.91)	Acc@1  86.33 ( 84.91)	Acc@3  96.09 ( 97.01)
Epoch: [8][300/412]	Loss 0.9997 (0.9569)	InvT  20.98 ( 20.91)	Acc@1  84.38 ( 84.90)	Acc@3  96.09 ( 96.98)
Epoch: [8][320/412]	Loss 0.8885 (0.9595)	InvT  20.99 ( 20.92)	Acc@1  83.20 ( 84.85)	Acc@3  98.44 ( 96.98)
Epoch: [8][340/412]	Loss 0.9118 (0.9607)	InvT  21.00 ( 20.92)	Acc@1  84.38 ( 84.80)	Acc@3  96.48 ( 96.98)
Epoch: [8][360/412]	Loss 0.9508 (0.9643)	InvT  21.00 ( 20.93)	Acc@1  83.98 ( 84.74)	Acc@3  96.88 ( 96.94)
Epoch: [8][380/412]	Loss 0.8844 (0.966)	InvT  21.01 ( 20.93)	Acc@1  86.33 ( 84.72)	Acc@3  97.27 ( 96.93)
Epoch: [8][400/412]	Loss 1.101 (0.9683)	InvT  21.02 ( 20.94)	Acc@1  82.42 ( 84.65)	Acc@3  95.31 ( 96.91)
Learning rate: 1.736922489499809e-05
Epoch 8, valid metric: {"Acc@1": 34.4, "Acc@3": 49.5, "loss": 3.336}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch8.mdl
Epoch: [9][  0/412]	Loss 0.7333 (0.7333)	InvT  21.02 ( 21.02)	Acc@1  87.89 ( 87.89)	Acc@3  97.66 ( 97.66)
Epoch: [9][ 20/412]	Loss 0.8572 (0.7756)	InvT  21.03 ( 21.03)	Acc@1  86.33 ( 88.08)	Acc@3  97.27 ( 98.21)
Epoch: [9][ 40/412]	Loss 0.8935 (0.7894)	InvT  21.04 ( 21.03)	Acc@1  86.72 ( 87.93)	Acc@3  97.27 ( 98.11)
Epoch: [9][ 60/412]	Loss 0.7397 (0.7837)	InvT  21.05 ( 21.04)	Acc@1  89.45 ( 88.01)	Acc@3  98.83 ( 98.10)
Epoch: [9][ 80/412]	Loss 0.6985 (0.785)	InvT  21.06 ( 21.04)	Acc@1  90.23 ( 87.95)	Acc@3  99.22 ( 98.09)
Epoch: [9][100/412]	Loss 0.6971 (0.7879)	InvT  21.07 ( 21.05)	Acc@1  87.50 ( 87.84)	Acc@3  98.05 ( 98.08)
Epoch: [9][120/412]	Loss 0.796 (0.791)	InvT  21.08 ( 21.05)	Acc@1  89.06 ( 87.76)	Acc@3  98.05 ( 98.10)
Epoch: [9][140/412]	Loss 0.7937 (0.7915)	InvT  21.09 ( 21.06)	Acc@1  84.38 ( 87.67)	Acc@3  98.83 ( 98.11)
Epoch: [9][160/412]	Loss 0.8684 (0.7948)	InvT  21.09 ( 21.06)	Acc@1  85.55 ( 87.58)	Acc@3  95.70 ( 98.06)
Epoch: [9][180/412]	Loss 0.7254 (0.7934)	InvT  21.10 ( 21.06)	Acc@1  89.45 ( 87.62)	Acc@3  99.61 ( 98.04)
Epoch: [9][200/412]	Loss 1.022 (0.7939)	InvT  21.11 ( 21.07)	Acc@1  83.98 ( 87.63)	Acc@3  96.88 ( 98.06)
Epoch: [9][220/412]	Loss 0.8744 (0.7964)	InvT  21.12 ( 21.07)	Acc@1  85.55 ( 87.60)	Acc@3  96.88 ( 98.04)
Epoch: [9][240/412]	Loss 0.6494 (0.7986)	InvT  21.13 ( 21.08)	Acc@1  88.67 ( 87.54)	Acc@3  99.61 ( 98.04)
Epoch: [9][260/412]	Loss 0.6857 (0.8009)	InvT  21.13 ( 21.08)	Acc@1  90.62 ( 87.49)	Acc@3  99.22 ( 98.03)
Epoch: [9][280/412]	Loss 0.7461 (0.8034)	InvT  21.14 ( 21.09)	Acc@1  89.84 ( 87.44)	Acc@3  97.66 ( 98.00)
Epoch: [9][300/412]	Loss 0.876 (0.8054)	InvT  21.15 ( 21.09)	Acc@1  85.55 ( 87.38)	Acc@3  96.88 ( 97.97)
Epoch: [9][320/412]	Loss 0.6586 (0.8068)	InvT  21.16 ( 21.09)	Acc@1  89.06 ( 87.36)	Acc@3  98.44 ( 97.96)
Epoch: [9][340/412]	Loss 0.8749 (0.8091)	InvT  21.16 ( 21.10)	Acc@1  84.77 ( 87.32)	Acc@3  96.88 ( 97.93)
Epoch: [9][360/412]	Loss 0.7389 (0.811)	InvT  21.17 ( 21.10)	Acc@1  87.11 ( 87.27)	Acc@3  98.44 ( 97.91)
Epoch: [9][380/412]	Loss 0.8032 (0.8132)	InvT  21.18 ( 21.10)	Acc@1  87.89 ( 87.22)	Acc@3  96.09 ( 97.89)
Epoch: [9][400/412]	Loss 0.9164 (0.8133)	InvT  21.18 ( 21.11)	Acc@1  87.89 ( 87.23)	Acc@3  97.66 ( 97.89)
Learning rate: 1.5796105383734253e-05
Epoch 9, valid metric: {"Acc@1": 33.3, "Acc@3": 49.8, "loss": 3.353}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch9.mdl
Epoch: [10][  0/412]	Loss 0.6942 (0.6942)	InvT  21.19 ( 21.19)	Acc@1  91.02 ( 91.02)	Acc@3  98.83 ( 98.83)
Epoch: [10][ 20/412]	Loss 0.6539 (0.6398)	InvT  21.20 ( 21.19)	Acc@1  88.67 ( 90.23)	Acc@3  99.22 ( 98.96)
Epoch: [10][ 40/412]	Loss 0.7357 (0.6552)	InvT  21.20 ( 21.20)	Acc@1  86.72 ( 89.66)	Acc@3  99.22 ( 98.82)
Epoch: [10][ 60/412]	Loss 0.6039 (0.6679)	InvT  21.21 ( 21.20)	Acc@1  89.84 ( 89.52)	Acc@3 100.00 ( 98.78)
Epoch: [10][ 80/412]	Loss 0.8892 (0.6744)	InvT  21.22 ( 21.20)	Acc@1  83.98 ( 89.43)	Acc@3  98.83 ( 98.68)
Epoch: [10][100/412]	Loss 0.7927 (0.6822)	InvT  21.23 ( 21.21)	Acc@1  85.94 ( 89.26)	Acc@3  97.66 ( 98.68)
Epoch: [10][120/412]	Loss 0.6574 (0.6821)	InvT  21.23 ( 21.21)	Acc@1  88.67 ( 89.25)	Acc@3  98.05 ( 98.65)
Epoch: [10][140/412]	Loss 0.5236 (0.6838)	InvT  21.24 ( 21.21)	Acc@1  92.58 ( 89.19)	Acc@3  99.22 ( 98.59)
Epoch: [10][160/412]	Loss 0.6863 (0.6869)	InvT  21.25 ( 21.22)	Acc@1  86.72 ( 89.18)	Acc@3  99.61 ( 98.61)
Epoch: [10][180/412]	Loss 0.6334 (0.6869)	InvT  21.26 ( 21.22)	Acc@1  90.23 ( 89.21)	Acc@3  99.22 ( 98.63)
Epoch: [10][200/412]	Loss 0.949 (0.6896)	InvT  21.26 ( 21.23)	Acc@1  83.59 ( 89.17)	Acc@3  97.27 ( 98.61)
Epoch: [10][220/412]	Loss 0.5733 (0.6871)	InvT  21.27 ( 21.23)	Acc@1  91.41 ( 89.23)	Acc@3  98.83 ( 98.61)
Epoch: [10][240/412]	Loss 0.6086 (0.6875)	InvT  21.28 ( 21.23)	Acc@1  91.02 ( 89.21)	Acc@3  99.22 ( 98.62)
Epoch: [10][260/412]	Loss 0.5221 (0.6875)	InvT  21.28 ( 21.24)	Acc@1  93.75 ( 89.22)	Acc@3  99.22 ( 98.59)
Epoch: [10][280/412]	Loss 0.7238 (0.6893)	InvT  21.29 ( 21.24)	Acc@1  90.62 ( 89.17)	Acc@3  97.66 ( 98.57)
Epoch: [10][300/412]	Loss 0.6267 (0.69)	InvT  21.30 ( 21.24)	Acc@1  90.23 ( 89.15)	Acc@3  97.66 ( 98.56)
Epoch: [10][320/412]	Loss 0.752 (0.6907)	InvT  21.30 ( 21.25)	Acc@1  87.11 ( 89.11)	Acc@3  97.66 ( 98.54)
Epoch: [10][340/412]	Loss 0.7567 (0.6905)	InvT  21.31 ( 21.25)	Acc@1  87.89 ( 89.11)	Acc@3  97.66 ( 98.54)
Epoch: [10][360/412]	Loss 0.6298 (0.6923)	InvT  21.32 ( 21.25)	Acc@1  90.23 ( 89.05)	Acc@3  98.44 ( 98.53)
Epoch: [10][380/412]	Loss 0.8884 (0.6933)	InvT  21.32 ( 21.26)	Acc@1  85.55 ( 89.02)	Acc@3  97.27 ( 98.52)
Epoch: [10][400/412]	Loss 0.5759 (0.6942)	InvT  21.33 ( 21.26)	Acc@1  92.19 ( 89.01)	Acc@3  99.61 ( 98.52)
Learning rate: 1.4222985872470409e-05
Epoch 10, valid metric: {"Acc@1": 33.8, "Acc@3": 51.1, "loss": 3.354}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch10.mdl
Epoch: [11][  0/412]	Loss 0.5919 (0.5919)	InvT  21.33 ( 21.33)	Acc@1  91.41 ( 91.41)	Acc@3  98.83 ( 98.83)
Epoch: [11][ 20/412]	Loss 0.62 (0.6008)	InvT  21.34 ( 21.34)	Acc@1  89.84 ( 90.53)	Acc@3  98.44 ( 98.83)
Epoch: [11][ 40/412]	Loss 0.6058 (0.5786)	InvT  21.35 ( 21.34)	Acc@1  90.62 ( 90.94)	Acc@3  99.22 ( 99.08)
Epoch: [11][ 60/412]	Loss 0.4603 (0.5707)	InvT  21.35 ( 21.34)	Acc@1  92.58 ( 90.95)	Acc@3 100.00 ( 99.08)
Epoch: [11][ 80/412]	Loss 0.5389 (0.57)	InvT  21.36 ( 21.35)	Acc@1  91.80 ( 90.96)	Acc@3  98.83 ( 99.09)
Epoch: [11][100/412]	Loss 0.5318 (0.5705)	InvT  21.37 ( 21.35)	Acc@1  90.62 ( 90.93)	Acc@3  99.61 ( 99.09)
Epoch: [11][120/412]	Loss 0.4948 (0.5743)	InvT  21.37 ( 21.35)	Acc@1  92.97 ( 90.97)	Acc@3  99.22 ( 99.07)
Epoch: [11][140/412]	Loss 0.5625 (0.5804)	InvT  21.38 ( 21.36)	Acc@1  91.41 ( 90.84)	Acc@3  99.61 ( 99.05)
Epoch: [11][160/412]	Loss 0.5452 (0.5827)	InvT  21.39 ( 21.36)	Acc@1  91.80 ( 90.79)	Acc@3 100.00 ( 99.06)
Epoch: [11][180/412]	Loss 0.5459 (0.5799)	InvT  21.39 ( 21.36)	Acc@1  92.19 ( 90.87)	Acc@3  98.83 ( 99.08)
Epoch: [11][200/412]	Loss 0.5481 (0.5805)	InvT  21.40 ( 21.37)	Acc@1  91.80 ( 90.85)	Acc@3  99.61 ( 99.09)
Epoch: [11][220/412]	Loss 0.4674 (0.5813)	InvT  21.40 ( 21.37)	Acc@1  92.19 ( 90.77)	Acc@3  99.61 ( 99.08)
Epoch: [11][240/412]	Loss 0.6937 (0.5808)	InvT  21.41 ( 21.37)	Acc@1  87.89 ( 90.77)	Acc@3  98.44 ( 99.09)
Epoch: [11][260/412]	Loss 0.5889 (0.5838)	InvT  21.42 ( 21.38)	Acc@1  91.02 ( 90.74)	Acc@3  98.83 ( 99.08)
Epoch: [11][280/412]	Loss 0.6198 (0.5871)	InvT  21.42 ( 21.38)	Acc@1  91.02 ( 90.74)	Acc@3  99.22 ( 99.06)
Epoch: [11][300/412]	Loss 0.5033 (0.5875)	InvT  21.43 ( 21.38)	Acc@1  93.36 ( 90.74)	Acc@3 100.00 ( 99.06)
Epoch: [11][320/412]	Loss 0.4541 (0.5891)	InvT  21.43 ( 21.38)	Acc@1  94.53 ( 90.68)	Acc@3  99.61 ( 99.05)
Epoch: [11][340/412]	Loss 0.7887 (0.5906)	InvT  21.44 ( 21.39)	Acc@1  85.94 ( 90.65)	Acc@3  98.83 ( 99.05)
Epoch: [11][360/412]	Loss 0.462 (0.5916)	InvT  21.45 ( 21.39)	Acc@1  93.75 ( 90.66)	Acc@3  98.44 ( 99.04)
Epoch: [11][380/412]	Loss 0.6025 (0.5925)	InvT  21.45 ( 21.39)	Acc@1  89.84 ( 90.65)	Acc@3  98.83 ( 99.03)
Epoch: [11][400/412]	Loss 0.6797 (0.5919)	InvT  21.46 ( 21.40)	Acc@1  89.45 ( 90.69)	Acc@3  98.05 ( 99.04)
Learning rate: 1.2649866361206568e-05
Epoch 11, valid metric: {"Acc@1": 34.4, "Acc@3": 50.4, "loss": 3.333}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch11.mdl
Epoch: [12][  0/412]	Loss 0.5665 (0.5665)	InvT  21.46 ( 21.46)	Acc@1  90.23 ( 90.23)	Acc@3  99.61 ( 99.61)
Epoch: [12][ 20/412]	Loss 0.4074 (0.4808)	InvT  21.47 ( 21.46)	Acc@1  94.53 ( 93.04)	Acc@3 100.00 ( 99.52)
Epoch: [12][ 40/412]	Loss 0.616 (0.4947)	InvT  21.47 ( 21.47)	Acc@1  89.45 ( 92.56)	Acc@3  99.22 ( 99.43)
Epoch: [12][ 60/412]	Loss 0.5258 (0.4926)	InvT  21.48 ( 21.47)	Acc@1  92.97 ( 92.59)	Acc@3  98.83 ( 99.35)
Epoch: [12][ 80/412]	Loss 0.4948 (0.4933)	InvT  21.48 ( 21.47)	Acc@1  94.14 ( 92.64)	Acc@3  99.61 ( 99.40)
Epoch: [12][100/412]	Loss 0.5257 (0.4989)	InvT  21.49 ( 21.48)	Acc@1  91.02 ( 92.47)	Acc@3  99.61 ( 99.39)
Epoch: [12][120/412]	Loss 0.4801 (0.4977)	InvT  21.50 ( 21.48)	Acc@1  93.75 ( 92.49)	Acc@3 100.00 ( 99.41)
Epoch: [12][140/412]	Loss 0.5823 (0.4995)	InvT  21.50 ( 21.48)	Acc@1  92.19 ( 92.45)	Acc@3  99.61 ( 99.40)
Epoch: [12][160/412]	Loss 0.498 (0.5003)	InvT  21.51 ( 21.48)	Acc@1  92.58 ( 92.43)	Acc@3  99.61 ( 99.39)
Epoch: [12][180/412]	Loss 0.5151 (0.5012)	InvT  21.51 ( 21.49)	Acc@1  91.80 ( 92.46)	Acc@3  99.61 ( 99.39)
Epoch: [12][200/412]	Loss 0.5965 (0.5012)	InvT  21.52 ( 21.49)	Acc@1  91.41 ( 92.43)	Acc@3  99.22 ( 99.39)
Epoch: [12][220/412]	Loss 0.4696 (0.5033)	InvT  21.52 ( 21.49)	Acc@1  91.80 ( 92.41)	Acc@3  99.61 ( 99.37)
Epoch: [12][240/412]	Loss 0.4977 (0.5055)	InvT  21.53 ( 21.49)	Acc@1  92.19 ( 92.36)	Acc@3  99.61 ( 99.35)
Epoch: [12][260/412]	Loss 0.486 (0.5057)	InvT  21.53 ( 21.50)	Acc@1  92.19 ( 92.33)	Acc@3 100.00 ( 99.35)
Epoch: [12][280/412]	Loss 0.4969 (0.5058)	InvT  21.54 ( 21.50)	Acc@1  92.58 ( 92.31)	Acc@3 100.00 ( 99.36)
Epoch: [12][300/412]	Loss 0.5151 (0.5067)	InvT  21.54 ( 21.50)	Acc@1  91.80 ( 92.28)	Acc@3  99.22 ( 99.36)
Epoch: [12][320/412]	Loss 0.5402 (0.5079)	InvT  21.55 ( 21.51)	Acc@1  92.58 ( 92.24)	Acc@3 100.00 ( 99.36)
Epoch: [12][340/412]	Loss 0.6164 (0.51)	InvT  21.55 ( 21.51)	Acc@1  89.84 ( 92.20)	Acc@3  98.44 ( 99.36)
Epoch: [12][360/412]	Loss 0.6224 (0.5106)	InvT  21.56 ( 21.51)	Acc@1  89.06 ( 92.20)	Acc@3  98.83 ( 99.34)
Epoch: [12][380/412]	Loss 0.5147 (0.5111)	InvT  21.56 ( 21.51)	Acc@1  91.02 ( 92.18)	Acc@3  98.05 ( 99.33)
Epoch: [12][400/412]	Loss 0.6423 (0.5123)	InvT  21.57 ( 21.52)	Acc@1  89.45 ( 92.13)	Acc@3  98.05 ( 99.33)
Learning rate: 1.1076746849942727e-05
Epoch 12, valid metric: {"Acc@1": 34.5, "Acc@3": 50.7, "loss": 3.394}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch12.mdl
Epoch: [13][  0/412]	Loss 0.5359 (0.5359)	InvT  21.57 ( 21.57)	Acc@1  92.58 ( 92.58)	Acc@3  99.22 ( 99.22)
Epoch: [13][ 20/412]	Loss 0.4588 (0.4627)	InvT  21.58 ( 21.57)	Acc@1  93.75 ( 92.71)	Acc@3  99.61 ( 99.46)
Epoch: [13][ 40/412]	Loss 0.4178 (0.456)	InvT  21.58 ( 21.58)	Acc@1  92.97 ( 92.97)	Acc@3  99.61 ( 99.55)
Epoch: [13][ 60/412]	Loss 0.5718 (0.4511)	InvT  21.59 ( 21.58)	Acc@1  90.23 ( 93.00)	Acc@3  99.22 ( 99.56)
Epoch: [13][ 80/412]	Loss 0.4004 (0.4472)	InvT  21.59 ( 21.58)	Acc@1  92.19 ( 93.02)	Acc@3 100.00 ( 99.62)
Epoch: [13][100/412]	Loss 0.4164 (0.4414)	InvT  21.60 ( 21.58)	Acc@1  92.97 ( 93.12)	Acc@3  99.22 ( 99.64)
Epoch: [13][120/412]	Loss 0.4029 (0.4411)	InvT  21.60 ( 21.59)	Acc@1  94.53 ( 93.06)	Acc@3 100.00 ( 99.64)
Epoch: [13][140/412]	Loss 0.4754 (0.4471)	InvT  21.60 ( 21.59)	Acc@1  92.58 ( 92.93)	Acc@3  99.61 ( 99.59)
Epoch: [13][160/412]	Loss 0.3959 (0.4466)	InvT  21.61 ( 21.59)	Acc@1  92.97 ( 92.90)	Acc@3  98.83 ( 99.59)
Epoch: [13][180/412]	Loss 0.4155 (0.4482)	InvT  21.61 ( 21.59)	Acc@1  95.70 ( 92.90)	Acc@3  99.61 ( 99.57)
Epoch: [13][200/412]	Loss 0.5117 (0.4504)	InvT  21.62 ( 21.60)	Acc@1  92.97 ( 92.90)	Acc@3  99.22 ( 99.56)
Epoch: [13][220/412]	Loss 0.4006 (0.4499)	InvT  21.62 ( 21.60)	Acc@1  94.14 ( 92.94)	Acc@3  99.61 ( 99.56)
Epoch: [13][240/412]	Loss 0.4667 (0.4498)	InvT  21.63 ( 21.60)	Acc@1  91.80 ( 92.94)	Acc@3  99.61 ( 99.56)
Epoch: [13][260/412]	Loss 0.5432 (0.4516)	InvT  21.63 ( 21.60)	Acc@1  91.80 ( 92.88)	Acc@3 100.00 ( 99.56)
Epoch: [13][280/412]	Loss 0.4498 (0.4519)	InvT  21.64 ( 21.60)	Acc@1  94.53 ( 92.88)	Acc@3  99.22 ( 99.57)
Epoch: [13][300/412]	Loss 0.4332 (0.4524)	InvT  21.64 ( 21.61)	Acc@1  93.75 ( 92.89)	Acc@3 100.00 ( 99.57)
Epoch: [13][320/412]	Loss 0.3061 (0.4532)	InvT  21.64 ( 21.61)	Acc@1  95.70 ( 92.89)	Acc@3 100.00 ( 99.57)
Epoch: [13][340/412]	Loss 0.3865 (0.4531)	InvT  21.65 ( 21.61)	Acc@1  92.97 ( 92.89)	Acc@3 100.00 ( 99.57)
Epoch: [13][360/412]	Loss 0.4833 (0.4529)	InvT  21.65 ( 21.61)	Acc@1  92.97 ( 92.90)	Acc@3  99.22 ( 99.56)
Epoch: [13][380/412]	Loss 0.4068 (0.4536)	InvT  21.66 ( 21.62)	Acc@1  93.36 ( 92.87)	Acc@3 100.00 ( 99.56)
Epoch: [13][400/412]	Loss 0.4227 (0.4539)	InvT  21.66 ( 21.62)	Acc@1  94.14 ( 92.88)	Acc@3 100.00 ( 99.55)
Learning rate: 9.503627338678886e-06
Epoch 13, valid metric: {"Acc@1": 35.6, "Acc@3": 51.9, "loss": 3.365}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch13.mdl
Epoch: [14][  0/412]	Loss 0.3984 (0.3984)	InvT  21.66 ( 21.66)	Acc@1  93.75 ( 93.75)	Acc@3  99.22 ( 99.22)
Epoch: [14][ 20/412]	Loss 0.3739 (0.3955)	InvT  21.67 ( 21.67)	Acc@1  95.70 ( 93.86)	Acc@3  99.61 ( 99.74)
Epoch: [14][ 40/412]	Loss 0.4063 (0.3912)	InvT  21.67 ( 21.67)	Acc@1  95.31 ( 94.10)	Acc@3 100.00 ( 99.77)
Epoch: [14][ 60/412]	Loss 0.3991 (0.3924)	InvT  21.68 ( 21.67)	Acc@1  94.92 ( 94.17)	Acc@3  99.61 ( 99.71)
Epoch: [14][ 80/412]	Loss 0.3682 (0.3949)	InvT  21.68 ( 21.67)	Acc@1  96.09 ( 94.11)	Acc@3  99.61 ( 99.66)
Epoch: [14][100/412]	Loss 0.3867 (0.3952)	InvT  21.68 ( 21.67)	Acc@1  93.75 ( 94.15)	Acc@3 100.00 ( 99.66)
Epoch: [14][120/412]	Loss 0.3586 (0.3982)	InvT  21.69 ( 21.68)	Acc@1  95.70 ( 94.06)	Acc@3 100.00 ( 99.67)
Epoch: [14][140/412]	Loss 0.4568 (0.3996)	InvT  21.69 ( 21.68)	Acc@1  91.41 ( 94.04)	Acc@3  99.61 ( 99.68)
Epoch: [14][160/412]	Loss 0.4871 (0.4001)	InvT  21.69 ( 21.68)	Acc@1  91.80 ( 94.01)	Acc@3 100.00 ( 99.67)
Epoch: [14][180/412]	Loss 0.35 (0.4004)	InvT  21.70 ( 21.68)	Acc@1  95.70 ( 94.00)	Acc@3 100.00 ( 99.67)
Epoch: [14][200/412]	Loss 0.4624 (0.4008)	InvT  21.70 ( 21.68)	Acc@1  92.97 ( 94.01)	Acc@3  99.22 ( 99.67)
Epoch: [14][220/412]	Loss 0.4374 (0.4002)	InvT  21.71 ( 21.69)	Acc@1  95.31 ( 94.04)	Acc@3  99.61 ( 99.67)
Epoch: [14][240/412]	Loss 0.3834 (0.4002)	InvT  21.71 ( 21.69)	Acc@1  92.97 ( 94.02)	Acc@3 100.00 ( 99.67)
Epoch: [14][260/412]	Loss 0.3355 (0.4)	InvT  21.71 ( 21.69)	Acc@1  96.09 ( 94.03)	Acc@3 100.00 ( 99.67)
Epoch: [14][280/412]	Loss 0.4571 (0.4002)	InvT  21.72 ( 21.69)	Acc@1  93.75 ( 94.01)	Acc@3 100.00 ( 99.67)
Epoch: [14][300/412]	Loss 0.4798 (0.401)	InvT  21.72 ( 21.69)	Acc@1  92.19 ( 93.96)	Acc@3  99.61 ( 99.67)
Epoch: [14][320/412]	Loss 0.3447 (0.4016)	InvT  21.72 ( 21.69)	Acc@1  94.53 ( 93.94)	Acc@3  99.61 ( 99.67)
Epoch: [14][340/412]	Loss 0.4115 (0.4024)	InvT  21.73 ( 21.70)	Acc@1  91.80 ( 93.93)	Acc@3  99.61 ( 99.67)
Epoch: [14][360/412]	Loss 0.363 (0.4021)	InvT  21.73 ( 21.70)	Acc@1  94.14 ( 93.94)	Acc@3 100.00 ( 99.66)
Epoch: [14][380/412]	Loss 0.3539 (0.4028)	InvT  21.73 ( 21.70)	Acc@1  94.92 ( 93.93)	Acc@3  99.61 ( 99.66)
Epoch: [14][400/412]	Loss 0.3434 (0.4027)	InvT  21.74 ( 21.70)	Acc@1  94.53 ( 93.92)	Acc@3 100.00 ( 99.65)
Learning rate: 7.930507827415045e-06
Epoch 14, valid metric: {"Acc@1": 35.2, "Acc@3": 52.9, "loss": 3.379}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch14.mdl
Epoch: [15][  0/412]	Loss 0.3438 (0.3438)	InvT  21.74 ( 21.74)	Acc@1  94.53 ( 94.53)	Acc@3 100.00 (100.00)
Epoch: [15][ 20/412]	Loss 0.3304 (0.3401)	InvT  21.74 ( 21.74)	Acc@1  95.70 ( 95.05)	Acc@3 100.00 ( 99.83)
Epoch: [15][ 40/412]	Loss 0.3476 (0.3473)	InvT  21.75 ( 21.74)	Acc@1  94.92 ( 94.78)	Acc@3 100.00 ( 99.81)
Epoch: [15][ 60/412]	Loss 0.3221 (0.3537)	InvT  21.75 ( 21.75)	Acc@1  94.92 ( 94.82)	Acc@3 100.00 ( 99.80)
Epoch: [15][ 80/412]	Loss 0.411 (0.3558)	InvT  21.75 ( 21.75)	Acc@1  94.14 ( 94.73)	Acc@3  99.22 ( 99.79)
Epoch: [15][100/412]	Loss 0.2267 (0.3563)	InvT  21.76 ( 21.75)	Acc@1  97.27 ( 94.70)	Acc@3 100.00 ( 99.80)
Epoch: [15][120/412]	Loss 0.4863 (0.3586)	InvT  21.76 ( 21.75)	Acc@1  92.97 ( 94.64)	Acc@3  99.22 ( 99.78)
Epoch: [15][140/412]	Loss 0.3405 (0.3593)	InvT  21.76 ( 21.75)	Acc@1  94.92 ( 94.58)	Acc@3 100.00 ( 99.79)
Epoch: [15][160/412]	Loss 0.3231 (0.3607)	InvT  21.77 ( 21.75)	Acc@1  95.70 ( 94.55)	Acc@3 100.00 ( 99.79)
Epoch: [15][180/412]	Loss 0.2971 (0.3599)	InvT  21.77 ( 21.75)	Acc@1  96.48 ( 94.57)	Acc@3  99.61 ( 99.78)
Epoch: [15][200/412]	Loss 0.3615 (0.3603)	InvT  21.77 ( 21.76)	Acc@1  93.75 ( 94.55)	Acc@3 100.00 ( 99.78)
Epoch: [15][220/412]	Loss 0.3963 (0.3599)	InvT  21.77 ( 21.76)	Acc@1  94.53 ( 94.55)	Acc@3 100.00 ( 99.79)
Epoch: [15][240/412]	Loss 0.3217 (0.3602)	InvT  21.78 ( 21.76)	Acc@1  94.14 ( 94.56)	Acc@3  99.61 ( 99.79)
Epoch: [15][260/412]	Loss 0.3035 (0.3608)	InvT  21.78 ( 21.76)	Acc@1  94.92 ( 94.59)	Acc@3  99.22 ( 99.78)
Epoch: [15][280/412]	Loss 0.4018 (0.3604)	InvT  21.78 ( 21.76)	Acc@1  95.31 ( 94.58)	Acc@3 100.00 ( 99.78)
Epoch: [15][300/412]	Loss 0.4 (0.3609)	InvT  21.79 ( 21.76)	Acc@1  93.36 ( 94.57)	Acc@3  99.22 ( 99.78)
Epoch: [15][320/412]	Loss 0.3681 (0.3616)	InvT  21.79 ( 21.77)	Acc@1  95.70 ( 94.58)	Acc@3 100.00 ( 99.77)
Epoch: [15][340/412]	Loss 0.3656 (0.3619)	InvT  21.79 ( 21.77)	Acc@1  92.97 ( 94.56)	Acc@3 100.00 ( 99.78)
Epoch: [15][360/412]	Loss 0.4041 (0.3634)	InvT  21.79 ( 21.77)	Acc@1  94.92 ( 94.53)	Acc@3  98.83 ( 99.77)
Epoch: [15][380/412]	Loss 0.4828 (0.3632)	InvT  21.80 ( 21.77)	Acc@1  93.75 ( 94.53)	Acc@3  99.61 ( 99.77)
Epoch: [15][400/412]	Loss 0.3637 (0.3623)	InvT  21.80 ( 21.77)	Acc@1  96.09 ( 94.55)	Acc@3 100.00 ( 99.78)
Learning rate: 6.357388316151203e-06
Epoch 15, valid metric: {"Acc@1": 35.4, "Acc@3": 51.4, "loss": 3.375}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch15.mdl
Epoch: [16][  0/412]	Loss 0.3002 (0.3002)	InvT  21.80 ( 21.80)	Acc@1  96.88 ( 96.88)	Acc@3 100.00 (100.00)
Epoch: [16][ 20/412]	Loss 0.2584 (0.3213)	InvT  21.80 ( 21.80)	Acc@1  97.27 ( 95.39)	Acc@3 100.00 ( 99.85)
Epoch: [16][ 40/412]	Loss 0.3168 (0.3202)	InvT  21.81 ( 21.80)	Acc@1  95.31 ( 95.52)	Acc@3  99.61 ( 99.84)
Epoch: [16][ 60/412]	Loss 0.293 (0.3171)	InvT  21.81 ( 21.81)	Acc@1  95.70 ( 95.39)	Acc@3 100.00 ( 99.87)
Epoch: [16][ 80/412]	Loss 0.3074 (0.3169)	InvT  21.81 ( 21.81)	Acc@1  94.92 ( 95.27)	Acc@3 100.00 ( 99.88)
Epoch: [16][100/412]	Loss 0.3223 (0.3219)	InvT  21.81 ( 21.81)	Acc@1  93.75 ( 95.15)	Acc@3 100.00 ( 99.87)
Epoch: [16][120/412]	Loss 0.3611 (0.3236)	InvT  21.82 ( 21.81)	Acc@1  95.31 ( 95.15)	Acc@3  99.61 ( 99.86)
Epoch: [16][140/412]	Loss 0.2485 (0.3258)	InvT  21.82 ( 21.81)	Acc@1  97.66 ( 95.10)	Acc@3 100.00 ( 99.87)
Epoch: [16][160/412]	Loss 0.3084 (0.3239)	InvT  21.82 ( 21.81)	Acc@1  95.70 ( 95.19)	Acc@3 100.00 ( 99.87)
Epoch: [16][180/412]	Loss 0.2439 (0.3254)	InvT  21.82 ( 21.81)	Acc@1  97.66 ( 95.20)	Acc@3 100.00 ( 99.86)
Epoch: [16][200/412]	Loss 0.476 (0.3253)	InvT  21.83 ( 21.81)	Acc@1  91.41 ( 95.21)	Acc@3  99.61 ( 99.87)
Epoch: [16][220/412]	Loss 0.3053 (0.3259)	InvT  21.83 ( 21.82)	Acc@1  94.92 ( 95.18)	Acc@3  99.61 ( 99.87)
Epoch: [16][240/412]	Loss 0.3106 (0.3271)	InvT  21.83 ( 21.82)	Acc@1  95.70 ( 95.15)	Acc@3 100.00 ( 99.86)
Epoch: [16][260/412]	Loss 0.2894 (0.3278)	InvT  21.83 ( 21.82)	Acc@1  95.31 ( 95.13)	Acc@3 100.00 ( 99.85)
Epoch: [16][280/412]	Loss 0.3271 (0.3264)	InvT  21.84 ( 21.82)	Acc@1  96.09 ( 95.17)	Acc@3 100.00 ( 99.85)
Epoch: [16][300/412]	Loss 0.4305 (0.3277)	InvT  21.84 ( 21.82)	Acc@1  93.75 ( 95.15)	Acc@3 100.00 ( 99.84)
Epoch: [16][320/412]	Loss 0.3683 (0.3279)	InvT  21.84 ( 21.82)	Acc@1  94.53 ( 95.15)	Acc@3 100.00 ( 99.84)
Epoch: [16][340/412]	Loss 0.3933 (0.3292)	InvT  21.84 ( 21.82)	Acc@1  94.53 ( 95.10)	Acc@3  99.61 ( 99.84)
Epoch: [16][360/412]	Loss 0.3349 (0.3288)	InvT  21.84 ( 21.82)	Acc@1  93.36 ( 95.12)	Acc@3 100.00 ( 99.83)
Epoch: [16][380/412]	Loss 0.4045 (0.3296)	InvT  21.85 ( 21.82)	Acc@1  94.14 ( 95.11)	Acc@3  99.61 ( 99.83)
Epoch: [16][400/412]	Loss 0.3009 (0.3294)	InvT  21.85 ( 21.83)	Acc@1  94.92 ( 95.12)	Acc@3 100.00 ( 99.83)
Learning rate: 4.784268804887362e-06
Epoch 16, valid metric: {"Acc@1": 35.7, "Acc@3": 51.6, "loss": 3.39}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch16.mdl
Epoch: [17][  0/412]	Loss 0.3551 (0.3551)	InvT  21.85 ( 21.85)	Acc@1  94.92 ( 94.92)	Acc@3 100.00 (100.00)
Epoch: [17][ 20/412]	Loss 0.2302 (0.295)	InvT  21.85 ( 21.85)	Acc@1  96.48 ( 95.61)	Acc@3 100.00 ( 99.91)
Epoch: [17][ 40/412]	Loss 0.2704 (0.2985)	InvT  21.85 ( 21.85)	Acc@1  96.48 ( 95.81)	Acc@3 100.00 ( 99.88)
Epoch: [17][ 60/412]	Loss 0.2789 (0.302)	InvT  21.85 ( 21.85)	Acc@1  96.09 ( 95.66)	Acc@3  99.61 ( 99.86)
Epoch: [17][ 80/412]	Loss 0.3228 (0.3)	InvT  21.86 ( 21.85)	Acc@1  96.48 ( 95.68)	Acc@3  99.61 ( 99.86)
Epoch: [17][100/412]	Loss 0.2686 (0.3003)	InvT  21.86 ( 21.85)	Acc@1  96.09 ( 95.69)	Acc@3 100.00 ( 99.87)
Epoch: [17][120/412]	Loss 0.2869 (0.3004)	InvT  21.86 ( 21.85)	Acc@1  96.48 ( 95.63)	Acc@3 100.00 ( 99.86)
Epoch: [17][140/412]	Loss 0.414 (0.3035)	InvT  21.86 ( 21.86)	Acc@1  92.97 ( 95.53)	Acc@3 100.00 ( 99.86)
Epoch: [17][160/412]	Loss 0.2254 (0.3022)	InvT  21.86 ( 21.86)	Acc@1  97.27 ( 95.55)	Acc@3 100.00 ( 99.86)
Epoch: [17][180/412]	Loss 0.2536 (0.3021)	InvT  21.86 ( 21.86)	Acc@1  96.09 ( 95.53)	Acc@3 100.00 ( 99.86)
Epoch: [17][200/412]	Loss 0.289 (0.3034)	InvT  21.87 ( 21.86)	Acc@1  95.31 ( 95.51)	Acc@3 100.00 ( 99.85)
Epoch: [17][220/412]	Loss 0.4031 (0.3042)	InvT  21.87 ( 21.86)	Acc@1  95.31 ( 95.48)	Acc@3  99.22 ( 99.85)
Epoch: [17][240/412]	Loss 0.2426 (0.3033)	InvT  21.87 ( 21.86)	Acc@1  97.66 ( 95.50)	Acc@3 100.00 ( 99.84)
Epoch: [17][260/412]	Loss 0.3256 (0.3055)	InvT  21.87 ( 21.86)	Acc@1  96.48 ( 95.44)	Acc@3 100.00 ( 99.84)
Epoch: [17][280/412]	Loss 0.3283 (0.3049)	InvT  21.87 ( 21.86)	Acc@1  96.48 ( 95.45)	Acc@3 100.00 ( 99.84)
Epoch: [17][300/412]	Loss 0.341 (0.3047)	InvT  21.87 ( 21.86)	Acc@1  93.75 ( 95.42)	Acc@3 100.00 ( 99.85)
Epoch: [17][320/412]	Loss 0.3238 (0.3053)	InvT  21.88 ( 21.86)	Acc@1  94.14 ( 95.41)	Acc@3  99.61 ( 99.85)
Epoch: [17][340/412]	Loss 0.2564 (0.3045)	InvT  21.88 ( 21.86)	Acc@1  96.88 ( 95.43)	Acc@3 100.00 ( 99.85)
Epoch: [17][360/412]	Loss 0.206 (0.3045)	InvT  21.88 ( 21.86)	Acc@1  98.44 ( 95.42)	Acc@3 100.00 ( 99.85)
Epoch: [17][380/412]	Loss 0.3449 (0.3044)	InvT  21.88 ( 21.87)	Acc@1  94.53 ( 95.42)	Acc@3 100.00 ( 99.86)
Epoch: [17][400/412]	Loss 0.3018 (0.3044)	InvT  21.88 ( 21.87)	Acc@1  94.92 ( 95.41)	Acc@3 100.00 ( 99.85)
Learning rate: 3.2111492936235205e-06
Epoch 17, valid metric: {"Acc@1": 35.6, "Acc@3": 51.7, "loss": 3.388}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch17.mdl
Epoch: [18][  0/412]	Loss 0.3214 (0.3214)	InvT  21.88 ( 21.88)	Acc@1  95.70 ( 95.70)	Acc@3  99.61 ( 99.61)
Epoch: [18][ 20/412]	Loss 0.2886 (0.294)	InvT  21.88 ( 21.88)	Acc@1  95.70 ( 95.20)	Acc@3  99.61 ( 99.87)
Epoch: [18][ 40/412]	Loss 0.2931 (0.2889)	InvT  21.88 ( 21.88)	Acc@1  96.88 ( 95.69)	Acc@3 100.00 ( 99.90)
Epoch: [18][ 60/412]	Loss 0.301 (0.2889)	InvT  21.89 ( 21.88)	Acc@1  95.70 ( 95.59)	Acc@3 100.00 ( 99.88)
Epoch: [18][ 80/412]	Loss 0.3282 (0.2849)	InvT  21.89 ( 21.88)	Acc@1  95.31 ( 95.69)	Acc@3 100.00 ( 99.90)
Epoch: [18][100/412]	Loss 0.2975 (0.2854)	InvT  21.89 ( 21.89)	Acc@1  95.31 ( 95.75)	Acc@3 100.00 ( 99.90)
Epoch: [18][120/412]	Loss 0.2573 (0.286)	InvT  21.89 ( 21.89)	Acc@1  96.48 ( 95.75)	Acc@3 100.00 ( 99.89)
Epoch: [18][140/412]	Loss 0.2716 (0.2862)	InvT  21.89 ( 21.89)	Acc@1  96.09 ( 95.78)	Acc@3 100.00 ( 99.89)
Epoch: [18][160/412]	Loss 0.2954 (0.2867)	InvT  21.89 ( 21.89)	Acc@1  95.31 ( 95.73)	Acc@3 100.00 ( 99.90)
Epoch: [18][180/412]	Loss 0.2702 (0.2879)	InvT  21.89 ( 21.89)	Acc@1  95.31 ( 95.70)	Acc@3  99.61 ( 99.90)
Epoch: [18][200/412]	Loss 0.2959 (0.288)	InvT  21.89 ( 21.89)	Acc@1  97.27 ( 95.72)	Acc@3 100.00 ( 99.90)
Epoch: [18][220/412]	Loss 0.2293 (0.2864)	InvT  21.89 ( 21.89)	Acc@1  96.88 ( 95.75)	Acc@3  99.61 ( 99.90)
Epoch: [18][240/412]	Loss 0.4021 (0.2867)	InvT  21.90 ( 21.89)	Acc@1  94.14 ( 95.75)	Acc@3  99.61 ( 99.90)
Epoch: [18][260/412]	Loss 0.2498 (0.2868)	InvT  21.90 ( 21.89)	Acc@1  95.31 ( 95.75)	Acc@3 100.00 ( 99.90)
Epoch: [18][280/412]	Loss 0.2175 (0.2871)	InvT  21.90 ( 21.89)	Acc@1  97.66 ( 95.74)	Acc@3 100.00 ( 99.90)
Epoch: [18][300/412]	Loss 0.3414 (0.2865)	InvT  21.90 ( 21.89)	Acc@1  94.14 ( 95.73)	Acc@3  99.22 ( 99.90)
Epoch: [18][320/412]	Loss 0.2817 (0.287)	InvT  21.90 ( 21.89)	Acc@1  96.48 ( 95.73)	Acc@3  99.61 ( 99.90)
Epoch: [18][340/412]	Loss 0.2756 (0.2863)	InvT  21.90 ( 21.89)	Acc@1  94.14 ( 95.76)	Acc@3 100.00 ( 99.90)
Epoch: [18][360/412]	Loss 0.3094 (0.2872)	InvT  21.90 ( 21.89)	Acc@1  93.75 ( 95.72)	Acc@3 100.00 ( 99.90)
Epoch: [18][380/412]	Loss 0.3544 (0.2881)	InvT  21.90 ( 21.89)	Acc@1  94.53 ( 95.70)	Acc@3  99.61 ( 99.90)
Epoch: [18][400/412]	Loss 0.2531 (0.2879)	InvT  21.90 ( 21.89)	Acc@1  96.88 ( 95.72)	Acc@3 100.00 ( 99.90)
Learning rate: 1.6380297823596795e-06
Epoch 18, valid metric: {"Acc@1": 36.3, "Acc@3": 52.0, "loss": 3.373}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch18.mdl
Epoch: [19][  0/412]	Loss 0.2169 (0.2169)	InvT  21.90 ( 21.90)	Acc@1  98.05 ( 98.05)	Acc@3  99.61 ( 99.61)
Epoch: [19][ 20/412]	Loss 0.2068 (0.2669)	InvT  21.90 ( 21.90)	Acc@1  98.83 ( 96.09)	Acc@3 100.00 ( 99.93)
Epoch: [19][ 40/412]	Loss 0.2254 (0.2606)	InvT  21.90 ( 21.90)	Acc@1  96.88 ( 96.22)	Acc@3 100.00 ( 99.92)
Epoch: [19][ 60/412]	Loss 0.3103 (0.2675)	InvT  21.90 ( 21.90)	Acc@1  96.09 ( 96.18)	Acc@3 100.00 ( 99.94)
Epoch: [19][ 80/412]	Loss 0.2409 (0.2691)	InvT  21.90 ( 21.90)	Acc@1  96.48 ( 96.13)	Acc@3 100.00 ( 99.94)
Epoch: [19][100/412]	Loss 0.2549 (0.27)	InvT  21.91 ( 21.90)	Acc@1  96.48 ( 96.12)	Acc@3  99.61 ( 99.93)
Epoch: [19][120/412]	Loss 0.2245 (0.2695)	InvT  21.91 ( 21.90)	Acc@1  96.09 ( 96.10)	Acc@3 100.00 ( 99.93)
Epoch: [19][140/412]	Loss 0.2993 (0.2705)	InvT  21.91 ( 21.90)	Acc@1  94.92 ( 96.07)	Acc@3 100.00 ( 99.93)
Epoch: [19][160/412]	Loss 0.2853 (0.2711)	InvT  21.91 ( 21.90)	Acc@1  96.48 ( 96.02)	Acc@3 100.00 ( 99.93)
Epoch: [19][180/412]	Loss 0.1775 (0.271)	InvT  21.91 ( 21.90)	Acc@1  98.05 ( 96.02)	Acc@3 100.00 ( 99.93)
Epoch: [19][200/412]	Loss 0.3065 (0.2721)	InvT  21.91 ( 21.91)	Acc@1  95.31 ( 96.00)	Acc@3 100.00 ( 99.92)
Epoch: [19][220/412]	Loss 0.3699 (0.2744)	InvT  21.91 ( 21.91)	Acc@1  93.36 ( 95.96)	Acc@3 100.00 ( 99.92)
Epoch: [19][240/412]	Loss 0.256 (0.2743)	InvT  21.91 ( 21.91)	Acc@1  95.70 ( 95.97)	Acc@3 100.00 ( 99.92)
Epoch: [19][260/412]	Loss 0.2232 (0.2753)	InvT  21.91 ( 21.91)	Acc@1  96.09 ( 95.95)	Acc@3 100.00 ( 99.92)
Epoch: [19][280/412]	Loss 0.3178 (0.276)	InvT  21.91 ( 21.91)	Acc@1  97.27 ( 95.93)	Acc@3  99.61 ( 99.91)
Epoch: [19][300/412]	Loss 0.2183 (0.2753)	InvT  21.91 ( 21.91)	Acc@1  96.48 ( 95.94)	Acc@3 100.00 ( 99.91)
Epoch: [19][320/412]	Loss 0.2802 (0.2753)	InvT  21.91 ( 21.91)	Acc@1  96.88 ( 95.93)	Acc@3 100.00 ( 99.91)
Epoch: [19][340/412]	Loss 0.3468 (0.275)	InvT  21.91 ( 21.91)	Acc@1  95.31 ( 95.96)	Acc@3 100.00 ( 99.90)
Epoch: [19][360/412]	Loss 0.1786 (0.275)	InvT  21.91 ( 21.91)	Acc@1  98.83 ( 95.98)	Acc@3 100.00 ( 99.90)
Epoch: [19][380/412]	Loss 0.2639 (0.2752)	InvT  21.91 ( 21.91)	Acc@1  96.09 ( 95.97)	Acc@3 100.00 ( 99.90)
Epoch: [19][400/412]	Loss 0.2608 (0.2747)	InvT  21.91 ( 21.91)	Acc@1  96.88 ( 95.99)	Acc@3  99.61 ( 99.91)
Learning rate: 6.49102710958381e-08
Epoch 19, valid metric: {"Acc@1": 35.6, "Acc@3": 52.3, "loss": 3.384}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch19.mdl
=> creating model
CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 41286, warmup steps: 400
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 400,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 100,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 2022,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
Epoch: [0][  0/412]	Loss 12.23 (12.23)	InvT  20.00 ( 20.00)	Acc@1   0.78 (  0.78)	Acc@3   1.95 (  1.95)
Epoch: [0][ 20/412]	Loss 10.93 (11.9)	InvT  20.00 ( 20.00)	Acc@1   2.34 (  0.73)	Acc@3   6.25 (  3.61)
Epoch: [0][ 40/412]	Loss 10.22 (11.36)	InvT  20.00 ( 20.00)	Acc@1   1.17 (  0.76)	Acc@3   7.42 (  4.54)
Epoch: [0][ 60/412]	Loss 9.41 (10.81)	InvT  20.00 ( 20.00)	Acc@1   0.78 (  0.86)	Acc@3   9.38 (  6.09)
Epoch: [0][ 80/412]	Loss 8.398 (10.31)	InvT  20.00 ( 20.00)	Acc@1   4.30 (  1.39)	Acc@3  15.62 (  7.97)
Epoch: [0][100/412]	Loss 7.88 (9.858)	InvT  19.99 ( 20.00)	Acc@1   7.03 (  2.32)	Acc@3  23.05 ( 10.32)
Epoch: [0][120/412]	Loss 7.331 (9.479)	InvT  19.99 ( 20.00)	Acc@1  10.55 (  3.48)	Acc@3  25.00 ( 12.70)
Epoch: [0][140/412]	Loss 6.984 (9.155)	InvT  19.99 ( 20.00)	Acc@1  10.94 (  4.61)	Acc@3  32.42 ( 15.12)
Epoch: [0][160/412]	Loss 6.748 (8.876)	InvT  19.99 ( 20.00)	Acc@1  18.36 (  5.84)	Acc@3  35.16 ( 17.42)
Epoch: [0][180/412]	Loss 6.594 (8.632)	InvT  19.98 ( 19.99)	Acc@1  20.70 (  7.03)	Acc@3  37.89 ( 19.51)
Epoch: [0][200/412]	Loss 6.166 (8.412)	InvT  19.98 ( 19.99)	Acc@1  21.09 (  8.28)	Acc@3  42.58 ( 21.45)
Epoch: [0][220/412]	Loss 5.524 (8.208)	InvT  19.98 ( 19.99)	Acc@1  27.34 (  9.49)	Acc@3  48.83 ( 23.36)
Epoch: [0][240/412]	Loss 5.703 (8.016)	InvT  19.97 ( 19.99)	Acc@1  26.95 ( 10.70)	Acc@3  51.17 ( 25.25)
Epoch: [0][260/412]	Loss 5.622 (7.847)	InvT  19.97 ( 19.99)	Acc@1  23.44 ( 11.81)	Acc@3  47.27 ( 26.86)
Epoch: [0][280/412]	Loss 5.475 (7.689)	InvT  19.97 ( 19.99)	Acc@1  28.91 ( 12.85)	Acc@3  50.00 ( 28.37)
Epoch: [0][300/412]	Loss 5.653 (7.544)	InvT  19.96 ( 19.99)	Acc@1  26.56 ( 13.81)	Acc@3  48.44 ( 29.82)
Epoch: [0][320/412]	Loss 5.483 (7.413)	InvT  19.96 ( 19.98)	Acc@1  30.08 ( 14.75)	Acc@3  48.44 ( 31.16)
Epoch: [0][340/412]	Loss 5.348 (7.288)	InvT  19.95 ( 19.98)	Acc@1  26.17 ( 15.63)	Acc@3  52.34 ( 32.43)
Epoch: [0][360/412]	Loss 5.252 (7.173)	InvT  19.95 ( 19.98)	Acc@1  27.73 ( 16.48)	Acc@3  55.47 ( 33.57)
Epoch: [0][380/412]	Loss 5.341 (7.061)	InvT  19.94 ( 19.98)	Acc@1  28.91 ( 17.39)	Acc@3  52.34 ( 34.72)
Epoch: [0][400/412]	Loss 5.147 (6.962)	InvT  19.93 ( 19.98)	Acc@1  32.42 ( 18.18)	Acc@3  55.08 ( 35.77)
Learning rate: 2.9991195030083647e-05
Epoch 0, valid metric: {"Acc@1": 19.2, "Acc@3": 31.3, "loss": 4.019}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch0.mdl
Epoch: [1][  0/412]	Loss 4.489 (4.489)	InvT  19.93 ( 19.93)	Acc@1  40.23 ( 40.23)	Acc@3  63.28 ( 63.28)
Epoch: [1][ 20/412]	Loss 4.456 (4.635)	InvT  19.92 ( 19.92)	Acc@1  39.84 ( 38.02)	Acc@3  66.80 ( 60.53)
Epoch: [1][ 40/412]	Loss 4.396 (4.64)	InvT  19.92 ( 19.92)	Acc@1  41.41 ( 37.80)	Acc@3  64.06 ( 60.65)
Epoch: [1][ 60/412]	Loss 4.634 (4.616)	InvT  19.91 ( 19.92)	Acc@1  35.94 ( 38.08)	Acc@3  59.77 ( 60.80)
Epoch: [1][ 80/412]	Loss 4.348 (4.57)	InvT  19.91 ( 19.92)	Acc@1  40.62 ( 38.54)	Acc@3  66.02 ( 61.30)
Epoch: [1][100/412]	Loss 4.479 (4.538)	InvT  19.90 ( 19.91)	Acc@1  36.72 ( 38.84)	Acc@3  62.50 ( 61.63)
Epoch: [1][120/412]	Loss 4.598 (4.512)	InvT  19.90 ( 19.91)	Acc@1  36.33 ( 39.13)	Acc@3  58.59 ( 61.91)
Epoch: [1][140/412]	Loss 4.097 (4.485)	InvT  19.89 ( 19.91)	Acc@1  45.70 ( 39.46)	Acc@3  67.97 ( 62.21)
Epoch: [1][160/412]	Loss 4.084 (4.469)	InvT  19.88 ( 19.91)	Acc@1  46.88 ( 39.74)	Acc@3  66.02 ( 62.41)
Epoch: [1][180/412]	Loss 4.782 (4.457)	InvT  19.88 ( 19.90)	Acc@1  38.67 ( 39.89)	Acc@3  60.16 ( 62.55)
Epoch: [1][200/412]	Loss 4.151 (4.44)	InvT  19.87 ( 19.90)	Acc@1  44.53 ( 40.17)	Acc@3  68.75 ( 62.70)
Epoch: [1][220/412]	Loss 4.33 (4.416)	InvT  19.87 ( 19.90)	Acc@1  42.19 ( 40.46)	Acc@3  62.11 ( 62.89)
Epoch: [1][240/412]	Loss 4.509 (4.398)	InvT  19.86 ( 19.90)	Acc@1  39.06 ( 40.67)	Acc@3  63.28 ( 63.11)
Epoch: [1][260/412]	Loss 3.972 (4.388)	InvT  19.85 ( 19.89)	Acc@1  45.70 ( 40.83)	Acc@3  66.80 ( 63.22)
Epoch: [1][280/412]	Loss 4.334 (4.372)	InvT  19.85 ( 19.89)	Acc@1  42.97 ( 41.03)	Acc@3  66.02 ( 63.40)
Epoch: [1][300/412]	Loss 3.965 (4.352)	InvT  19.84 ( 19.89)	Acc@1  42.97 ( 41.27)	Acc@3  69.14 ( 63.61)
Epoch: [1][320/412]	Loss 3.912 (4.33)	InvT  19.84 ( 19.88)	Acc@1  44.53 ( 41.55)	Acc@3  64.45 ( 63.85)
Epoch: [1][340/412]	Loss 4.074 (4.316)	InvT  19.83 ( 19.88)	Acc@1  46.09 ( 41.72)	Acc@3  65.23 ( 64.02)
Epoch: [1][360/412]	Loss 3.787 (4.308)	InvT  19.83 ( 19.88)	Acc@1  48.44 ( 41.80)	Acc@3  69.92 ( 64.06)
Epoch: [1][380/412]	Loss 3.755 (4.289)	InvT  19.82 ( 19.87)	Acc@1  47.66 ( 41.98)	Acc@3  70.31 ( 64.26)
Epoch: [1][400/412]	Loss 4.413 (4.276)	InvT  19.81 ( 19.87)	Acc@1  42.19 ( 42.14)	Acc@3  62.89 ( 64.38)
Learning rate: 2.9688891062955536e-05
Epoch 1, valid metric: {"Acc@1": 24.1, "Acc@3": 39.8, "loss": 3.637}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch1.mdl
Epoch: [2][  0/412]	Loss 3.367 (3.367)	InvT  19.81 ( 19.81)	Acc@1  54.69 ( 54.69)	Acc@3  71.88 ( 71.88)
Epoch: [2][ 20/412]	Loss 2.993 (3.279)	InvT  19.81 ( 19.81)	Acc@1  53.12 ( 53.14)	Acc@3  78.52 ( 75.35)
Epoch: [2][ 40/412]	Loss 3.106 (3.286)	InvT  19.81 ( 19.81)	Acc@1  54.30 ( 53.30)	Acc@3  75.00 ( 75.23)
Epoch: [2][ 60/412]	Loss 3.264 (3.288)	InvT  19.81 ( 19.81)	Acc@1  51.95 ( 52.89)	Acc@3  74.22 ( 75.19)
Epoch: [2][ 80/412]	Loss 3.0 (3.291)	InvT  19.82 ( 19.81)	Acc@1  57.03 ( 52.95)	Acc@3  77.34 ( 75.03)
Epoch: [2][100/412]	Loss 3.401 (3.288)	InvT  19.82 ( 19.81)	Acc@1  51.95 ( 53.10)	Acc@3  72.27 ( 75.06)
Epoch: [2][120/412]	Loss 3.444 (3.292)	InvT  19.82 ( 19.81)	Acc@1  50.39 ( 52.93)	Acc@3  71.88 ( 75.07)
Epoch: [2][140/412]	Loss 3.358 (3.284)	InvT  19.82 ( 19.81)	Acc@1  50.39 ( 52.97)	Acc@3  73.44 ( 75.14)
Epoch: [2][160/412]	Loss 3.381 (3.297)	InvT  19.82 ( 19.82)	Acc@1  55.47 ( 52.95)	Acc@3  73.83 ( 74.98)
Epoch: [2][180/412]	Loss 3.317 (3.286)	InvT  19.82 ( 19.82)	Acc@1  53.52 ( 53.08)	Acc@3  70.31 ( 75.07)
Epoch: [2][200/412]	Loss 3.317 (3.283)	InvT  19.82 ( 19.82)	Acc@1  54.30 ( 53.19)	Acc@3  73.05 ( 75.07)
Epoch: [2][220/412]	Loss 3.338 (3.285)	InvT  19.82 ( 19.82)	Acc@1  51.95 ( 53.25)	Acc@3  73.83 ( 75.05)
Epoch: [2][240/412]	Loss 3.432 (3.281)	InvT  19.82 ( 19.82)	Acc@1  54.30 ( 53.34)	Acc@3  72.27 ( 75.08)
Epoch: [2][260/412]	Loss 3.255 (3.281)	InvT  19.82 ( 19.82)	Acc@1  53.52 ( 53.33)	Acc@3  76.17 ( 75.06)
Epoch: [2][280/412]	Loss 2.957 (3.275)	InvT  19.82 ( 19.82)	Acc@1  55.86 ( 53.42)	Acc@3  79.69 ( 75.17)
Epoch: [2][300/412]	Loss 3.235 (3.278)	InvT  19.82 ( 19.82)	Acc@1  52.34 ( 53.35)	Acc@3  75.39 ( 75.17)
Epoch: [2][320/412]	Loss 3.075 (3.279)	InvT  19.82 ( 19.82)	Acc@1  57.03 ( 53.36)	Acc@3  76.95 ( 75.13)
Epoch: [2][340/412]	Loss 3.679 (3.278)	InvT  19.82 ( 19.82)	Acc@1  49.61 ( 53.40)	Acc@3  71.88 ( 75.18)
Epoch: [2][360/412]	Loss 3.387 (3.27)	InvT  19.82 ( 19.82)	Acc@1  55.86 ( 53.50)	Acc@3  72.66 ( 75.23)
Epoch: [2][380/412]	Loss 3.317 (3.269)	InvT  19.82 ( 19.82)	Acc@1  55.47 ( 53.52)	Acc@3  74.61 ( 75.25)
Epoch: [2][400/412]	Loss 3.39 (3.266)	InvT  19.82 ( 19.82)	Acc@1  52.73 ( 53.58)	Acc@3  77.34 ( 75.28)
Learning rate: 2.938658709582742e-05
Epoch 2, valid metric: {"Acc@1": 27.2, "Acc@3": 43.3, "loss": 3.465}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch2.mdl
Epoch: [3][  0/412]	Loss 2.802 (2.802)	InvT  19.82 ( 19.82)	Acc@1  58.59 ( 58.59)	Acc@3  81.64 ( 81.64)
Epoch: [3][ 20/412]	Loss 2.534 (2.567)	InvT  19.82 ( 19.82)	Acc@1  63.67 ( 62.13)	Acc@3  80.08 ( 82.42)
Epoch: [3][ 40/412]	Loss 2.662 (2.554)	InvT  19.83 ( 19.82)	Acc@1  59.38 ( 62.44)	Acc@3  85.16 ( 82.54)
Epoch: [3][ 60/412]	Loss 2.611 (2.569)	InvT  19.84 ( 19.83)	Acc@1  65.23 ( 62.35)	Acc@3  80.47 ( 82.32)
Epoch: [3][ 80/412]	Loss 2.521 (2.565)	InvT  19.85 ( 19.83)	Acc@1  60.16 ( 62.33)	Acc@3  79.30 ( 82.42)
Epoch: [3][100/412]	Loss 2.539 (2.567)	InvT  19.86 ( 19.84)	Acc@1  62.50 ( 62.36)	Acc@3  82.81 ( 82.47)
Epoch: [3][120/412]	Loss 2.846 (2.571)	InvT  19.87 ( 19.84)	Acc@1  57.42 ( 62.29)	Acc@3  78.91 ( 82.41)
Epoch: [3][140/412]	Loss 3.011 (2.588)	InvT  19.88 ( 19.85)	Acc@1  57.81 ( 62.05)	Acc@3  78.52 ( 82.24)
Epoch: [3][160/412]	Loss 2.248 (2.586)	InvT  19.89 ( 19.85)	Acc@1  65.62 ( 62.07)	Acc@3  84.38 ( 82.30)
Epoch: [3][180/412]	Loss 2.514 (2.584)	InvT  19.89 ( 19.86)	Acc@1  59.38 ( 62.09)	Acc@3  85.94 ( 82.38)
Epoch: [3][200/412]	Loss 2.573 (2.588)	InvT  19.90 ( 19.86)	Acc@1  64.84 ( 61.99)	Acc@3  82.42 ( 82.31)
Epoch: [3][220/412]	Loss 2.868 (2.593)	InvT  19.91 ( 19.86)	Acc@1  56.25 ( 61.89)	Acc@3  78.52 ( 82.26)
Epoch: [3][240/412]	Loss 2.943 (2.596)	InvT  19.92 ( 19.87)	Acc@1  57.03 ( 61.94)	Acc@3  75.78 ( 82.16)
Epoch: [3][260/412]	Loss 2.667 (2.59)	InvT  19.92 ( 19.87)	Acc@1  58.20 ( 62.00)	Acc@3  83.20 ( 82.20)
Epoch: [3][280/412]	Loss 2.655 (2.592)	InvT  19.93 ( 19.88)	Acc@1  58.20 ( 61.99)	Acc@3  80.86 ( 82.18)
Epoch: [3][300/412]	Loss 2.75 (2.592)	InvT  19.94 ( 19.88)	Acc@1  57.03 ( 61.98)	Acc@3  82.03 ( 82.20)
Epoch: [3][320/412]	Loss 2.168 (2.594)	InvT  19.94 ( 19.88)	Acc@1  67.97 ( 62.00)	Acc@3  88.67 ( 82.17)
Epoch: [3][340/412]	Loss 2.38 (2.597)	InvT  19.95 ( 19.89)	Acc@1  66.41 ( 61.96)	Acc@3  86.72 ( 82.19)
Epoch: [3][360/412]	Loss 2.362 (2.596)	InvT  19.96 ( 19.89)	Acc@1  65.23 ( 62.02)	Acc@3  83.59 ( 82.22)
Epoch: [3][380/412]	Loss 2.593 (2.596)	InvT  19.96 ( 19.90)	Acc@1  63.28 ( 62.05)	Acc@3  82.03 ( 82.23)
Epoch: [3][400/412]	Loss 2.491 (2.592)	InvT  19.97 ( 19.90)	Acc@1  64.45 ( 62.09)	Acc@3  82.42 ( 82.26)
Learning rate: 2.908428312869931e-05
Epoch 3, valid metric: {"Acc@1": 28.8, "Acc@3": 45.6, "loss": 3.383}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch3.mdl
Epoch: [4][  0/412]	Loss 1.915 (1.915)	InvT  19.97 ( 19.97)	Acc@1  69.92 ( 69.92)	Acc@3  88.28 ( 88.28)
Epoch: [4][ 20/412]	Loss 1.86 (1.991)	InvT  19.99 ( 19.98)	Acc@1  73.05 ( 69.55)	Acc@3  87.11 ( 88.00)
Epoch: [4][ 40/412]	Loss 2.293 (2.006)	InvT  20.01 ( 19.99)	Acc@1  66.02 ( 69.09)	Acc@3  85.16 ( 88.05)
Epoch: [4][ 60/412]	Loss 2.093 (2.011)	InvT  20.02 ( 20.00)	Acc@1  66.80 ( 69.22)	Acc@3  86.72 ( 88.17)
Epoch: [4][ 80/412]	Loss 1.675 (2.017)	InvT  20.04 ( 20.01)	Acc@1  76.17 ( 69.29)	Acc@3  91.80 ( 88.14)
Epoch: [4][100/412]	Loss 2.069 (2.022)	InvT  20.05 ( 20.01)	Acc@1  69.53 ( 69.27)	Acc@3  88.28 ( 88.08)
Epoch: [4][120/412]	Loss 2.227 (2.02)	InvT  20.07 ( 20.02)	Acc@1  66.41 ( 69.41)	Acc@3  84.38 ( 88.07)
Epoch: [4][140/412]	Loss 2.097 (2.032)	InvT  20.08 ( 20.03)	Acc@1  69.14 ( 69.25)	Acc@3  87.50 ( 87.97)
Epoch: [4][160/412]	Loss 2.078 (2.038)	InvT  20.09 ( 20.04)	Acc@1  67.19 ( 69.19)	Acc@3  85.16 ( 87.90)
Epoch: [4][180/412]	Loss 2.086 (2.041)	InvT  20.11 ( 20.04)	Acc@1  67.19 ( 69.05)	Acc@3  87.89 ( 87.85)
Epoch: [4][200/412]	Loss 1.924 (2.047)	InvT  20.12 ( 20.05)	Acc@1  71.88 ( 69.00)	Acc@3  90.62 ( 87.79)
Epoch: [4][220/412]	Loss 2.018 (2.054)	InvT  20.13 ( 20.06)	Acc@1  70.31 ( 68.89)	Acc@3  90.23 ( 87.75)
Epoch: [4][240/412]	Loss 2.325 (2.068)	InvT  20.14 ( 20.06)	Acc@1  65.62 ( 68.76)	Acc@3  86.72 ( 87.64)
Epoch: [4][260/412]	Loss 2.202 (2.074)	InvT  20.15 ( 20.07)	Acc@1  65.23 ( 68.67)	Acc@3  87.11 ( 87.60)
Epoch: [4][280/412]	Loss 2.123 (2.079)	InvT  20.16 ( 20.08)	Acc@1  69.14 ( 68.61)	Acc@3  88.67 ( 87.56)
Epoch: [4][300/412]	Loss 2.202 (2.077)	InvT  20.17 ( 20.08)	Acc@1  65.23 ( 68.56)	Acc@3  88.28 ( 87.56)
Epoch: [4][320/412]	Loss 2.156 (2.08)	InvT  20.18 ( 20.09)	Acc@1  65.62 ( 68.49)	Acc@3  86.72 ( 87.52)
Epoch: [4][340/412]	Loss 1.97 (2.087)	InvT  20.19 ( 20.09)	Acc@1  71.09 ( 68.42)	Acc@3  87.89 ( 87.42)
Epoch: [4][360/412]	Loss 2.045 (2.086)	InvT  20.20 ( 20.10)	Acc@1  69.53 ( 68.46)	Acc@3  87.11 ( 87.42)
Epoch: [4][380/412]	Loss 2.283 (2.089)	InvT  20.21 ( 20.10)	Acc@1  68.75 ( 68.44)	Acc@3  86.33 ( 87.40)
Epoch: [4][400/412]	Loss 1.844 (2.09)	InvT  20.22 ( 20.11)	Acc@1  73.44 ( 68.45)	Acc@3  88.67 ( 87.37)
Learning rate: 2.87819791615712e-05
Epoch 4, valid metric: {"Acc@1": 29.1, "Acc@3": 46.2, "loss": 3.344}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch4.mdl
Epoch: [5][  0/412]	Loss 1.639 (1.639)	InvT  20.22 ( 20.22)	Acc@1  72.66 ( 72.66)	Acc@3  92.58 ( 92.58)
Epoch: [5][ 20/412]	Loss 1.506 (1.627)	InvT  20.24 ( 20.23)	Acc@1  76.56 ( 74.24)	Acc@3  92.19 ( 92.26)
Epoch: [5][ 40/412]	Loss 1.601 (1.593)	InvT  20.26 ( 20.24)	Acc@1  78.91 ( 75.64)	Acc@3  92.58 ( 92.19)
Epoch: [5][ 60/412]	Loss 1.593 (1.607)	InvT  20.28 ( 20.25)	Acc@1  75.78 ( 75.54)	Acc@3  93.75 ( 91.86)
Epoch: [5][ 80/412]	Loss 1.636 (1.624)	InvT  20.30 ( 20.26)	Acc@1  75.00 ( 75.22)	Acc@3  92.58 ( 91.76)
Epoch: [5][100/412]	Loss 1.624 (1.619)	InvT  20.32 ( 20.27)	Acc@1  74.22 ( 75.38)	Acc@3  93.75 ( 91.80)
Epoch: [5][120/412]	Loss 1.601 (1.615)	InvT  20.33 ( 20.28)	Acc@1  73.44 ( 75.35)	Acc@3  93.75 ( 91.87)
Epoch: [5][140/412]	Loss 1.573 (1.62)	InvT  20.35 ( 20.29)	Acc@1  75.00 ( 75.10)	Acc@3  92.19 ( 91.79)
Epoch: [5][160/412]	Loss 1.796 (1.629)	InvT  20.36 ( 20.30)	Acc@1  71.48 ( 74.92)	Acc@3  89.84 ( 91.68)
Epoch: [5][180/412]	Loss 1.86 (1.641)	InvT  20.38 ( 20.30)	Acc@1  71.88 ( 74.75)	Acc@3  89.84 ( 91.59)
Epoch: [5][200/412]	Loss 1.74 (1.646)	InvT  20.39 ( 20.31)	Acc@1  71.88 ( 74.66)	Acc@3  89.06 ( 91.52)
Epoch: [5][220/412]	Loss 1.91 (1.65)	InvT  20.40 ( 20.32)	Acc@1  71.48 ( 74.63)	Acc@3  88.67 ( 91.51)
Epoch: [5][240/412]	Loss 1.716 (1.655)	InvT  20.42 ( 20.33)	Acc@1  75.39 ( 74.53)	Acc@3  91.41 ( 91.45)
Epoch: [5][260/412]	Loss 1.842 (1.662)	InvT  20.43 ( 20.34)	Acc@1  72.66 ( 74.44)	Acc@3  89.84 ( 91.36)
Epoch: [5][280/412]	Loss 1.702 (1.666)	InvT  20.44 ( 20.34)	Acc@1  76.56 ( 74.40)	Acc@3  88.67 ( 91.28)
Epoch: [5][300/412]	Loss 1.763 (1.669)	InvT  20.45 ( 20.35)	Acc@1  76.17 ( 74.38)	Acc@3  91.02 ( 91.25)
Epoch: [5][320/412]	Loss 1.884 (1.671)	InvT  20.47 ( 20.36)	Acc@1  70.31 ( 74.34)	Acc@3  90.23 ( 91.23)
Epoch: [5][340/412]	Loss 1.615 (1.673)	InvT  20.48 ( 20.36)	Acc@1  73.05 ( 74.31)	Acc@3  92.19 ( 91.22)
Epoch: [5][360/412]	Loss 1.581 (1.674)	InvT  20.49 ( 20.37)	Acc@1  76.17 ( 74.28)	Acc@3  91.41 ( 91.18)
Epoch: [5][380/412]	Loss 1.518 (1.677)	InvT  20.50 ( 20.38)	Acc@1  79.30 ( 74.29)	Acc@3  92.97 ( 91.15)
Epoch: [5][400/412]	Loss 1.637 (1.679)	InvT  20.51 ( 20.38)	Acc@1  76.95 ( 74.26)	Acc@3  91.02 ( 91.11)
Learning rate: 2.8479675194443087e-05
Epoch 5, valid metric: {"Acc@1": 31.4, "Acc@3": 48.5, "loss": 3.338}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch5.mdl
Epoch: [6][  0/412]	Loss 1.22 (1.22)	InvT  20.52 ( 20.52)	Acc@1  83.20 ( 83.20)	Acc@3  95.70 ( 95.70)
Epoch: [6][ 20/412]	Loss 1.285 (1.281)	InvT  20.54 ( 20.53)	Acc@1  82.03 ( 80.36)	Acc@3  94.14 ( 94.40)
Epoch: [6][ 40/412]	Loss 1.284 (1.263)	InvT  20.56 ( 20.54)	Acc@1  80.08 ( 80.38)	Acc@3  94.53 ( 94.66)
Epoch: [6][ 60/412]	Loss 1.265 (1.277)	InvT  20.57 ( 20.55)	Acc@1  78.12 ( 80.14)	Acc@3  95.31 ( 94.45)
Epoch: [6][ 80/412]	Loss 1.251 (1.276)	InvT  20.59 ( 20.56)	Acc@1  79.69 ( 80.00)	Acc@3  94.92 ( 94.51)
Epoch: [6][100/412]	Loss 1.298 (1.282)	InvT  20.61 ( 20.57)	Acc@1  79.69 ( 80.00)	Acc@3  94.53 ( 94.50)
Epoch: [6][120/412]	Loss 1.222 (1.289)	InvT  20.63 ( 20.57)	Acc@1  77.73 ( 79.94)	Acc@3  94.53 ( 94.52)
Epoch: [6][140/412]	Loss 1.599 (1.299)	InvT  20.64 ( 20.58)	Acc@1  73.44 ( 79.76)	Acc@3  92.97 ( 94.46)
Epoch: [6][160/412]	Loss 1.551 (1.308)	InvT  20.66 ( 20.59)	Acc@1  75.78 ( 79.60)	Acc@3  92.19 ( 94.43)
Epoch: [6][180/412]	Loss 1.42 (1.318)	InvT  20.67 ( 20.60)	Acc@1  77.73 ( 79.50)	Acc@3  94.14 ( 94.36)
Epoch: [6][200/412]	Loss 1.339 (1.319)	InvT  20.68 ( 20.61)	Acc@1  81.64 ( 79.45)	Acc@3  94.53 ( 94.33)
Epoch: [6][220/412]	Loss 1.477 (1.323)	InvT  20.70 ( 20.61)	Acc@1  78.12 ( 79.34)	Acc@3  92.19 ( 94.26)
Epoch: [6][240/412]	Loss 1.376 (1.328)	InvT  20.71 ( 20.62)	Acc@1  78.12 ( 79.25)	Acc@3  92.19 ( 94.20)
Epoch: [6][260/412]	Loss 1.377 (1.332)	InvT  20.72 ( 20.63)	Acc@1  77.34 ( 79.17)	Acc@3  95.31 ( 94.18)
Epoch: [6][280/412]	Loss 1.312 (1.339)	InvT  20.74 ( 20.64)	Acc@1  78.12 ( 79.04)	Acc@3  95.31 ( 94.09)
Epoch: [6][300/412]	Loss 1.336 (1.344)	InvT  20.75 ( 20.64)	Acc@1  82.03 ( 78.99)	Acc@3  95.31 ( 94.06)
Epoch: [6][320/412]	Loss 1.243 (1.347)	InvT  20.76 ( 20.65)	Acc@1  80.86 ( 78.96)	Acc@3  94.92 ( 94.03)
Epoch: [6][340/412]	Loss 1.7 (1.352)	InvT  20.77 ( 20.66)	Acc@1  76.56 ( 78.88)	Acc@3  89.06 ( 93.97)
Epoch: [6][360/412]	Loss 1.387 (1.353)	InvT  20.78 ( 20.66)	Acc@1  79.69 ( 78.87)	Acc@3  94.53 ( 93.94)
Epoch: [6][380/412]	Loss 1.271 (1.357)	InvT  20.80 ( 20.67)	Acc@1  80.08 ( 78.80)	Acc@3  93.36 ( 93.92)
Epoch: [6][400/412]	Loss 1.446 (1.358)	InvT  20.81 ( 20.68)	Acc@1  79.30 ( 78.74)	Acc@3  92.97 ( 93.92)
Learning rate: 2.8177371227314972e-05
Epoch 6, valid metric: {"Acc@1": 32.0, "Acc@3": 48.3, "loss": 3.357}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch6.mdl
Epoch: [7][  0/412]	Loss 1.068 (1.068)	InvT  20.82 ( 20.82)	Acc@1  82.81 ( 82.81)	Acc@3  95.70 ( 95.70)
Epoch: [7][ 20/412]	Loss 0.9735 (1.057)	InvT  20.83 ( 20.82)	Acc@1  85.94 ( 83.09)	Acc@3  96.88 ( 96.60)
Epoch: [7][ 40/412]	Loss 1.111 (1.067)	InvT  20.85 ( 20.83)	Acc@1  83.20 ( 83.17)	Acc@3  96.48 ( 96.32)
Epoch: [7][ 60/412]	Loss 1.297 (1.064)	InvT  20.87 ( 20.84)	Acc@1  78.12 ( 83.38)	Acc@3  94.14 ( 96.29)
Epoch: [7][ 80/412]	Loss 1.144 (1.051)	InvT  20.88 ( 20.85)	Acc@1  81.25 ( 83.59)	Acc@3  96.88 ( 96.39)
Epoch: [7][100/412]	Loss 1.006 (1.055)	InvT  20.90 ( 20.86)	Acc@1  85.16 ( 83.49)	Acc@3  96.88 ( 96.40)
Epoch: [7][120/412]	Loss 1.17 (1.064)	InvT  20.91 ( 20.87)	Acc@1  78.91 ( 83.34)	Acc@3  97.27 ( 96.30)
Epoch: [7][140/412]	Loss 1.391 (1.065)	InvT  20.93 ( 20.87)	Acc@1  80.08 ( 83.25)	Acc@3  92.19 ( 96.29)
Epoch: [7][160/412]	Loss 1.042 (1.067)	InvT  20.94 ( 20.88)	Acc@1  83.98 ( 83.20)	Acc@3  96.88 ( 96.26)
Epoch: [7][180/412]	Loss 1.303 (1.073)	InvT  20.96 ( 20.89)	Acc@1  80.08 ( 83.04)	Acc@3  94.53 ( 96.23)
Epoch: [7][200/412]	Loss 1.02 (1.079)	InvT  20.97 ( 20.90)	Acc@1  85.16 ( 82.96)	Acc@3  96.48 ( 96.19)
Epoch: [7][220/412]	Loss 1.078 (1.08)	InvT  20.98 ( 20.90)	Acc@1  80.86 ( 82.95)	Acc@3  96.48 ( 96.22)
Epoch: [7][240/412]	Loss 1.121 (1.083)	InvT  21.00 ( 20.91)	Acc@1  82.42 ( 82.93)	Acc@3  95.70 ( 96.22)
Epoch: [7][260/412]	Loss 1.12 (1.086)	InvT  21.01 ( 20.92)	Acc@1  83.98 ( 82.88)	Acc@3  95.70 ( 96.22)
Epoch: [7][280/412]	Loss 1.343 (1.088)	InvT  21.02 ( 20.93)	Acc@1  80.08 ( 82.84)	Acc@3  92.97 ( 96.20)
Epoch: [7][300/412]	Loss 0.9103 (1.093)	InvT  21.04 ( 20.93)	Acc@1  85.94 ( 82.72)	Acc@3  95.70 ( 96.15)
Epoch: [7][320/412]	Loss 1.261 (1.097)	InvT  21.05 ( 20.94)	Acc@1  82.03 ( 82.67)	Acc@3  92.58 ( 96.10)
Epoch: [7][340/412]	Loss 1.093 (1.098)	InvT  21.06 ( 20.95)	Acc@1  81.64 ( 82.64)	Acc@3  94.92 ( 96.10)
Epoch: [7][360/412]	Loss 0.9988 (1.098)	InvT  21.07 ( 20.95)	Acc@1  83.59 ( 82.65)	Acc@3  96.09 ( 96.10)
Epoch: [7][380/412]	Loss 1.045 (1.102)	InvT  21.09 ( 20.96)	Acc@1  82.42 ( 82.58)	Acc@3  97.66 ( 96.05)
Epoch: [7][400/412]	Loss 1.122 (1.105)	InvT  21.10 ( 20.97)	Acc@1  79.30 ( 82.53)	Acc@3  95.31 ( 96.04)
Learning rate: 2.787506726018686e-05
Epoch 7, valid metric: {"Acc@1": 33.9, "Acc@3": 50.1, "loss": 3.329}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch7.mdl
Epoch: [8][  0/412]	Loss 0.8536 (0.8536)	InvT  21.11 ( 21.11)	Acc@1  85.16 ( 85.16)	Acc@3  97.66 ( 97.66)
Epoch: [8][ 20/412]	Loss 0.7756 (0.8356)	InvT  21.12 ( 21.11)	Acc@1  87.50 ( 87.30)	Acc@3  98.44 ( 97.88)
Epoch: [8][ 40/412]	Loss 0.8232 (0.8307)	InvT  21.14 ( 21.12)	Acc@1  89.06 ( 87.37)	Acc@3  97.66 ( 97.83)
Epoch: [8][ 60/412]	Loss 0.9364 (0.852)	InvT  21.15 ( 21.13)	Acc@1  86.72 ( 86.99)	Acc@3  97.27 ( 97.64)
Epoch: [8][ 80/412]	Loss 0.8111 (0.8527)	InvT  21.17 ( 21.14)	Acc@1  86.72 ( 86.92)	Acc@3  98.44 ( 97.65)
Epoch: [8][100/412]	Loss 0.7976 (0.8536)	InvT  21.18 ( 21.15)	Acc@1  88.28 ( 86.81)	Acc@3  96.09 ( 97.64)
Epoch: [8][120/412]	Loss 0.9451 (0.8564)	InvT  21.20 ( 21.15)	Acc@1  86.72 ( 86.74)	Acc@3  96.88 ( 97.60)
Epoch: [8][140/412]	Loss 0.7487 (0.8596)	InvT  21.21 ( 21.16)	Acc@1  89.06 ( 86.66)	Acc@3  98.44 ( 97.56)
Epoch: [8][160/412]	Loss 0.7956 (0.8665)	InvT  21.23 ( 21.17)	Acc@1  89.84 ( 86.51)	Acc@3  97.66 ( 97.48)
Epoch: [8][180/412]	Loss 0.8615 (0.8632)	InvT  21.24 ( 21.18)	Acc@1  87.11 ( 86.56)	Acc@3  97.66 ( 97.52)
Epoch: [8][200/412]	Loss 0.728 (0.8699)	InvT  21.25 ( 21.18)	Acc@1  87.89 ( 86.41)	Acc@3  99.61 ( 97.49)
Epoch: [8][220/412]	Loss 0.8074 (0.8726)	InvT  21.27 ( 21.19)	Acc@1  87.89 ( 86.37)	Acc@3  97.66 ( 97.47)
Epoch: [8][240/412]	Loss 0.9309 (0.8775)	InvT  21.28 ( 21.20)	Acc@1  85.55 ( 86.26)	Acc@3  94.92 ( 97.41)
Epoch: [8][260/412]	Loss 0.76 (0.8807)	InvT  21.29 ( 21.20)	Acc@1  88.28 ( 86.20)	Acc@3  98.44 ( 97.37)
Epoch: [8][280/412]	Loss 0.7945 (0.8826)	InvT  21.31 ( 21.21)	Acc@1  88.28 ( 86.13)	Acc@3  96.88 ( 97.35)
Epoch: [8][300/412]	Loss 0.9298 (0.8857)	InvT  21.32 ( 21.22)	Acc@1  85.55 ( 86.10)	Acc@3  96.48 ( 97.34)
Epoch: [8][320/412]	Loss 0.8682 (0.8898)	InvT  21.33 ( 21.22)	Acc@1  86.33 ( 86.02)	Acc@3  98.05 ( 97.32)
Epoch: [8][340/412]	Loss 0.8689 (0.8919)	InvT  21.34 ( 21.23)	Acc@1  83.59 ( 85.96)	Acc@3  96.48 ( 97.31)
Epoch: [8][360/412]	Loss 0.8608 (0.8965)	InvT  21.36 ( 21.24)	Acc@1  87.11 ( 85.87)	Acc@3  98.44 ( 97.28)
Epoch: [8][380/412]	Loss 0.842 (0.8997)	InvT  21.37 ( 21.24)	Acc@1  85.55 ( 85.82)	Acc@3  97.66 ( 97.26)
Epoch: [8][400/412]	Loss 1.03 (0.9028)	InvT  21.38 ( 21.25)	Acc@1  83.59 ( 85.75)	Acc@3  94.92 ( 97.24)
Learning rate: 2.757276329305875e-05
Epoch 8, valid metric: {"Acc@1": 34.1, "Acc@3": 49.9, "loss": 3.37}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch8.mdl
Epoch: [9][  0/412]	Loss 0.6207 (0.6207)	InvT  21.39 ( 21.39)	Acc@1  89.45 ( 89.45)	Acc@3  98.44 ( 98.44)
Epoch: [9][ 20/412]	Loss 0.7819 (0.6856)	InvT  21.40 ( 21.39)	Acc@1  87.50 ( 88.97)	Acc@3  98.05 ( 98.75)
Epoch: [9][ 40/412]	Loss 0.7832 (0.7018)	InvT  21.42 ( 21.40)	Acc@1  85.55 ( 88.77)	Acc@3  98.05 ( 98.56)
Epoch: [9][ 60/412]	Loss 0.6775 (0.6973)	InvT  21.43 ( 21.41)	Acc@1  87.89 ( 88.83)	Acc@3  98.44 ( 98.56)
Epoch: [9][ 80/412]	Loss 0.5805 (0.6993)	InvT  21.44 ( 21.42)	Acc@1  92.58 ( 88.85)	Acc@3  99.61 ( 98.52)
Epoch: [9][100/412]	Loss 0.6344 (0.7038)	InvT  21.46 ( 21.42)	Acc@1  89.06 ( 88.78)	Acc@3  98.44 ( 98.49)
Epoch: [9][120/412]	Loss 0.744 (0.7077)	InvT  21.47 ( 21.43)	Acc@1  89.45 ( 88.76)	Acc@3  97.66 ( 98.51)
Epoch: [9][140/412]	Loss 0.7374 (0.7092)	InvT  21.49 ( 21.44)	Acc@1  84.77 ( 88.74)	Acc@3  98.44 ( 98.50)
Epoch: [9][160/412]	Loss 0.812 (0.7146)	InvT  21.50 ( 21.44)	Acc@1  87.50 ( 88.64)	Acc@3  96.48 ( 98.47)
Epoch: [9][180/412]	Loss 0.6187 (0.7144)	InvT  21.51 ( 21.45)	Acc@1  91.80 ( 88.66)	Acc@3 100.00 ( 98.45)
Epoch: [9][200/412]	Loss 0.9295 (0.7142)	InvT  21.53 ( 21.46)	Acc@1  83.59 ( 88.64)	Acc@3  97.27 ( 98.44)
Epoch: [9][220/412]	Loss 0.8103 (0.7174)	InvT  21.54 ( 21.46)	Acc@1  86.72 ( 88.60)	Acc@3  97.27 ( 98.41)
Epoch: [9][240/412]	Loss 0.6037 (0.72)	InvT  21.55 ( 21.47)	Acc@1  88.67 ( 88.52)	Acc@3 100.00 ( 98.40)
Epoch: [9][260/412]	Loss 0.6328 (0.7232)	InvT  21.57 ( 21.48)	Acc@1  91.02 ( 88.49)	Acc@3  99.61 ( 98.40)
Epoch: [9][280/412]	Loss 0.7016 (0.7265)	InvT  21.58 ( 21.48)	Acc@1  89.45 ( 88.42)	Acc@3  98.05 ( 98.37)
Epoch: [9][300/412]	Loss 0.8213 (0.7293)	InvT  21.59 ( 21.49)	Acc@1  84.77 ( 88.37)	Acc@3  98.05 ( 98.34)
Epoch: [9][320/412]	Loss 0.6311 (0.7326)	InvT  21.60 ( 21.50)	Acc@1  89.45 ( 88.27)	Acc@3  98.05 ( 98.33)
Epoch: [9][340/412]	Loss 0.8187 (0.736)	InvT  21.61 ( 21.50)	Acc@1  85.55 ( 88.21)	Acc@3  98.05 ( 98.29)
Epoch: [9][360/412]	Loss 0.6768 (0.7385)	InvT  21.62 ( 21.51)	Acc@1  87.11 ( 88.14)	Acc@3  99.22 ( 98.26)
Epoch: [9][380/412]	Loss 0.7121 (0.7408)	InvT  21.64 ( 21.52)	Acc@1  89.06 ( 88.11)	Acc@3  97.27 ( 98.25)
Epoch: [9][400/412]	Loss 0.8108 (0.7419)	InvT  21.65 ( 21.52)	Acc@1  86.72 ( 88.08)	Acc@3  97.66 ( 98.23)
Learning rate: 2.7270459325930635e-05
Epoch 9, valid metric: {"Acc@1": 33.1, "Acc@3": 50.8, "loss": 3.377}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch9.mdl
Epoch: [10][  0/412]	Loss 0.5983 (0.5983)	InvT  21.66 ( 21.66)	Acc@1  90.23 ( 90.23)	Acc@3  98.83 ( 98.83)
Epoch: [10][ 20/412]	Loss 0.5709 (0.5487)	InvT  21.67 ( 21.66)	Acc@1  90.23 ( 91.29)	Acc@3  98.83 ( 99.26)
Epoch: [10][ 40/412]	Loss 0.6773 (0.5642)	InvT  21.68 ( 21.67)	Acc@1  89.06 ( 91.01)	Acc@3  99.61 ( 99.17)
Epoch: [10][ 60/412]	Loss 0.5322 (0.574)	InvT  21.70 ( 21.68)	Acc@1  90.23 ( 90.92)	Acc@3  99.61 ( 99.17)
Epoch: [10][ 80/412]	Loss 0.812 (0.5787)	InvT  21.71 ( 21.68)	Acc@1  85.16 ( 90.80)	Acc@3  97.66 ( 99.10)
Epoch: [10][100/412]	Loss 0.7028 (0.5898)	InvT  21.72 ( 21.69)	Acc@1  88.28 ( 90.60)	Acc@3  97.66 ( 99.04)
Epoch: [10][120/412]	Loss 0.5497 (0.5899)	InvT  21.74 ( 21.70)	Acc@1  91.41 ( 90.62)	Acc@3  98.44 ( 99.05)
Epoch: [10][140/412]	Loss 0.481 (0.5938)	InvT  21.75 ( 21.70)	Acc@1  94.53 ( 90.52)	Acc@3  99.61 ( 99.00)
Epoch: [10][160/412]	Loss 0.5975 (0.5977)	InvT  21.76 ( 21.71)	Acc@1  90.23 ( 90.48)	Acc@3  99.61 ( 99.00)
Epoch: [10][180/412]	Loss 0.5969 (0.5984)	InvT  21.78 ( 21.72)	Acc@1  90.62 ( 90.45)	Acc@3  99.22 ( 98.99)
Epoch: [10][200/412]	Loss 0.8093 (0.6023)	InvT  21.79 ( 21.72)	Acc@1  83.98 ( 90.40)	Acc@3  97.66 ( 98.96)
Epoch: [10][220/412]	Loss 0.5088 (0.6021)	InvT  21.80 ( 21.73)	Acc@1  91.41 ( 90.42)	Acc@3  99.22 ( 98.95)
Epoch: [10][240/412]	Loss 0.4864 (0.6034)	InvT  21.81 ( 21.74)	Acc@1  94.14 ( 90.41)	Acc@3  99.22 ( 98.94)
Epoch: [10][260/412]	Loss 0.4596 (0.6037)	InvT  21.83 ( 21.74)	Acc@1  94.92 ( 90.43)	Acc@3  99.22 ( 98.95)
Epoch: [10][280/412]	Loss 0.6483 (0.6065)	InvT  21.84 ( 21.75)	Acc@1  91.80 ( 90.37)	Acc@3  98.44 ( 98.93)
Epoch: [10][300/412]	Loss 0.5016 (0.6089)	InvT  21.85 ( 21.75)	Acc@1  92.58 ( 90.34)	Acc@3  98.83 ( 98.92)
Epoch: [10][320/412]	Loss 0.7099 (0.6109)	InvT  21.86 ( 21.76)	Acc@1  89.06 ( 90.27)	Acc@3  96.88 ( 98.88)
Epoch: [10][340/412]	Loss 0.697 (0.6111)	InvT  21.87 ( 21.77)	Acc@1  89.84 ( 90.26)	Acc@3  98.05 ( 98.88)
Epoch: [10][360/412]	Loss 0.6544 (0.6144)	InvT  21.89 ( 21.77)	Acc@1  91.41 ( 90.17)	Acc@3  98.05 ( 98.86)
Epoch: [10][380/412]	Loss 0.8021 (0.6166)	InvT  21.90 ( 21.78)	Acc@1  85.94 ( 90.11)	Acc@3  98.83 ( 98.86)
Epoch: [10][400/412]	Loss 0.4974 (0.6183)	InvT  21.91 ( 21.79)	Acc@1  92.97 ( 90.09)	Acc@3  99.61 ( 98.86)
Learning rate: 2.6968155358802524e-05
Epoch 10, valid metric: {"Acc@1": 34.0, "Acc@3": 50.2, "loss": 3.438}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch10.mdl
Epoch: [11][  0/412]	Loss 0.4879 (0.4879)	InvT  21.92 ( 21.92)	Acc@1  93.36 ( 93.36)	Acc@3  99.22 ( 99.22)
Epoch: [11][ 20/412]	Loss 0.5378 (0.5073)	InvT  21.93 ( 21.92)	Acc@1  92.58 ( 91.76)	Acc@3  99.61 ( 99.27)
Epoch: [11][ 40/412]	Loss 0.5588 (0.4964)	InvT  21.94 ( 21.93)	Acc@1  90.62 ( 92.01)	Acc@3  99.61 ( 99.42)
Epoch: [11][ 60/412]	Loss 0.3938 (0.4894)	InvT  21.95 ( 21.93)	Acc@1  96.09 ( 92.22)	Acc@3 100.00 ( 99.42)
Epoch: [11][ 80/412]	Loss 0.4462 (0.486)	InvT  21.97 ( 21.94)	Acc@1  93.75 ( 92.30)	Acc@3  99.22 ( 99.46)
Epoch: [11][100/412]	Loss 0.4399 (0.4872)	InvT  21.98 ( 21.95)	Acc@1  92.19 ( 92.24)	Acc@3  99.61 ( 99.43)
Epoch: [11][120/412]	Loss 0.4529 (0.4908)	InvT  21.99 ( 21.95)	Acc@1  94.53 ( 92.26)	Acc@3  99.61 ( 99.42)
Epoch: [11][140/412]	Loss 0.5062 (0.4952)	InvT  22.00 ( 21.96)	Acc@1  92.58 ( 92.14)	Acc@3  99.61 ( 99.41)
Epoch: [11][160/412]	Loss 0.5068 (0.4986)	InvT  22.02 ( 21.97)	Acc@1  95.31 ( 92.10)	Acc@3 100.00 ( 99.39)
Epoch: [11][180/412]	Loss 0.5038 (0.4984)	InvT  22.03 ( 21.97)	Acc@1  92.58 ( 92.10)	Acc@3  98.83 ( 99.39)
Epoch: [11][200/412]	Loss 0.4979 (0.5008)	InvT  22.04 ( 21.98)	Acc@1  92.97 ( 92.07)	Acc@3  99.61 ( 99.38)
Epoch: [11][220/412]	Loss 0.4229 (0.5017)	InvT  22.05 ( 21.98)	Acc@1  91.80 ( 92.02)	Acc@3  99.61 ( 99.36)
Epoch: [11][240/412]	Loss 0.6122 (0.502)	InvT  22.07 ( 21.99)	Acc@1  89.06 ( 92.00)	Acc@3  98.83 ( 99.36)
Epoch: [11][260/412]	Loss 0.5338 (0.5053)	InvT  22.08 ( 22.00)	Acc@1  91.80 ( 91.95)	Acc@3  98.83 ( 99.35)
Epoch: [11][280/412]	Loss 0.5406 (0.5088)	InvT  22.09 ( 22.00)	Acc@1  91.02 ( 91.92)	Acc@3  99.61 ( 99.34)
Epoch: [11][300/412]	Loss 0.4689 (0.5104)	InvT  22.10 ( 22.01)	Acc@1  91.41 ( 91.89)	Acc@3  99.61 ( 99.33)
Epoch: [11][320/412]	Loss 0.4415 (0.5133)	InvT  22.11 ( 22.02)	Acc@1  93.75 ( 91.83)	Acc@3  99.22 ( 99.32)
Epoch: [11][340/412]	Loss 0.7199 (0.5152)	InvT  22.12 ( 22.02)	Acc@1  85.94 ( 91.81)	Acc@3  99.22 ( 99.31)
Epoch: [11][360/412]	Loss 0.398 (0.5166)	InvT  22.14 ( 22.03)	Acc@1  94.14 ( 91.78)	Acc@3  99.22 ( 99.30)
Epoch: [11][380/412]	Loss 0.581 (0.5183)	InvT  22.15 ( 22.03)	Acc@1  90.62 ( 91.77)	Acc@3  99.22 ( 99.29)
Epoch: [11][400/412]	Loss 0.6979 (0.519)	InvT  22.16 ( 22.04)	Acc@1  89.84 ( 91.76)	Acc@3  97.66 ( 99.28)
Learning rate: 2.6665851391674412e-05
Epoch 11, valid metric: {"Acc@1": 33.9, "Acc@3": 50.2, "loss": 3.446}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch11.mdl
Epoch: [12][  0/412]	Loss 0.4658 (0.4658)	InvT  22.17 ( 22.17)	Acc@1  92.58 ( 92.58)	Acc@3  99.22 ( 99.22)
Epoch: [12][ 20/412]	Loss 0.3652 (0.3906)	InvT  22.18 ( 22.17)	Acc@1  94.53 ( 94.33)	Acc@3  99.61 ( 99.76)
Epoch: [12][ 40/412]	Loss 0.4789 (0.4062)	InvT  22.19 ( 22.18)	Acc@1  92.19 ( 93.84)	Acc@3  99.22 ( 99.67)
Epoch: [12][ 60/412]	Loss 0.4574 (0.4046)	InvT  22.20 ( 22.18)	Acc@1  94.14 ( 93.88)	Acc@3  98.83 ( 99.66)
Epoch: [12][ 80/412]	Loss 0.3914 (0.4053)	InvT  22.21 ( 22.19)	Acc@1  95.70 ( 93.96)	Acc@3  99.22 ( 99.69)
Epoch: [12][100/412]	Loss 0.4461 (0.4104)	InvT  22.22 ( 22.20)	Acc@1  92.97 ( 93.75)	Acc@3  99.61 ( 99.67)
Epoch: [12][120/412]	Loss 0.4415 (0.4113)	InvT  22.24 ( 22.20)	Acc@1  92.97 ( 93.72)	Acc@3 100.00 ( 99.65)
Epoch: [12][140/412]	Loss 0.4705 (0.4135)	InvT  22.25 ( 22.21)	Acc@1  93.75 ( 93.64)	Acc@3  99.61 ( 99.65)
Epoch: [12][160/412]	Loss 0.495 (0.4143)	InvT  22.26 ( 22.21)	Acc@1  91.80 ( 93.61)	Acc@3  99.61 ( 99.63)
Epoch: [12][180/412]	Loss 0.4405 (0.4157)	InvT  22.27 ( 22.22)	Acc@1  91.80 ( 93.61)	Acc@3  99.61 ( 99.62)
Epoch: [12][200/412]	Loss 0.5712 (0.4177)	InvT  22.28 ( 22.22)	Acc@1  90.62 ( 93.58)	Acc@3  98.05 ( 99.61)
Epoch: [12][220/412]	Loss 0.3692 (0.4206)	InvT  22.30 ( 22.23)	Acc@1  95.31 ( 93.55)	Acc@3  99.22 ( 99.59)
Epoch: [12][240/412]	Loss 0.4058 (0.4232)	InvT  22.31 ( 22.24)	Acc@1  93.75 ( 93.47)	Acc@3  99.61 ( 99.59)
Epoch: [12][260/412]	Loss 0.4288 (0.4237)	InvT  22.32 ( 22.24)	Acc@1  93.36 ( 93.44)	Acc@3 100.00 ( 99.58)
Epoch: [12][280/412]	Loss 0.441 (0.4248)	InvT  22.33 ( 22.25)	Acc@1  91.80 ( 93.40)	Acc@3  99.61 ( 99.58)
Epoch: [12][300/412]	Loss 0.4435 (0.4262)	InvT  22.34 ( 22.25)	Acc@1  94.92 ( 93.37)	Acc@3  98.44 ( 99.57)
Epoch: [12][320/412]	Loss 0.4618 (0.4275)	InvT  22.35 ( 22.26)	Acc@1  92.97 ( 93.35)	Acc@3 100.00 ( 99.57)
Epoch: [12][340/412]	Loss 0.5587 (0.4306)	InvT  22.36 ( 22.27)	Acc@1  92.58 ( 93.29)	Acc@3  98.83 ( 99.57)
Epoch: [12][360/412]	Loss 0.5883 (0.4319)	InvT  22.38 ( 22.27)	Acc@1  90.62 ( 93.26)	Acc@3  98.83 ( 99.55)
Epoch: [12][380/412]	Loss 0.4824 (0.4338)	InvT  22.39 ( 22.28)	Acc@1  91.80 ( 93.23)	Acc@3  99.22 ( 99.54)
Epoch: [12][400/412]	Loss 0.5785 (0.4359)	InvT  22.40 ( 22.28)	Acc@1  90.23 ( 93.17)	Acc@3  98.83 ( 99.54)
Learning rate: 2.63635474245463e-05
Epoch 12, valid metric: {"Acc@1": 35.4, "Acc@3": 49.5, "loss": 3.499}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch12.mdl
Epoch: [13][  0/412]	Loss 0.4526 (0.4526)	InvT  22.40 ( 22.40)	Acc@1  93.36 ( 93.36)	Acc@3  99.61 ( 99.61)
Epoch: [13][ 20/412]	Loss 0.4099 (0.377)	InvT  22.42 ( 22.41)	Acc@1  94.14 ( 93.75)	Acc@3  99.61 ( 99.72)
Epoch: [13][ 40/412]	Loss 0.3883 (0.3709)	InvT  22.43 ( 22.42)	Acc@1  92.97 ( 94.02)	Acc@3  99.61 ( 99.76)
Epoch: [13][ 60/412]	Loss 0.4653 (0.3677)	InvT  22.44 ( 22.42)	Acc@1  89.84 ( 93.97)	Acc@3 100.00 ( 99.74)
Epoch: [13][ 80/412]	Loss 0.3155 (0.3619)	InvT  22.45 ( 22.43)	Acc@1  94.53 ( 94.19)	Acc@3 100.00 ( 99.78)
Epoch: [13][100/412]	Loss 0.3045 (0.358)	InvT  22.46 ( 22.43)	Acc@1  93.36 ( 94.33)	Acc@3  99.61 ( 99.78)
Epoch: [13][120/412]	Loss 0.3091 (0.3597)	InvT  22.47 ( 22.44)	Acc@1  95.31 ( 94.27)	Acc@3 100.00 ( 99.78)
Epoch: [13][140/412]	Loss 0.3732 (0.3632)	InvT  22.48 ( 22.44)	Acc@1  93.75 ( 94.21)	Acc@3  99.61 ( 99.76)
Epoch: [13][160/412]	Loss 0.2947 (0.3644)	InvT  22.49 ( 22.45)	Acc@1  95.70 ( 94.17)	Acc@3  99.61 ( 99.74)
Epoch: [13][180/412]	Loss 0.3535 (0.3667)	InvT  22.50 ( 22.45)	Acc@1  95.31 ( 94.13)	Acc@3  99.22 ( 99.73)
Epoch: [13][200/412]	Loss 0.4145 (0.3688)	InvT  22.51 ( 22.46)	Acc@1  94.53 ( 94.14)	Acc@3  99.61 ( 99.71)
Epoch: [13][220/412]	Loss 0.3586 (0.3687)	InvT  22.53 ( 22.47)	Acc@1  93.75 ( 94.15)	Acc@3  99.61 ( 99.70)
Epoch: [13][240/412]	Loss 0.4237 (0.3689)	InvT  22.54 ( 22.47)	Acc@1  91.41 ( 94.16)	Acc@3  99.61 ( 99.69)
Epoch: [13][260/412]	Loss 0.4688 (0.3713)	InvT  22.55 ( 22.48)	Acc@1  91.80 ( 94.11)	Acc@3  99.61 ( 99.70)
Epoch: [13][280/412]	Loss 0.3299 (0.3714)	InvT  22.56 ( 22.48)	Acc@1  96.09 ( 94.08)	Acc@3  99.61 ( 99.70)
Epoch: [13][300/412]	Loss 0.325 (0.3721)	InvT  22.57 ( 22.49)	Acc@1  96.09 ( 94.09)	Acc@3 100.00 ( 99.70)
Epoch: [13][320/412]	Loss 0.2472 (0.3735)	InvT  22.58 ( 22.49)	Acc@1  96.88 ( 94.06)	Acc@3 100.00 ( 99.69)
Epoch: [13][340/412]	Loss 0.362 (0.3747)	InvT  22.59 ( 22.50)	Acc@1  94.92 ( 94.04)	Acc@3 100.00 ( 99.69)
Epoch: [13][360/412]	Loss 0.4264 (0.3752)	InvT  22.60 ( 22.50)	Acc@1  91.80 ( 94.02)	Acc@3  99.61 ( 99.69)
Epoch: [13][380/412]	Loss 0.3206 (0.3765)	InvT  22.61 ( 22.51)	Acc@1  95.70 ( 93.99)	Acc@3  99.61 ( 99.68)
Epoch: [13][400/412]	Loss 0.3439 (0.3775)	InvT  22.62 ( 22.51)	Acc@1  94.14 ( 93.98)	Acc@3 100.00 ( 99.68)
Learning rate: 2.6061243457418186e-05
Epoch 13, valid metric: {"Acc@1": 35.1, "Acc@3": 51.5, "loss": 3.496}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch13.mdl
Epoch: [14][  0/412]	Loss 0.3067 (0.3067)	InvT  22.63 ( 22.63)	Acc@1  94.53 ( 94.53)	Acc@3  98.83 ( 98.83)
Epoch: [14][ 20/412]	Loss 0.281 (0.3128)	InvT  22.64 ( 22.63)	Acc@1  97.27 ( 95.18)	Acc@3  99.61 ( 99.78)
Epoch: [14][ 40/412]	Loss 0.2789 (0.3053)	InvT  22.65 ( 22.64)	Acc@1  96.48 ( 95.28)	Acc@3 100.00 ( 99.80)
Epoch: [14][ 60/412]	Loss 0.3012 (0.3065)	InvT  22.66 ( 22.65)	Acc@1  94.92 ( 95.29)	Acc@3 100.00 ( 99.80)
Epoch: [14][ 80/412]	Loss 0.337 (0.3093)	InvT  22.67 ( 22.65)	Acc@1  94.92 ( 95.27)	Acc@3 100.00 ( 99.80)
Epoch: [14][100/412]	Loss 0.3227 (0.3083)	InvT  22.68 ( 22.66)	Acc@1  95.31 ( 95.28)	Acc@3  99.61 ( 99.80)
Epoch: [14][120/412]	Loss 0.2906 (0.3112)	InvT  22.69 ( 22.66)	Acc@1  96.48 ( 95.17)	Acc@3 100.00 ( 99.79)
Epoch: [14][140/412]	Loss 0.4224 (0.3142)	InvT  22.70 ( 22.67)	Acc@1  94.53 ( 95.13)	Acc@3  99.61 ( 99.78)
Epoch: [14][160/412]	Loss 0.3377 (0.3141)	InvT  22.71 ( 22.67)	Acc@1  93.36 ( 95.06)	Acc@3 100.00 ( 99.79)
Epoch: [14][180/412]	Loss 0.2795 (0.3152)	InvT  22.72 ( 22.68)	Acc@1  96.88 ( 95.04)	Acc@3 100.00 ( 99.79)
Epoch: [14][200/412]	Loss 0.3218 (0.3177)	InvT  22.73 ( 22.68)	Acc@1  94.92 ( 95.03)	Acc@3  99.61 ( 99.78)
Epoch: [14][220/412]	Loss 0.3473 (0.3179)	InvT  22.74 ( 22.69)	Acc@1  96.09 ( 95.05)	Acc@3 100.00 ( 99.79)
Epoch: [14][240/412]	Loss 0.3154 (0.3176)	InvT  22.75 ( 22.69)	Acc@1  94.92 ( 95.04)	Acc@3  99.61 ( 99.78)
Epoch: [14][260/412]	Loss 0.2898 (0.318)	InvT  22.76 ( 22.70)	Acc@1  96.09 ( 95.02)	Acc@3 100.00 ( 99.78)
Epoch: [14][280/412]	Loss 0.3836 (0.3189)	InvT  22.78 ( 22.70)	Acc@1  94.53 ( 95.00)	Acc@3 100.00 ( 99.78)
Epoch: [14][300/412]	Loss 0.3905 (0.3201)	InvT  22.79 ( 22.71)	Acc@1  92.58 ( 94.98)	Acc@3  99.22 ( 99.78)
Epoch: [14][320/412]	Loss 0.3014 (0.3212)	InvT  22.80 ( 22.71)	Acc@1  93.75 ( 94.94)	Acc@3  99.22 ( 99.78)
Epoch: [14][340/412]	Loss 0.3496 (0.323)	InvT  22.81 ( 22.72)	Acc@1  94.53 ( 94.90)	Acc@3  99.61 ( 99.78)
Epoch: [14][360/412]	Loss 0.3194 (0.3237)	InvT  22.82 ( 22.72)	Acc@1  94.92 ( 94.89)	Acc@3 100.00 ( 99.77)
Epoch: [14][380/412]	Loss 0.3166 (0.3252)	InvT  22.83 ( 22.73)	Acc@1  94.92 ( 94.86)	Acc@3  99.61 ( 99.77)
Epoch: [14][400/412]	Loss 0.2801 (0.3258)	InvT  22.84 ( 22.73)	Acc@1  95.31 ( 94.86)	Acc@3  99.61 ( 99.76)
Learning rate: 2.5758939490290075e-05
Epoch 14, valid metric: {"Acc@1": 35.1, "Acc@3": 51.1, "loss": 3.527}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch14.mdl
Epoch: [15][  0/412]	Loss 0.2453 (0.2453)	InvT  22.85 ( 22.85)	Acc@1  95.70 ( 95.70)	Acc@3 100.00 (100.00)
Epoch: [15][ 20/412]	Loss 0.2738 (0.253)	InvT  22.86 ( 22.85)	Acc@1  95.70 ( 96.34)	Acc@3  99.61 ( 99.85)
Epoch: [15][ 40/412]	Loss 0.2131 (0.2581)	InvT  22.87 ( 22.86)	Acc@1  96.88 ( 96.12)	Acc@3 100.00 ( 99.88)
Epoch: [15][ 60/412]	Loss 0.25 (0.263)	InvT  22.88 ( 22.86)	Acc@1  96.09 ( 96.04)	Acc@3 100.00 ( 99.90)
Epoch: [15][ 80/412]	Loss 0.3261 (0.2671)	InvT  22.89 ( 22.87)	Acc@1  94.92 ( 95.93)	Acc@3  99.22 ( 99.88)
Epoch: [15][100/412]	Loss 0.1886 (0.2681)	InvT  22.89 ( 22.87)	Acc@1  96.88 ( 95.88)	Acc@3 100.00 ( 99.88)
Epoch: [15][120/412]	Loss 0.3976 (0.2705)	InvT  22.90 ( 22.88)	Acc@1  96.09 ( 95.90)	Acc@3 100.00 ( 99.88)
Epoch: [15][140/412]	Loss 0.2786 (0.273)	InvT  22.91 ( 22.88)	Acc@1  94.53 ( 95.83)	Acc@3 100.00 ( 99.88)
Epoch: [15][160/412]	Loss 0.2727 (0.2745)	InvT  22.92 ( 22.89)	Acc@1  94.92 ( 95.79)	Acc@3 100.00 ( 99.88)
Epoch: [15][180/412]	Loss 0.2297 (0.2748)	InvT  22.93 ( 22.89)	Acc@1  98.05 ( 95.80)	Acc@3 100.00 ( 99.87)
Epoch: [15][200/412]	Loss 0.2957 (0.2761)	InvT  22.94 ( 22.89)	Acc@1  96.88 ( 95.81)	Acc@3 100.00 ( 99.87)
Epoch: [15][220/412]	Loss 0.2963 (0.277)	InvT  22.95 ( 22.90)	Acc@1  95.70 ( 95.79)	Acc@3 100.00 ( 99.87)
Epoch: [15][240/412]	Loss 0.2852 (0.2783)	InvT  22.96 ( 22.90)	Acc@1  93.36 ( 95.76)	Acc@3  99.61 ( 99.87)
Epoch: [15][260/412]	Loss 0.2354 (0.2799)	InvT  22.97 ( 22.91)	Acc@1  94.92 ( 95.73)	Acc@3 100.00 ( 99.86)
Epoch: [15][280/412]	Loss 0.2937 (0.2797)	InvT  22.98 ( 22.91)	Acc@1  96.88 ( 95.75)	Acc@3 100.00 ( 99.87)
Epoch: [15][300/412]	Loss 0.3106 (0.281)	InvT  22.99 ( 22.92)	Acc@1  94.14 ( 95.73)	Acc@3  99.22 ( 99.87)
Epoch: [15][320/412]	Loss 0.2984 (0.2825)	InvT  23.00 ( 22.92)	Acc@1  94.92 ( 95.71)	Acc@3  99.61 ( 99.87)
Epoch: [15][340/412]	Loss 0.3476 (0.2835)	InvT  23.01 ( 22.93)	Acc@1  92.97 ( 95.70)	Acc@3 100.00 ( 99.86)
Epoch: [15][360/412]	Loss 0.3296 (0.2855)	InvT  23.02 ( 22.93)	Acc@1  95.31 ( 95.67)	Acc@3  99.22 ( 99.85)
Epoch: [15][380/412]	Loss 0.4212 (0.2861)	InvT  23.03 ( 22.94)	Acc@1  94.92 ( 95.67)	Acc@3  99.22 ( 99.85)
Epoch: [15][400/412]	Loss 0.2602 (0.2856)	InvT  23.04 ( 22.94)	Acc@1  96.88 ( 95.67)	Acc@3 100.00 ( 99.85)
Learning rate: 2.5456635523161963e-05
Epoch 15, valid metric: {"Acc@1": 36.7, "Acc@3": 52.1, "loss": 3.488}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch15.mdl
Epoch: [16][  0/412]	Loss 0.2469 (0.2469)	InvT  23.05 ( 23.05)	Acc@1  97.66 ( 97.66)	Acc@3  99.61 ( 99.61)
Epoch: [16][ 20/412]	Loss 0.2282 (0.2354)	InvT  23.06 ( 23.06)	Acc@1  95.70 ( 96.65)	Acc@3 100.00 ( 99.93)
Epoch: [16][ 40/412]	Loss 0.2583 (0.2363)	InvT  23.07 ( 23.06)	Acc@1  96.88 ( 96.59)	Acc@3  99.61 ( 99.91)
Epoch: [16][ 60/412]	Loss 0.2202 (0.2352)	InvT  23.08 ( 23.07)	Acc@1  96.09 ( 96.53)	Acc@3  99.22 ( 99.88)
Epoch: [16][ 80/412]	Loss 0.2067 (0.2367)	InvT  23.09 ( 23.07)	Acc@1  96.48 ( 96.43)	Acc@3 100.00 ( 99.90)
Epoch: [16][100/412]	Loss 0.2072 (0.2405)	InvT  23.10 ( 23.08)	Acc@1  96.88 ( 96.33)	Acc@3 100.00 ( 99.90)
Epoch: [16][120/412]	Loss 0.2726 (0.242)	InvT  23.11 ( 23.08)	Acc@1  95.70 ( 96.28)	Acc@3 100.00 ( 99.91)
Epoch: [16][140/412]	Loss 0.2101 (0.2449)	InvT  23.12 ( 23.08)	Acc@1  97.27 ( 96.22)	Acc@3  99.61 ( 99.91)
Epoch: [16][160/412]	Loss 0.2172 (0.2439)	InvT  23.13 ( 23.09)	Acc@1  96.88 ( 96.22)	Acc@3 100.00 ( 99.91)
Epoch: [16][180/412]	Loss 0.1931 (0.2453)	InvT  23.14 ( 23.09)	Acc@1  98.05 ( 96.21)	Acc@3 100.00 ( 99.91)
Epoch: [16][200/412]	Loss 0.3966 (0.2459)	InvT  23.15 ( 23.10)	Acc@1  91.41 ( 96.19)	Acc@3  99.22 ( 99.91)
Epoch: [16][220/412]	Loss 0.2285 (0.2466)	InvT  23.16 ( 23.10)	Acc@1  98.05 ( 96.18)	Acc@3  99.61 ( 99.91)
Epoch: [16][240/412]	Loss 0.2261 (0.2475)	InvT  23.17 ( 23.11)	Acc@1  96.09 ( 96.17)	Acc@3 100.00 ( 99.91)
Epoch: [16][260/412]	Loss 0.2591 (0.249)	InvT  23.18 ( 23.11)	Acc@1  96.48 ( 96.14)	Acc@3  99.61 ( 99.90)
Epoch: [16][280/412]	Loss 0.2443 (0.2491)	InvT  23.19 ( 23.12)	Acc@1  96.88 ( 96.13)	Acc@3 100.00 ( 99.89)
Epoch: [16][300/412]	Loss 0.332 (0.251)	InvT  23.19 ( 23.12)	Acc@1  93.75 ( 96.11)	Acc@3 100.00 ( 99.88)
Epoch: [16][320/412]	Loss 0.2957 (0.2521)	InvT  23.20 ( 23.13)	Acc@1  94.92 ( 96.10)	Acc@3  99.61 ( 99.88)
Epoch: [16][340/412]	Loss 0.3369 (0.2542)	InvT  23.21 ( 23.13)	Acc@1  93.75 ( 96.07)	Acc@3  99.61 ( 99.88)
Epoch: [16][360/412]	Loss 0.2387 (0.254)	InvT  23.22 ( 23.14)	Acc@1  96.48 ( 96.10)	Acc@3 100.00 ( 99.87)
Epoch: [16][380/412]	Loss 0.281 (0.2549)	InvT  23.23 ( 23.14)	Acc@1  96.48 ( 96.09)	Acc@3  99.61 ( 99.87)
Epoch: [16][400/412]	Loss 0.3029 (0.2557)	InvT  23.24 ( 23.15)	Acc@1  95.31 ( 96.09)	Acc@3 100.00 ( 99.87)
Learning rate: 2.515433155603385e-05
Epoch 16, valid metric: {"Acc@1": 36.0, "Acc@3": 51.8, "loss": 3.51}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch16.mdl
Epoch: [17][  0/412]	Loss 0.2422 (0.2422)	InvT  23.25 ( 23.25)	Acc@1  96.09 ( 96.09)	Acc@3 100.00 (100.00)
Epoch: [17][ 20/412]	Loss 0.1862 (0.2111)	InvT  23.26 ( 23.26)	Acc@1  97.27 ( 96.86)	Acc@3 100.00 ( 99.96)
Epoch: [17][ 40/412]	Loss 0.205 (0.2106)	InvT  23.27 ( 23.26)	Acc@1  97.27 ( 97.09)	Acc@3 100.00 ( 99.93)
Epoch: [17][ 60/412]	Loss 0.1745 (0.2145)	InvT  23.28 ( 23.26)	Acc@1  98.05 ( 96.95)	Acc@3 100.00 ( 99.95)
Epoch: [17][ 80/412]	Loss 0.2528 (0.2145)	InvT  23.29 ( 23.27)	Acc@1  96.88 ( 96.89)	Acc@3 100.00 ( 99.95)
Epoch: [17][100/412]	Loss 0.172 (0.2139)	InvT  23.30 ( 23.27)	Acc@1  97.66 ( 96.85)	Acc@3 100.00 ( 99.95)
Epoch: [17][120/412]	Loss 0.1908 (0.2143)	InvT  23.31 ( 23.28)	Acc@1  97.27 ( 96.79)	Acc@3 100.00 ( 99.94)
Epoch: [17][140/412]	Loss 0.3312 (0.2176)	InvT  23.32 ( 23.28)	Acc@1  93.75 ( 96.71)	Acc@3 100.00 ( 99.94)
Epoch: [17][160/412]	Loss 0.1531 (0.2193)	InvT  23.32 ( 23.29)	Acc@1  98.05 ( 96.64)	Acc@3 100.00 ( 99.94)
Epoch: [17][180/412]	Loss 0.1837 (0.2196)	InvT  23.33 ( 23.29)	Acc@1  98.44 ( 96.65)	Acc@3 100.00 ( 99.93)
Epoch: [17][200/412]	Loss 0.2225 (0.2208)	InvT  23.34 ( 23.30)	Acc@1  96.48 ( 96.61)	Acc@3 100.00 ( 99.93)
Epoch: [17][220/412]	Loss 0.2486 (0.222)	InvT  23.35 ( 23.30)	Acc@1  96.48 ( 96.59)	Acc@3  99.61 ( 99.92)
Epoch: [17][240/412]	Loss 0.1749 (0.2223)	InvT  23.36 ( 23.31)	Acc@1  97.27 ( 96.58)	Acc@3 100.00 ( 99.92)
Epoch: [17][260/412]	Loss 0.2276 (0.225)	InvT  23.37 ( 23.31)	Acc@1  97.27 ( 96.52)	Acc@3 100.00 ( 99.92)
Epoch: [17][280/412]	Loss 0.2391 (0.2259)	InvT  23.38 ( 23.32)	Acc@1  96.09 ( 96.50)	Acc@3 100.00 ( 99.92)
Epoch: [17][300/412]	Loss 0.2766 (0.2263)	InvT  23.39 ( 23.32)	Acc@1  94.92 ( 96.48)	Acc@3 100.00 ( 99.92)
Epoch: [17][320/412]	Loss 0.2506 (0.2272)	InvT  23.40 ( 23.33)	Acc@1  96.48 ( 96.48)	Acc@3 100.00 ( 99.92)
Epoch: [17][340/412]	Loss 0.2243 (0.2279)	InvT  23.41 ( 23.33)	Acc@1  96.88 ( 96.47)	Acc@3 100.00 ( 99.92)
Epoch: [17][360/412]	Loss 0.1777 (0.2288)	InvT  23.42 ( 23.33)	Acc@1  97.66 ( 96.44)	Acc@3 100.00 ( 99.92)
Epoch: [17][380/412]	Loss 0.2714 (0.2291)	InvT  23.43 ( 23.34)	Acc@1  95.31 ( 96.44)	Acc@3 100.00 ( 99.92)
Epoch: [17][400/412]	Loss 0.2054 (0.2299)	InvT  23.44 ( 23.34)	Acc@1  97.66 ( 96.41)	Acc@3 100.00 ( 99.92)
Learning rate: 2.4852027588905737e-05
Epoch 17, valid metric: {"Acc@1": 37.5, "Acc@3": 52.5, "loss": 3.508}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch17.mdl
Epoch: [18][  0/412]	Loss 0.2364 (0.2364)	InvT  23.45 ( 23.45)	Acc@1  96.88 ( 96.88)	Acc@3 100.00 (100.00)
Epoch: [18][ 20/412]	Loss 0.2117 (0.202)	InvT  23.45 ( 23.45)	Acc@1  97.66 ( 96.93)	Acc@3 100.00 ( 99.96)
Epoch: [18][ 40/412]	Loss 0.1775 (0.1996)	InvT  23.46 ( 23.45)	Acc@1  98.05 ( 97.07)	Acc@3 100.00 ( 99.95)
Epoch: [18][ 60/412]	Loss 0.2452 (0.2003)	InvT  23.47 ( 23.46)	Acc@1  96.88 ( 97.04)	Acc@3  99.61 ( 99.94)
Epoch: [18][ 80/412]	Loss 0.2793 (0.1994)	InvT  23.48 ( 23.46)	Acc@1  96.09 ( 97.07)	Acc@3 100.00 ( 99.93)
Epoch: [18][100/412]	Loss 0.2211 (0.2015)	InvT  23.49 ( 23.47)	Acc@1  96.48 ( 96.99)	Acc@3 100.00 ( 99.91)
Epoch: [18][120/412]	Loss 0.1849 (0.2006)	InvT  23.50 ( 23.47)	Acc@1  97.66 ( 97.00)	Acc@3 100.00 ( 99.92)
Epoch: [18][140/412]	Loss 0.175 (0.2005)	InvT  23.51 ( 23.48)	Acc@1  98.05 ( 96.99)	Acc@3 100.00 ( 99.92)
Epoch: [18][160/412]	Loss 0.2388 (0.2011)	InvT  23.52 ( 23.48)	Acc@1  96.48 ( 96.96)	Acc@3 100.00 ( 99.92)
Epoch: [18][180/412]	Loss 0.1648 (0.2027)	InvT  23.53 ( 23.49)	Acc@1  97.66 ( 96.92)	Acc@3 100.00 ( 99.92)
Epoch: [18][200/412]	Loss 0.2388 (0.2043)	InvT  23.54 ( 23.49)	Acc@1  96.09 ( 96.87)	Acc@3 100.00 ( 99.91)
Epoch: [18][220/412]	Loss 0.194 (0.2036)	InvT  23.55 ( 23.50)	Acc@1  97.66 ( 96.89)	Acc@3  99.22 ( 99.91)
Epoch: [18][240/412]	Loss 0.2493 (0.2045)	InvT  23.56 ( 23.50)	Acc@1  96.88 ( 96.88)	Acc@3 100.00 ( 99.91)
Epoch: [18][260/412]	Loss 0.185 (0.205)	InvT  23.57 ( 23.51)	Acc@1  97.27 ( 96.88)	Acc@3  99.61 ( 99.90)
Epoch: [18][280/412]	Loss 0.1556 (0.2059)	InvT  23.58 ( 23.51)	Acc@1  98.05 ( 96.86)	Acc@3 100.00 ( 99.90)
Epoch: [18][300/412]	Loss 0.1863 (0.2058)	InvT  23.59 ( 23.52)	Acc@1  97.27 ( 96.86)	Acc@3  99.61 ( 99.91)
Epoch: [18][320/412]	Loss 0.2157 (0.2064)	InvT  23.60 ( 23.52)	Acc@1  97.27 ( 96.84)	Acc@3 100.00 ( 99.91)
Epoch: [18][340/412]	Loss 0.1879 (0.2062)	InvT  23.61 ( 23.53)	Acc@1  96.48 ( 96.86)	Acc@3  99.61 ( 99.91)
Epoch: [18][360/412]	Loss 0.2181 (0.2073)	InvT  23.62 ( 23.53)	Acc@1  95.31 ( 96.84)	Acc@3 100.00 ( 99.91)
Epoch: [18][380/412]	Loss 0.2539 (0.2087)	InvT  23.63 ( 23.53)	Acc@1  94.53 ( 96.80)	Acc@3  99.61 ( 99.91)
Epoch: [18][400/412]	Loss 0.2042 (0.2091)	InvT  23.64 ( 23.54)	Acc@1  96.88 ( 96.80)	Acc@3  99.61 ( 99.91)
Learning rate: 2.4549723621777626e-05
Epoch 18, valid metric: {"Acc@1": 37.8, "Acc@3": 52.0, "loss": 3.554}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch18.mdl
Epoch: [19][  0/412]	Loss 0.1649 (0.1649)	InvT  23.64 ( 23.64)	Acc@1  96.88 ( 96.88)	Acc@3 100.00 (100.00)
Epoch: [19][ 20/412]	Loss 0.1201 (0.1833)	InvT  23.65 ( 23.65)	Acc@1  98.44 ( 96.88)	Acc@3 100.00 ( 99.94)
Epoch: [19][ 40/412]	Loss 0.149 (0.1766)	InvT  23.66 ( 23.65)	Acc@1  98.44 ( 97.21)	Acc@3 100.00 ( 99.94)
Epoch: [19][ 60/412]	Loss 0.1934 (0.1803)	InvT  23.67 ( 23.65)	Acc@1  97.66 ( 97.23)	Acc@3 100.00 ( 99.95)
Epoch: [19][ 80/412]	Loss 0.1426 (0.1761)	InvT  23.68 ( 23.66)	Acc@1  97.66 ( 97.28)	Acc@3  99.61 ( 99.95)
Epoch: [19][100/412]	Loss 0.1601 (0.1771)	InvT  23.69 ( 23.66)	Acc@1  98.05 ( 97.29)	Acc@3 100.00 ( 99.95)
Epoch: [19][120/412]	Loss 0.1705 (0.1781)	InvT  23.70 ( 23.67)	Acc@1  97.66 ( 97.30)	Acc@3 100.00 ( 99.95)
Epoch: [19][140/412]	Loss 0.2163 (0.1794)	InvT  23.70 ( 23.67)	Acc@1  97.66 ( 97.27)	Acc@3  99.61 ( 99.94)
Epoch: [19][160/412]	Loss 0.2304 (0.1813)	InvT  23.71 ( 23.68)	Acc@1  97.27 ( 97.21)	Acc@3  99.61 ( 99.93)
Epoch: [19][180/412]	Loss 0.109 (0.182)	InvT  23.72 ( 23.68)	Acc@1  98.44 ( 97.20)	Acc@3 100.00 ( 99.94)
Epoch: [19][200/412]	Loss 0.1952 (0.1828)	InvT  23.73 ( 23.69)	Acc@1  97.27 ( 97.20)	Acc@3 100.00 ( 99.93)
Epoch: [19][220/412]	Loss 0.2369 (0.1849)	InvT  23.74 ( 23.69)	Acc@1  94.92 ( 97.16)	Acc@3 100.00 ( 99.93)
Epoch: [19][240/412]	Loss 0.1811 (0.1863)	InvT  23.75 ( 23.70)	Acc@1  96.09 ( 97.12)	Acc@3 100.00 ( 99.93)
Epoch: [19][260/412]	Loss 0.1512 (0.1875)	InvT  23.76 ( 23.70)	Acc@1  97.66 ( 97.12)	Acc@3 100.00 ( 99.93)
Epoch: [19][280/412]	Loss 0.2145 (0.1888)	InvT  23.77 ( 23.70)	Acc@1  97.66 ( 97.09)	Acc@3 100.00 ( 99.92)
Epoch: [19][300/412]	Loss 0.1199 (0.1888)	InvT  23.78 ( 23.71)	Acc@1  98.44 ( 97.09)	Acc@3 100.00 ( 99.92)
Epoch: [19][320/412]	Loss 0.2049 (0.189)	InvT  23.79 ( 23.71)	Acc@1  97.66 ( 97.09)	Acc@3 100.00 ( 99.92)
Epoch: [19][340/412]	Loss 0.1937 (0.1887)	InvT  23.80 ( 23.72)	Acc@1  98.44 ( 97.11)	Acc@3 100.00 ( 99.92)
Epoch: [19][360/412]	Loss 0.1234 (0.1894)	InvT  23.81 ( 23.72)	Acc@1  98.44 ( 97.10)	Acc@3 100.00 ( 99.92)
Epoch: [19][380/412]	Loss 0.1809 (0.1902)	InvT  23.82 ( 23.73)	Acc@1  98.05 ( 97.08)	Acc@3 100.00 ( 99.92)
Epoch: [19][400/412]	Loss 0.1592 (0.1906)	InvT  23.83 ( 23.73)	Acc@1  98.44 ( 97.07)	Acc@3 100.00 ( 99.92)
Learning rate: 2.4247419654649515e-05
Epoch 19, valid metric: {"Acc@1": 36.6, "Acc@3": 51.2, "loss": 3.553}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch19.mdl
Epoch: [20][  0/412]	Loss 0.134 (0.134)	InvT  23.83 ( 23.83)	Acc@1  96.88 ( 96.88)	Acc@3 100.00 (100.00)
Epoch: [20][ 20/412]	Loss 0.1803 (0.1551)	InvT  23.84 ( 23.84)	Acc@1  96.88 ( 97.51)	Acc@3  99.61 ( 99.96)
Epoch: [20][ 40/412]	Loss 0.1006 (0.1591)	InvT  23.85 ( 23.84)	Acc@1  98.83 ( 97.60)	Acc@3 100.00 ( 99.94)
Epoch: [20][ 60/412]	Loss 0.182 (0.1589)	InvT  23.86 ( 23.85)	Acc@1  96.09 ( 97.61)	Acc@3 100.00 ( 99.94)
Epoch: [20][ 80/412]	Loss 0.1679 (0.1634)	InvT  23.87 ( 23.85)	Acc@1  97.66 ( 97.51)	Acc@3 100.00 ( 99.94)
Epoch: [20][100/412]	Loss 0.1273 (0.1645)	InvT  23.88 ( 23.86)	Acc@1  98.44 ( 97.50)	Acc@3 100.00 ( 99.94)
Epoch: [20][120/412]	Loss 0.1267 (0.1635)	InvT  23.89 ( 23.86)	Acc@1  99.22 ( 97.54)	Acc@3 100.00 ( 99.95)
Epoch: [20][140/412]	Loss 0.1704 (0.1649)	InvT  23.90 ( 23.86)	Acc@1  97.27 ( 97.53)	Acc@3 100.00 ( 99.95)
Epoch: [20][160/412]	Loss 0.2081 (0.163)	InvT  23.91 ( 23.87)	Acc@1  96.48 ( 97.58)	Acc@3 100.00 ( 99.95)
Epoch: [20][180/412]	Loss 0.1699 (0.1641)	InvT  23.91 ( 23.87)	Acc@1  96.88 ( 97.56)	Acc@3 100.00 ( 99.95)
Epoch: [20][200/412]	Loss 0.1585 (0.1648)	InvT  23.92 ( 23.88)	Acc@1  97.27 ( 97.56)	Acc@3 100.00 ( 99.95)
Epoch: [20][220/412]	Loss 0.1957 (0.1651)	InvT  23.93 ( 23.88)	Acc@1  96.48 ( 97.52)	Acc@3 100.00 ( 99.95)
Epoch: [20][240/412]	Loss 0.2502 (0.1678)	InvT  23.94 ( 23.89)	Acc@1  94.92 ( 97.49)	Acc@3 100.00 ( 99.95)
Epoch: [20][260/412]	Loss 0.1622 (0.1685)	InvT  23.95 ( 23.89)	Acc@1  96.48 ( 97.47)	Acc@3 100.00 ( 99.95)
Epoch: [20][280/412]	Loss 0.1607 (0.1691)	InvT  23.96 ( 23.90)	Acc@1  98.44 ( 97.45)	Acc@3 100.00 ( 99.95)
Epoch: [20][300/412]	Loss 0.1457 (0.1688)	InvT  23.97 ( 23.90)	Acc@1  97.66 ( 97.46)	Acc@3 100.00 ( 99.95)
Epoch: [20][320/412]	Loss 0.2724 (0.1695)	InvT  23.98 ( 23.91)	Acc@1  94.53 ( 97.44)	Acc@3  99.61 ( 99.95)
Epoch: [20][340/412]	Loss 0.1242 (0.1703)	InvT  23.99 ( 23.91)	Acc@1  98.44 ( 97.43)	Acc@3 100.00 ( 99.95)
Epoch: [20][360/412]	Loss 0.1416 (0.1714)	InvT  24.00 ( 23.92)	Acc@1  97.66 ( 97.41)	Acc@3 100.00 ( 99.95)
Epoch: [20][380/412]	Loss 0.1311 (0.1713)	InvT  24.01 ( 23.92)	Acc@1  98.44 ( 97.42)	Acc@3 100.00 ( 99.95)
Epoch: [20][400/412]	Loss 0.1615 (0.1723)	InvT  24.02 ( 23.92)	Acc@1  97.66 ( 97.39)	Acc@3 100.00 ( 99.95)
Learning rate: 2.39451156875214e-05
Epoch 20, valid metric: {"Acc@1": 36.8, "Acc@3": 51.6, "loss": 3.636}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch20.mdl
Epoch: [21][  0/412]	Loss 0.1448 (0.1448)	InvT  24.02 ( 24.02)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [21][ 20/412]	Loss 0.104 (0.1404)	InvT  24.03 ( 24.03)	Acc@1  98.83 ( 97.99)	Acc@3 100.00 (100.00)
Epoch: [21][ 40/412]	Loss 0.1145 (0.1461)	InvT  24.04 ( 24.03)	Acc@1  98.05 ( 97.72)	Acc@3  99.61 ( 99.98)
Epoch: [21][ 60/412]	Loss 0.1123 (0.1495)	InvT  24.05 ( 24.04)	Acc@1  99.22 ( 97.67)	Acc@3 100.00 ( 99.99)
Epoch: [21][ 80/412]	Loss 0.1457 (0.1493)	InvT  24.06 ( 24.04)	Acc@1  96.48 ( 97.68)	Acc@3 100.00 ( 99.98)
Epoch: [21][100/412]	Loss 0.1093 (0.1489)	InvT  24.07 ( 24.05)	Acc@1  98.44 ( 97.73)	Acc@3  99.61 ( 99.97)
Epoch: [21][120/412]	Loss 0.1353 (0.1515)	InvT  24.08 ( 24.05)	Acc@1  98.83 ( 97.66)	Acc@3 100.00 ( 99.97)
Epoch: [21][140/412]	Loss 0.1717 (0.1532)	InvT  24.09 ( 24.06)	Acc@1  96.88 ( 97.65)	Acc@3 100.00 ( 99.97)
Epoch: [21][160/412]	Loss 0.1581 (0.1551)	InvT  24.09 ( 24.06)	Acc@1  96.88 ( 97.60)	Acc@3 100.00 ( 99.97)
Epoch: [21][180/412]	Loss 0.1239 (0.1554)	InvT  24.10 ( 24.06)	Acc@1  97.66 ( 97.55)	Acc@3 100.00 ( 99.97)
Epoch: [21][200/412]	Loss 0.1885 (0.1555)	InvT  24.11 ( 24.07)	Acc@1  97.27 ( 97.57)	Acc@3 100.00 ( 99.97)
Epoch: [21][220/412]	Loss 0.1411 (0.156)	InvT  24.12 ( 24.07)	Acc@1  98.05 ( 97.58)	Acc@3 100.00 ( 99.96)
Epoch: [21][240/412]	Loss 0.1575 (0.1563)	InvT  24.13 ( 24.08)	Acc@1  98.05 ( 97.58)	Acc@3 100.00 ( 99.96)
Epoch: [21][260/412]	Loss 0.168 (0.1567)	InvT  24.14 ( 24.08)	Acc@1  97.27 ( 97.58)	Acc@3 100.00 ( 99.96)
Epoch: [21][280/412]	Loss 0.1494 (0.1576)	InvT  24.15 ( 24.09)	Acc@1  97.27 ( 97.56)	Acc@3 100.00 ( 99.97)
Epoch: [21][300/412]	Loss 0.1195 (0.158)	InvT  24.16 ( 24.09)	Acc@1  98.44 ( 97.55)	Acc@3 100.00 ( 99.96)
Epoch: [21][320/412]	Loss 0.1849 (0.1592)	InvT  24.17 ( 24.10)	Acc@1  97.66 ( 97.55)	Acc@3 100.00 ( 99.96)
Epoch: [21][340/412]	Loss 0.1816 (0.159)	InvT  24.18 ( 24.10)	Acc@1  96.09 ( 97.57)	Acc@3 100.00 ( 99.96)
Epoch: [21][360/412]	Loss 0.1823 (0.1596)	InvT  24.19 ( 24.11)	Acc@1  97.66 ( 97.57)	Acc@3 100.00 ( 99.97)
Epoch: [21][380/412]	Loss 0.1578 (0.1599)	InvT  24.20 ( 24.11)	Acc@1  98.83 ( 97.58)	Acc@3 100.00 ( 99.97)
Epoch: [21][400/412]	Loss 0.2047 (0.1607)	InvT  24.21 ( 24.11)	Acc@1  97.27 ( 97.57)	Acc@3 100.00 ( 99.96)
Learning rate: 2.364281172039329e-05
Epoch 21, valid metric: {"Acc@1": 37.8, "Acc@3": 51.1, "loss": 3.608}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch21.mdl
Epoch: [22][  0/412]	Loss 0.1614 (0.1614)	InvT  24.22 ( 24.22)	Acc@1  97.66 ( 97.66)	Acc@3 100.00 (100.00)
Epoch: [22][ 20/412]	Loss 0.1121 (0.1514)	InvT  24.22 ( 24.22)	Acc@1  97.66 ( 97.69)	Acc@3 100.00 ( 99.98)
Epoch: [22][ 40/412]	Loss 0.1416 (0.1474)	InvT  24.23 ( 24.22)	Acc@1  98.44 ( 97.80)	Acc@3 100.00 ( 99.97)
Epoch: [22][ 60/412]	Loss 0.2269 (0.1495)	InvT  24.24 ( 24.23)	Acc@1  96.88 ( 97.73)	Acc@3  99.61 ( 99.97)
Epoch: [22][ 80/412]	Loss 0.1587 (0.1482)	InvT  24.25 ( 24.23)	Acc@1  98.05 ( 97.77)	Acc@3 100.00 ( 99.97)
Epoch: [22][100/412]	Loss 0.154 (0.1486)	InvT  24.26 ( 24.24)	Acc@1  98.83 ( 97.79)	Acc@3 100.00 ( 99.97)
Epoch: [22][120/412]	Loss 0.1125 (0.1501)	InvT  24.27 ( 24.24)	Acc@1  98.05 ( 97.73)	Acc@3 100.00 ( 99.97)
Epoch: [22][140/412]	Loss 0.1372 (0.1502)	InvT  24.28 ( 24.25)	Acc@1  98.05 ( 97.72)	Acc@3 100.00 ( 99.97)
Epoch: [22][160/412]	Loss 0.144 (0.1486)	InvT  24.29 ( 24.25)	Acc@1  96.09 ( 97.76)	Acc@3 100.00 ( 99.97)
Epoch: [22][180/412]	Loss 0.1108 (0.1473)	InvT  24.30 ( 24.26)	Acc@1  98.05 ( 97.82)	Acc@3 100.00 ( 99.97)
Epoch: [22][200/412]	Loss 0.2587 (0.1481)	InvT  24.31 ( 24.26)	Acc@1  96.48 ( 97.80)	Acc@3 100.00 ( 99.97)
Epoch: [22][220/412]	Loss 0.1529 (0.1483)	InvT  24.31 ( 24.26)	Acc@1  98.05 ( 97.79)	Acc@3 100.00 ( 99.97)
Epoch: [22][240/412]	Loss 0.1122 (0.1486)	InvT  24.32 ( 24.27)	Acc@1  99.22 ( 97.79)	Acc@3 100.00 ( 99.97)
Epoch: [22][260/412]	Loss 0.1343 (0.1488)	InvT  24.33 ( 24.27)	Acc@1  98.83 ( 97.78)	Acc@3 100.00 ( 99.97)
Epoch: [22][280/412]	Loss 0.2446 (0.1489)	InvT  24.34 ( 24.28)	Acc@1  96.09 ( 97.78)	Acc@3 100.00 ( 99.97)
Epoch: [22][300/412]	Loss 0.1359 (0.1492)	InvT  24.35 ( 24.28)	Acc@1  97.66 ( 97.75)	Acc@3 100.00 ( 99.97)
Epoch: [22][320/412]	Loss 0.1585 (0.1497)	InvT  24.36 ( 24.29)	Acc@1  97.27 ( 97.74)	Acc@3 100.00 ( 99.97)
Epoch: [22][340/412]	Loss 0.1756 (0.1504)	InvT  24.37 ( 24.29)	Acc@1  96.48 ( 97.70)	Acc@3 100.00 ( 99.97)
Epoch: [22][360/412]	Loss 0.1973 (0.1515)	InvT  24.38 ( 24.30)	Acc@1  96.48 ( 97.69)	Acc@3  99.22 ( 99.96)
Epoch: [22][380/412]	Loss 0.1235 (0.152)	InvT  24.39 ( 24.30)	Acc@1  99.61 ( 97.70)	Acc@3 100.00 ( 99.96)
Epoch: [22][400/412]	Loss 0.2271 (0.1531)	InvT  24.40 ( 24.31)	Acc@1  96.09 ( 97.69)	Acc@3 100.00 ( 99.96)
Learning rate: 2.3340507753265177e-05
Epoch 22, valid metric: {"Acc@1": 38.1, "Acc@3": 52.1, "loss": 3.614}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch22.mdl
Epoch: [23][  0/412]	Loss 0.1251 (0.1251)	InvT  24.41 ( 24.41)	Acc@1  98.83 ( 98.83)	Acc@3 100.00 (100.00)
Epoch: [23][ 20/412]	Loss 0.183 (0.1428)	InvT  24.41 ( 24.41)	Acc@1  97.27 ( 97.60)	Acc@3 100.00 ( 99.96)
Epoch: [23][ 40/412]	Loss 0.184 (0.1436)	InvT  24.42 ( 24.41)	Acc@1  96.88 ( 97.71)	Acc@3 100.00 ( 99.97)
Epoch: [23][ 60/412]	Loss 0.184 (0.1426)	InvT  24.43 ( 24.42)	Acc@1  97.27 ( 97.66)	Acc@3 100.00 ( 99.96)
Epoch: [23][ 80/412]	Loss 0.1121 (0.1428)	InvT  24.44 ( 24.42)	Acc@1  97.27 ( 97.69)	Acc@3 100.00 ( 99.96)
Epoch: [23][100/412]	Loss 0.1336 (0.1391)	InvT  24.45 ( 24.43)	Acc@1  97.27 ( 97.78)	Acc@3 100.00 ( 99.97)
Epoch: [23][120/412]	Loss 0.1376 (0.1369)	InvT  24.46 ( 24.43)	Acc@1  96.88 ( 97.84)	Acc@3 100.00 ( 99.96)
Epoch: [23][140/412]	Loss 0.1339 (0.1368)	InvT  24.47 ( 24.44)	Acc@1  97.27 ( 97.83)	Acc@3 100.00 ( 99.97)
Epoch: [23][160/412]	Loss 0.1285 (0.1376)	InvT  24.48 ( 24.44)	Acc@1  98.44 ( 97.82)	Acc@3 100.00 ( 99.97)
Epoch: [23][180/412]	Loss 0.1325 (0.1377)	InvT  24.49 ( 24.45)	Acc@1  97.66 ( 97.82)	Acc@3 100.00 ( 99.97)
Epoch: [23][200/412]	Loss 0.1341 (0.1368)	InvT  24.50 ( 24.45)	Acc@1  98.83 ( 97.86)	Acc@3 100.00 ( 99.97)
Epoch: [23][220/412]	Loss 0.1486 (0.1367)	InvT  24.51 ( 24.46)	Acc@1  97.66 ( 97.89)	Acc@3 100.00 ( 99.98)
Epoch: [23][240/412]	Loss 0.1144 (0.1371)	InvT  24.52 ( 24.46)	Acc@1  97.66 ( 97.90)	Acc@3 100.00 ( 99.98)
Epoch: [23][260/412]	Loss 0.1168 (0.1373)	InvT  24.52 ( 24.46)	Acc@1  97.66 ( 97.90)	Acc@3 100.00 ( 99.97)
Epoch: [23][280/412]	Loss 0.1558 (0.1383)	InvT  24.53 ( 24.47)	Acc@1  96.88 ( 97.88)	Acc@3  99.61 ( 99.97)
Epoch: [23][300/412]	Loss 0.1843 (0.1394)	InvT  24.54 ( 24.47)	Acc@1  97.27 ( 97.87)	Acc@3  99.61 ( 99.97)
Epoch: [23][320/412]	Loss 0.118 (0.1399)	InvT  24.55 ( 24.48)	Acc@1  98.44 ( 97.85)	Acc@3 100.00 ( 99.97)
Epoch: [23][340/412]	Loss 0.1161 (0.1403)	InvT  24.56 ( 24.48)	Acc@1  99.22 ( 97.84)	Acc@3 100.00 ( 99.97)
Epoch: [23][360/412]	Loss 0.1415 (0.1417)	InvT  24.57 ( 24.49)	Acc@1  98.44 ( 97.80)	Acc@3 100.00 ( 99.97)
Epoch: [23][380/412]	Loss 0.1809 (0.1422)	InvT  24.58 ( 24.49)	Acc@1  97.66 ( 97.79)	Acc@3 100.00 ( 99.97)
Epoch: [23][400/412]	Loss 0.09453 (0.1424)	InvT  24.59 ( 24.50)	Acc@1  98.83 ( 97.78)	Acc@3 100.00 ( 99.97)
Learning rate: 2.3038203786137063e-05
Epoch 23, valid metric: {"Acc@1": 37.4, "Acc@3": 51.5, "loss": 3.678}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch23.mdl
Epoch: [24][  0/412]	Loss 0.1198 (0.1198)	InvT  24.60 ( 24.60)	Acc@1  98.44 ( 98.44)	Acc@3 100.00 (100.00)
Epoch: [24][ 20/412]	Loss 0.1186 (0.1225)	InvT  24.61 ( 24.60)	Acc@1  98.05 ( 98.31)	Acc@3  99.61 ( 99.98)
Epoch: [24][ 40/412]	Loss 0.08676 (0.1189)	InvT  24.61 ( 24.61)	Acc@1  98.83 ( 98.35)	Acc@3 100.00 ( 99.96)
Epoch: [24][ 60/412]	Loss 0.08048 (0.1198)	InvT  24.62 ( 24.61)	Acc@1  98.44 ( 98.27)	Acc@3 100.00 ( 99.97)
Epoch: [24][ 80/412]	Loss 0.1304 (0.123)	InvT  24.63 ( 24.61)	Acc@1  97.66 ( 98.21)	Acc@3 100.00 ( 99.98)
Epoch: [24][100/412]	Loss 0.1474 (0.1242)	InvT  24.64 ( 24.62)	Acc@1  97.66 ( 98.19)	Acc@3 100.00 ( 99.98)
Epoch: [24][120/412]	Loss 0.07609 (0.1265)	InvT  24.65 ( 24.62)	Acc@1  99.22 ( 98.13)	Acc@3 100.00 ( 99.97)
Epoch: [24][140/412]	Loss 0.1631 (0.1281)	InvT  24.66 ( 24.63)	Acc@1  97.27 ( 98.11)	Acc@3 100.00 ( 99.97)
Epoch: [24][160/412]	Loss 0.1624 (0.1282)	InvT  24.67 ( 24.63)	Acc@1  95.70 ( 98.09)	Acc@3 100.00 ( 99.97)
Epoch: [24][180/412]	Loss 0.1525 (0.1294)	InvT  24.68 ( 24.64)	Acc@1  97.66 ( 98.08)	Acc@3 100.00 ( 99.97)
Epoch: [24][200/412]	Loss 0.09637 (0.1299)	InvT  24.68 ( 24.64)	Acc@1  99.22 ( 98.07)	Acc@3 100.00 ( 99.97)
Epoch: [24][220/412]	Loss 0.2084 (0.1291)	InvT  24.69 ( 24.64)	Acc@1  97.66 ( 98.09)	Acc@3 100.00 ( 99.97)
Epoch: [24][240/412]	Loss 0.1241 (0.1292)	InvT  24.70 ( 24.65)	Acc@1  97.66 ( 98.10)	Acc@3 100.00 ( 99.97)
Epoch: [24][260/412]	Loss 0.1474 (0.1299)	InvT  24.71 ( 24.65)	Acc@1  98.05 ( 98.09)	Acc@3  99.61 ( 99.97)
Epoch: [24][280/412]	Loss 0.1637 (0.1301)	InvT  24.72 ( 24.66)	Acc@1  98.05 ( 98.08)	Acc@3 100.00 ( 99.97)
Epoch: [24][300/412]	Loss 0.09992 (0.131)	InvT  24.73 ( 24.66)	Acc@1  98.83 ( 98.06)	Acc@3 100.00 ( 99.97)
Epoch: [24][320/412]	Loss 0.2184 (0.1322)	InvT  24.74 ( 24.67)	Acc@1  96.88 ( 98.04)	Acc@3 100.00 ( 99.97)
Epoch: [24][340/412]	Loss 0.1544 (0.1319)	InvT  24.75 ( 24.67)	Acc@1  97.66 ( 98.04)	Acc@3 100.00 ( 99.96)
Epoch: [24][360/412]	Loss 0.1057 (0.1321)	InvT  24.76 ( 24.68)	Acc@1  99.22 ( 98.03)	Acc@3 100.00 ( 99.96)
Epoch: [24][380/412]	Loss 0.1339 (0.1328)	InvT  24.77 ( 24.68)	Acc@1  98.83 ( 98.02)	Acc@3 100.00 ( 99.97)
Epoch: [24][400/412]	Loss 0.08662 (0.1329)	InvT  24.78 ( 24.69)	Acc@1  99.22 ( 98.01)	Acc@3 100.00 ( 99.97)
Learning rate: 2.273589981900895e-05
Epoch 24, valid metric: {"Acc@1": 38.4, "Acc@3": 51.5, "loss": 3.691}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch24.mdl
Epoch: [25][  0/412]	Loss 0.1196 (0.1196)	InvT  24.78 ( 24.78)	Acc@1  97.66 ( 97.66)	Acc@3 100.00 (100.00)
Epoch: [25][ 20/412]	Loss 0.07446 (0.117)	InvT  24.79 ( 24.79)	Acc@1  99.61 ( 98.05)	Acc@3 100.00 (100.00)
Epoch: [25][ 40/412]	Loss 0.0951 (0.115)	InvT  24.80 ( 24.79)	Acc@1  98.44 ( 98.22)	Acc@3 100.00 ( 99.99)
Epoch: [25][ 60/412]	Loss 0.1108 (0.1182)	InvT  24.81 ( 24.80)	Acc@1  98.83 ( 98.16)	Acc@3 100.00 ( 99.98)
Epoch: [25][ 80/412]	Loss 0.1486 (0.1213)	InvT  24.82 ( 24.80)	Acc@1  97.27 ( 98.11)	Acc@3 100.00 ( 99.99)
Epoch: [25][100/412]	Loss 0.1292 (0.1201)	InvT  24.83 ( 24.81)	Acc@1  98.44 ( 98.14)	Acc@3 100.00 ( 99.98)
Epoch: [25][120/412]	Loss 0.08786 (0.1195)	InvT  24.84 ( 24.81)	Acc@1  98.44 ( 98.17)	Acc@3 100.00 ( 99.98)
Epoch: [25][140/412]	Loss 0.1507 (0.1202)	InvT  24.85 ( 24.82)	Acc@1  97.27 ( 98.14)	Acc@3  99.61 ( 99.98)
Epoch: [25][160/412]	Loss 0.1529 (0.1211)	InvT  24.86 ( 24.82)	Acc@1  97.27 ( 98.10)	Acc@3 100.00 ( 99.98)
Epoch: [25][180/412]	Loss 0.1245 (0.1231)	InvT  24.86 ( 24.82)	Acc@1  98.44 ( 98.04)	Acc@3 100.00 ( 99.98)
Epoch: [25][200/412]	Loss 0.1068 (0.1227)	InvT  24.87 ( 24.83)	Acc@1  98.05 ( 98.08)	Acc@3 100.00 ( 99.98)
Epoch: [25][220/412]	Loss 0.09882 (0.1223)	InvT  24.88 ( 24.83)	Acc@1  98.05 ( 98.09)	Acc@3 100.00 ( 99.98)
Epoch: [25][240/412]	Loss 0.1343 (0.1227)	InvT  24.89 ( 24.84)	Acc@1  97.27 ( 98.10)	Acc@3  99.61 ( 99.98)
Epoch: [25][260/412]	Loss 0.2106 (0.1242)	InvT  24.90 ( 24.84)	Acc@1  94.92 ( 98.05)	Acc@3 100.00 ( 99.97)
Epoch: [25][280/412]	Loss 0.116 (0.1256)	InvT  24.91 ( 24.85)	Acc@1  98.44 ( 98.03)	Acc@3 100.00 ( 99.97)
Epoch: [25][300/412]	Loss 0.1219 (0.1255)	InvT  24.92 ( 24.85)	Acc@1  98.05 ( 98.04)	Acc@3 100.00 ( 99.97)
Epoch: [25][320/412]	Loss 0.1442 (0.1265)	InvT  24.93 ( 24.86)	Acc@1  97.27 ( 98.01)	Acc@3 100.00 ( 99.97)
Epoch: [25][340/412]	Loss 0.158 (0.1267)	InvT  24.94 ( 24.86)	Acc@1  96.88 ( 98.00)	Acc@3 100.00 ( 99.97)
Epoch: [25][360/412]	Loss 0.1889 (0.1263)	InvT  24.95 ( 24.87)	Acc@1  96.88 ( 97.99)	Acc@3 100.00 ( 99.97)
Epoch: [25][380/412]	Loss 0.1482 (0.1262)	InvT  24.96 ( 24.87)	Acc@1  97.66 ( 97.99)	Acc@3 100.00 ( 99.97)
Epoch: [25][400/412]	Loss 0.1073 (0.1257)	InvT  24.97 ( 24.88)	Acc@1  97.27 ( 98.00)	Acc@3 100.00 ( 99.97)
Learning rate: 2.243359585188084e-05
Epoch 25, valid metric: {"Acc@1": 37.7, "Acc@3": 52.1, "loss": 3.725}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch25.mdl
Epoch: [26][  0/412]	Loss 0.1384 (0.1384)	InvT  24.98 ( 24.98)	Acc@1  96.88 ( 96.88)	Acc@3 100.00 (100.00)
Epoch: [26][ 20/412]	Loss 0.1182 (0.1134)	InvT  24.98 ( 24.98)	Acc@1  98.44 ( 98.23)	Acc@3 100.00 ( 99.94)
Epoch: [26][ 40/412]	Loss 0.1279 (0.1114)	InvT  24.99 ( 24.98)	Acc@1  97.66 ( 98.30)	Acc@3 100.00 ( 99.96)
Epoch: [26][ 60/412]	Loss 0.1498 (0.1148)	InvT  25.00 ( 24.99)	Acc@1  96.88 ( 98.27)	Acc@3 100.00 ( 99.96)
Epoch: [26][ 80/412]	Loss 0.1398 (0.1155)	InvT  25.01 ( 24.99)	Acc@1  97.66 ( 98.25)	Acc@3  99.61 ( 99.96)
Epoch: [26][100/412]	Loss 0.0929 (0.1147)	InvT  25.02 ( 25.00)	Acc@1  98.44 ( 98.27)	Acc@3 100.00 ( 99.97)
Epoch: [26][120/412]	Loss 0.1575 (0.1145)	InvT  25.03 ( 25.00)	Acc@1  97.27 ( 98.26)	Acc@3 100.00 ( 99.97)
Epoch: [26][140/412]	Loss 0.1058 (0.1144)	InvT  25.04 ( 25.01)	Acc@1  96.88 ( 98.22)	Acc@3 100.00 ( 99.96)
Epoch: [26][160/412]	Loss 0.1005 (0.1147)	InvT  25.04 ( 25.01)	Acc@1  98.44 ( 98.20)	Acc@3 100.00 ( 99.97)
Epoch: [26][180/412]	Loss 0.1135 (0.1147)	InvT  25.05 ( 25.01)	Acc@1  98.05 ( 98.21)	Acc@3 100.00 ( 99.97)
Epoch: [26][200/412]	Loss 0.08254 (0.1145)	InvT  25.06 ( 25.02)	Acc@1  98.05 ( 98.21)	Acc@3 100.00 ( 99.97)
Epoch: [26][220/412]	Loss 0.1598 (0.1148)	InvT  25.07 ( 25.02)	Acc@1  97.66 ( 98.21)	Acc@3 100.00 ( 99.97)
Epoch: [26][240/412]	Loss 0.07957 (0.1155)	InvT  25.08 ( 25.03)	Acc@1  98.44 ( 98.20)	Acc@3 100.00 ( 99.97)
Epoch: [26][260/412]	Loss 0.08444 (0.1168)	InvT  25.09 ( 25.03)	Acc@1  98.83 ( 98.17)	Acc@3 100.00 ( 99.97)
Epoch: [26][280/412]	Loss 0.1507 (0.117)	InvT  25.10 ( 25.04)	Acc@1  97.66 ( 98.17)	Acc@3 100.00 ( 99.97)
Epoch: [26][300/412]	Loss 0.08097 (0.1169)	InvT  25.11 ( 25.04)	Acc@1  99.22 ( 98.19)	Acc@3 100.00 ( 99.97)
Epoch: [26][320/412]	Loss 0.1366 (0.117)	InvT  25.12 ( 25.05)	Acc@1  98.05 ( 98.20)	Acc@3 100.00 ( 99.97)
Epoch: [26][340/412]	Loss 0.07584 (0.1173)	InvT  25.13 ( 25.05)	Acc@1  99.22 ( 98.19)	Acc@3 100.00 ( 99.97)
Epoch: [26][360/412]	Loss 0.1325 (0.1177)	InvT  25.14 ( 25.06)	Acc@1  98.44 ( 98.17)	Acc@3 100.00 ( 99.97)
Epoch: [26][380/412]	Loss 0.1516 (0.1185)	InvT  25.15 ( 25.06)	Acc@1  97.27 ( 98.16)	Acc@3 100.00 ( 99.97)
Epoch: [26][400/412]	Loss 0.1345 (0.1192)	InvT  25.16 ( 25.06)	Acc@1  97.66 ( 98.15)	Acc@3 100.00 ( 99.97)
Learning rate: 2.213129188475273e-05
Epoch 26, valid metric: {"Acc@1": 37.8, "Acc@3": 52.6, "loss": 3.716}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch26.mdl
Epoch: [27][  0/412]	Loss 0.1441 (0.1441)	InvT  25.16 ( 25.16)	Acc@1  97.66 ( 97.66)	Acc@3 100.00 (100.00)
Epoch: [27][ 20/412]	Loss 0.05386 (0.09584)	InvT  25.17 ( 25.17)	Acc@1  99.61 ( 98.68)	Acc@3 100.00 ( 99.98)
Epoch: [27][ 40/412]	Loss 0.04211 (0.1034)	InvT  25.18 ( 25.17)	Acc@1 100.00 ( 98.43)	Acc@3 100.00 ( 99.97)
Epoch: [27][ 60/412]	Loss 0.1379 (0.1048)	InvT  25.19 ( 25.18)	Acc@1  98.83 ( 98.41)	Acc@3 100.00 ( 99.97)
Epoch: [27][ 80/412]	Loss 0.1278 (0.1054)	InvT  25.20 ( 25.18)	Acc@1  97.27 ( 98.40)	Acc@3 100.00 ( 99.98)
Epoch: [27][100/412]	Loss 0.1273 (0.1091)	InvT  25.21 ( 25.19)	Acc@1  98.44 ( 98.33)	Acc@3 100.00 ( 99.97)
Epoch: [27][120/412]	Loss 0.143 (0.1105)	InvT  25.22 ( 25.19)	Acc@1  97.27 ( 98.31)	Acc@3 100.00 ( 99.97)
Epoch: [27][140/412]	Loss 0.1608 (0.1103)	InvT  25.22 ( 25.19)	Acc@1  96.48 ( 98.30)	Acc@3 100.00 ( 99.98)
Epoch: [27][160/412]	Loss 0.1292 (0.1099)	InvT  25.23 ( 25.20)	Acc@1  97.66 ( 98.28)	Acc@3 100.00 ( 99.98)
Epoch: [27][180/412]	Loss 0.06903 (0.1098)	InvT  25.24 ( 25.20)	Acc@1  99.61 ( 98.28)	Acc@3 100.00 ( 99.98)
Epoch: [27][200/412]	Loss 0.08995 (0.1099)	InvT  25.25 ( 25.21)	Acc@1 100.00 ( 98.28)	Acc@3 100.00 ( 99.98)
Epoch: [27][220/412]	Loss 0.08885 (0.1085)	InvT  25.26 ( 25.21)	Acc@1  99.61 ( 98.31)	Acc@3 100.00 ( 99.98)
Epoch: [27][240/412]	Loss 0.1369 (0.1088)	InvT  25.27 ( 25.22)	Acc@1  98.05 ( 98.30)	Acc@3 100.00 ( 99.98)
Epoch: [27][260/412]	Loss 0.1022 (0.109)	InvT  25.28 ( 25.22)	Acc@1  98.44 ( 98.30)	Acc@3 100.00 ( 99.98)
Epoch: [27][280/412]	Loss 0.1347 (0.109)	InvT  25.29 ( 25.23)	Acc@1  98.05 ( 98.31)	Acc@3 100.00 ( 99.98)
Epoch: [27][300/412]	Loss 0.1184 (0.1087)	InvT  25.30 ( 25.23)	Acc@1  98.05 ( 98.31)	Acc@3 100.00 ( 99.98)
Epoch: [27][320/412]	Loss 0.06532 (0.1093)	InvT  25.31 ( 25.23)	Acc@1  99.61 ( 98.30)	Acc@3 100.00 ( 99.98)
Epoch: [27][340/412]	Loss 0.1044 (0.1095)	InvT  25.32 ( 25.24)	Acc@1  98.83 ( 98.29)	Acc@3 100.00 ( 99.98)
Epoch: [27][360/412]	Loss 0.1164 (0.11)	InvT  25.33 ( 25.24)	Acc@1  98.83 ( 98.27)	Acc@3 100.00 ( 99.98)
Epoch: [27][380/412]	Loss 0.1317 (0.1097)	InvT  25.34 ( 25.25)	Acc@1  98.44 ( 98.28)	Acc@3  99.61 ( 99.98)
Epoch: [27][400/412]	Loss 0.1264 (0.1105)	InvT  25.34 ( 25.25)	Acc@1  97.66 ( 98.27)	Acc@3 100.00 ( 99.98)
Learning rate: 2.1828987917624614e-05
Epoch 27, valid metric: {"Acc@1": 38.0, "Acc@3": 52.5, "loss": 3.738}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch27.mdl
Epoch: [28][  0/412]	Loss 0.0381 (0.0381)	InvT  25.35 ( 25.35)	Acc@1 100.00 (100.00)	Acc@3 100.00 (100.00)
Epoch: [28][ 20/412]	Loss 0.09589 (0.09164)	InvT  25.36 ( 25.35)	Acc@1  98.44 ( 98.66)	Acc@3 100.00 ( 99.98)
Epoch: [28][ 40/412]	Loss 0.1336 (0.09192)	InvT  25.37 ( 25.36)	Acc@1  98.05 ( 98.57)	Acc@3 100.00 ( 99.99)
Epoch: [28][ 60/412]	Loss 0.09703 (0.09402)	InvT  25.38 ( 25.36)	Acc@1  98.05 ( 98.44)	Acc@3 100.00 ( 99.99)
Epoch: [28][ 80/412]	Loss 0.09177 (0.09368)	InvT  25.39 ( 25.37)	Acc@1  99.22 ( 98.45)	Acc@3  99.61 ( 99.99)
Epoch: [28][100/412]	Loss 0.09867 (0.09562)	InvT  25.39 ( 25.37)	Acc@1  98.83 ( 98.43)	Acc@3 100.00 ( 99.98)
Epoch: [28][120/412]	Loss 0.09975 (0.09606)	InvT  25.40 ( 25.38)	Acc@1  98.44 ( 98.42)	Acc@3 100.00 ( 99.99)
Epoch: [28][140/412]	Loss 0.1029 (0.09625)	InvT  25.41 ( 25.38)	Acc@1  98.05 ( 98.45)	Acc@3 100.00 ( 99.99)
Epoch: [28][160/412]	Loss 0.1677 (0.09694)	InvT  25.42 ( 25.39)	Acc@1  97.66 ( 98.46)	Acc@3 100.00 ( 99.99)
Epoch: [28][180/412]	Loss 0.1311 (0.0977)	InvT  25.43 ( 25.39)	Acc@1  98.05 ( 98.45)	Acc@3 100.00 ( 99.98)
Epoch: [28][200/412]	Loss 0.1062 (0.09777)	InvT  25.44 ( 25.39)	Acc@1  98.05 ( 98.46)	Acc@3 100.00 ( 99.98)
Epoch: [28][220/412]	Loss 0.08448 (0.09704)	InvT  25.45 ( 25.40)	Acc@1  98.44 ( 98.49)	Acc@3 100.00 ( 99.98)
Epoch: [28][240/412]	Loss 0.09179 (0.09751)	InvT  25.46 ( 25.40)	Acc@1  98.05 ( 98.50)	Acc@3 100.00 ( 99.99)
Epoch: [28][260/412]	Loss 0.09829 (0.09875)	InvT  25.47 ( 25.41)	Acc@1  98.05 ( 98.47)	Acc@3 100.00 ( 99.98)
Epoch: [28][280/412]	Loss 0.09789 (0.09891)	InvT  25.47 ( 25.41)	Acc@1  98.44 ( 98.48)	Acc@3 100.00 ( 99.98)
Epoch: [28][300/412]	Loss 0.08694 (0.09926)	InvT  25.48 ( 25.42)	Acc@1  98.83 ( 98.48)	Acc@3 100.00 ( 99.98)
Epoch: [28][320/412]	Loss 0.1292 (0.09992)	InvT  25.49 ( 25.42)	Acc@1  98.05 ( 98.47)	Acc@3 100.00 ( 99.98)
Epoch: [28][340/412]	Loss 0.1021 (0.09923)	InvT  25.50 ( 25.43)	Acc@1  98.05 ( 98.49)	Acc@3 100.00 ( 99.98)
Epoch: [28][360/412]	Loss 0.1674 (0.09984)	InvT  25.51 ( 25.43)	Acc@1  97.66 ( 98.49)	Acc@3 100.00 ( 99.98)
Epoch: [28][380/412]	Loss 0.05698 (0.09998)	InvT  25.52 ( 25.43)	Acc@1  99.22 ( 98.47)	Acc@3 100.00 ( 99.98)
Epoch: [28][400/412]	Loss 0.09086 (0.1005)	InvT  25.53 ( 25.44)	Acc@1  98.44 ( 98.46)	Acc@3 100.00 ( 99.98)
Learning rate: 2.1526683950496502e-05
Epoch 28, valid metric: {"Acc@1": 37.3, "Acc@3": 53.0, "loss": 3.764}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch28.mdl
Epoch: [29][  0/412]	Loss 0.08593 (0.08593)	InvT  25.54 ( 25.54)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [29][ 20/412]	Loss 0.1009 (0.09169)	InvT  25.55 ( 25.54)	Acc@1  97.27 ( 98.60)	Acc@3  99.61 ( 99.96)
Epoch: [29][ 40/412]	Loss 0.09117 (0.08951)	InvT  25.55 ( 25.55)	Acc@1  99.22 ( 98.53)	Acc@3 100.00 ( 99.98)
Epoch: [29][ 60/412]	Loss 0.1437 (0.09044)	InvT  25.56 ( 25.55)	Acc@1  97.27 ( 98.54)	Acc@3 100.00 ( 99.99)
Epoch: [29][ 80/412]	Loss 0.1714 (0.09273)	InvT  25.57 ( 25.55)	Acc@1  98.05 ( 98.51)	Acc@3 100.00 ( 99.98)
Epoch: [29][100/412]	Loss 0.1284 (0.09203)	InvT  25.58 ( 25.56)	Acc@1  97.27 ( 98.54)	Acc@3 100.00 ( 99.98)
Epoch: [29][120/412]	Loss 0.1309 (0.09231)	InvT  25.59 ( 25.56)	Acc@1  97.27 ( 98.52)	Acc@3 100.00 ( 99.99)
Epoch: [29][140/412]	Loss 0.06345 (0.09239)	InvT  25.60 ( 25.57)	Acc@1  99.61 ( 98.52)	Acc@3 100.00 ( 99.99)
Epoch: [29][160/412]	Loss 0.08493 (0.09308)	InvT  25.61 ( 25.57)	Acc@1  97.66 ( 98.51)	Acc@3 100.00 ( 99.99)
Epoch: [29][180/412]	Loss 0.08604 (0.09405)	InvT  25.62 ( 25.58)	Acc@1  99.22 ( 98.51)	Acc@3 100.00 ( 99.98)
Epoch: [29][200/412]	Loss 0.1123 (0.09536)	InvT  25.62 ( 25.58)	Acc@1  97.66 ( 98.50)	Acc@3 100.00 ( 99.98)
Epoch: [29][220/412]	Loss 0.06217 (0.09482)	InvT  25.63 ( 25.58)	Acc@1  98.83 ( 98.51)	Acc@3 100.00 ( 99.98)
Epoch: [29][240/412]	Loss 0.07548 (0.09538)	InvT  25.64 ( 25.59)	Acc@1  98.44 ( 98.49)	Acc@3 100.00 ( 99.98)
Epoch: [29][260/412]	Loss 0.08779 (0.09526)	InvT  25.65 ( 25.59)	Acc@1  99.22 ( 98.50)	Acc@3 100.00 ( 99.98)
Epoch: [29][280/412]	Loss 0.152 (0.09611)	InvT  25.66 ( 25.60)	Acc@1  97.27 ( 98.49)	Acc@3 100.00 ( 99.97)
Epoch: [29][300/412]	Loss 0.08367 (0.09756)	InvT  25.67 ( 25.60)	Acc@1  98.44 ( 98.47)	Acc@3 100.00 ( 99.97)
Epoch: [29][320/412]	Loss 0.05627 (0.09762)	InvT  25.68 ( 25.61)	Acc@1  99.22 ( 98.47)	Acc@3 100.00 ( 99.97)
Epoch: [29][340/412]	Loss 0.107 (0.098)	InvT  25.69 ( 25.61)	Acc@1  98.44 ( 98.46)	Acc@3 100.00 ( 99.98)
Epoch: [29][360/412]	Loss 0.09936 (0.09843)	InvT  25.70 ( 25.62)	Acc@1  99.22 ( 98.44)	Acc@3 100.00 ( 99.98)
Epoch: [29][380/412]	Loss 0.05285 (0.09869)	InvT  25.71 ( 25.62)	Acc@1  99.22 ( 98.44)	Acc@3 100.00 ( 99.98)
Epoch: [29][400/412]	Loss 0.05722 (0.098)	InvT  25.72 ( 25.63)	Acc@1  99.22 ( 98.46)	Acc@3 100.00 ( 99.98)
Learning rate: 2.122437998336839e-05
Epoch 29, valid metric: {"Acc@1": 36.7, "Acc@3": 51.3, "loss": 3.75}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch29.mdl
Epoch: [30][  0/412]	Loss 0.05303 (0.05303)	InvT  25.72 ( 25.72)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
Epoch: [30][ 20/412]	Loss 0.0964 (0.0901)	InvT  25.73 ( 25.73)	Acc@1  98.44 ( 98.77)	Acc@3 100.00 ( 99.98)
Epoch: [30][ 40/412]	Loss 0.09389 (0.08886)	InvT  25.74 ( 25.73)	Acc@1  98.44 ( 98.79)	Acc@3 100.00 ( 99.98)
Epoch: [30][ 60/412]	Loss 0.1122 (0.0903)	InvT  25.75 ( 25.74)	Acc@1  97.27 ( 98.71)	Acc@3 100.00 ( 99.99)
Epoch: [30][ 80/412]	Loss 0.1068 (0.0921)	InvT  25.76 ( 25.74)	Acc@1  98.44 ( 98.69)	Acc@3 100.00 ( 99.98)
Epoch: [30][100/412]	Loss 0.08971 (0.09219)	InvT  25.77 ( 25.74)	Acc@1  98.83 ( 98.63)	Acc@3 100.00 ( 99.97)
Epoch: [30][120/412]	Loss 0.08863 (0.09239)	InvT  25.78 ( 25.75)	Acc@1  98.83 ( 98.63)	Acc@3 100.00 ( 99.97)
Epoch: [30][140/412]	Loss 0.1157 (0.09327)	InvT  25.78 ( 25.75)	Acc@1  97.27 ( 98.59)	Acc@3 100.00 ( 99.97)
Epoch: [30][160/412]	Loss 0.08385 (0.0923)	InvT  25.79 ( 25.76)	Acc@1  98.44 ( 98.58)	Acc@3 100.00 ( 99.98)
Epoch: [30][180/412]	Loss 0.09138 (0.09218)	InvT  25.80 ( 25.76)	Acc@1  99.61 ( 98.58)	Acc@3 100.00 ( 99.98)
Epoch: [30][200/412]	Loss 0.1108 (0.09241)	InvT  25.81 ( 25.77)	Acc@1  98.05 ( 98.58)	Acc@3 100.00 ( 99.98)
Epoch: [30][220/412]	Loss 0.07684 (0.0927)	InvT  25.82 ( 25.77)	Acc@1  98.44 ( 98.57)	Acc@3 100.00 ( 99.98)
Epoch: [30][240/412]	Loss 0.09635 (0.09241)	InvT  25.83 ( 25.78)	Acc@1  98.44 ( 98.58)	Acc@3 100.00 ( 99.98)
Epoch: [30][260/412]	Loss 0.09684 (0.0938)	InvT  25.84 ( 25.78)	Acc@1  98.05 ( 98.54)	Acc@3 100.00 ( 99.98)
Epoch: [30][280/412]	Loss 0.08165 (0.09443)	InvT  25.85 ( 25.79)	Acc@1  99.61 ( 98.54)	Acc@3 100.00 ( 99.98)
Epoch: [30][300/412]	Loss 0.04979 (0.09451)	InvT  25.86 ( 25.79)	Acc@1  99.22 ( 98.54)	Acc@3 100.00 ( 99.98)
Epoch: [30][320/412]	Loss 0.1286 (0.09465)	InvT  25.87 ( 25.79)	Acc@1  98.05 ( 98.54)	Acc@3 100.00 ( 99.98)
Epoch: [30][340/412]	Loss 0.129 (0.09504)	InvT  25.88 ( 25.80)	Acc@1  97.27 ( 98.53)	Acc@3 100.00 ( 99.98)
Epoch: [30][360/412]	Loss 0.09019 (0.09481)	InvT  25.89 ( 25.80)	Acc@1  98.44 ( 98.54)	Acc@3 100.00 ( 99.98)
Epoch: [30][380/412]	Loss 0.1132 (0.09564)	InvT  25.90 ( 25.81)	Acc@1  98.83 ( 98.52)	Acc@3 100.00 ( 99.98)
Epoch: [30][400/412]	Loss 0.08717 (0.09595)	InvT  25.91 ( 25.81)	Acc@1  98.83 ( 98.51)	Acc@3 100.00 ( 99.98)
Learning rate: 2.0922076016240276e-05
Epoch 30, valid metric: {"Acc@1": 37.4, "Acc@3": 51.5, "loss": 3.802}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch30.mdl
Epoch: [31][  0/412]	Loss 0.0721 (0.0721)	InvT  25.91 ( 25.91)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [31][ 20/412]	Loss 0.0853 (0.09029)	InvT  25.92 ( 25.92)	Acc@1  98.44 ( 98.55)	Acc@3 100.00 (100.00)
Epoch: [31][ 40/412]	Loss 0.1073 (0.09462)	InvT  25.93 ( 25.92)	Acc@1  98.44 ( 98.48)	Acc@3  99.61 ( 99.97)
Epoch: [31][ 60/412]	Loss 0.0821 (0.08999)	InvT  25.94 ( 25.92)	Acc@1  99.22 ( 98.55)	Acc@3 100.00 ( 99.97)
Epoch: [31][ 80/412]	Loss 0.1351 (0.08873)	InvT  25.95 ( 25.93)	Acc@1  97.66 ( 98.62)	Acc@3 100.00 ( 99.98)
Epoch: [31][100/412]	Loss 0.04865 (0.09052)	InvT  25.95 ( 25.93)	Acc@1 100.00 ( 98.55)	Acc@3 100.00 ( 99.98)
Epoch: [31][120/412]	Loss 0.1215 (0.08989)	InvT  25.96 ( 25.94)	Acc@1  98.05 ( 98.55)	Acc@3 100.00 ( 99.98)
Epoch: [31][140/412]	Loss 0.05575 (0.08951)	InvT  25.97 ( 25.94)	Acc@1 100.00 ( 98.57)	Acc@3 100.00 ( 99.98)
Epoch: [31][160/412]	Loss 0.06732 (0.08904)	InvT  25.98 ( 25.95)	Acc@1  98.83 ( 98.57)	Acc@3 100.00 ( 99.98)
Epoch: [31][180/412]	Loss 0.1068 (0.08816)	InvT  25.99 ( 25.95)	Acc@1  98.05 ( 98.60)	Acc@3 100.00 ( 99.98)
Epoch: [31][200/412]	Loss 0.1194 (0.08724)	InvT  26.00 ( 25.95)	Acc@1  98.83 ( 98.65)	Acc@3 100.00 ( 99.98)
Epoch: [31][220/412]	Loss 0.05347 (0.0872)	InvT  26.01 ( 25.96)	Acc@1  99.61 ( 98.65)	Acc@3 100.00 ( 99.98)
Epoch: [31][240/412]	Loss 0.09076 (0.08765)	InvT  26.02 ( 25.96)	Acc@1  98.44 ( 98.64)	Acc@3  99.61 ( 99.98)
Epoch: [31][260/412]	Loss 0.104 (0.08774)	InvT  26.03 ( 25.97)	Acc@1  98.05 ( 98.65)	Acc@3 100.00 ( 99.98)
Epoch: [31][280/412]	Loss 0.07171 (0.08758)	InvT  26.04 ( 25.97)	Acc@1  98.83 ( 98.65)	Acc@3 100.00 ( 99.98)
Epoch: [31][300/412]	Loss 0.09931 (0.08854)	InvT  26.05 ( 25.98)	Acc@1  99.22 ( 98.63)	Acc@3 100.00 ( 99.98)
Epoch: [31][320/412]	Loss 0.1048 (0.08941)	InvT  26.05 ( 25.98)	Acc@1  98.44 ( 98.61)	Acc@3 100.00 ( 99.98)
Epoch: [31][340/412]	Loss 0.1284 (0.08995)	InvT  26.06 ( 25.99)	Acc@1  96.09 ( 98.60)	Acc@3 100.00 ( 99.97)
Epoch: [31][360/412]	Loss 0.072 (0.09014)	InvT  26.07 ( 25.99)	Acc@1  98.83 ( 98.59)	Acc@3 100.00 ( 99.97)
Epoch: [31][380/412]	Loss 0.08936 (0.09031)	InvT  26.08 ( 26.00)	Acc@1  98.83 ( 98.58)	Acc@3 100.00 ( 99.97)
Epoch: [31][400/412]	Loss 0.104 (0.09039)	InvT  26.09 ( 26.00)	Acc@1  98.05 ( 98.58)	Acc@3 100.00 ( 99.97)
Learning rate: 2.0619772049112165e-05
Epoch 31, valid metric: {"Acc@1": 38.6, "Acc@3": 51.3, "loss": 3.772}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch31.mdl
Epoch: [32][  0/412]	Loss 0.09063 (0.09063)	InvT  26.10 ( 26.10)	Acc@1  98.83 ( 98.83)	Acc@3  99.61 ( 99.61)
Epoch: [32][ 20/412]	Loss 0.09454 (0.08806)	InvT  26.11 ( 26.10)	Acc@1  98.44 ( 98.55)	Acc@3 100.00 ( 99.98)
Epoch: [32][ 40/412]	Loss 0.09892 (0.08635)	InvT  26.12 ( 26.11)	Acc@1  98.44 ( 98.60)	Acc@3 100.00 ( 99.98)
Epoch: [32][ 60/412]	Loss 0.08807 (0.08787)	InvT  26.12 ( 26.11)	Acc@1  98.83 ( 98.57)	Acc@3 100.00 ( 99.99)
Epoch: [32][ 80/412]	Loss 0.09597 (0.08766)	InvT  26.13 ( 26.12)	Acc@1  97.66 ( 98.61)	Acc@3 100.00 ( 99.99)
Epoch: [32][100/412]	Loss 0.06522 (0.08637)	InvT  26.14 ( 26.12)	Acc@1  99.61 ( 98.65)	Acc@3 100.00 ( 99.98)
Epoch: [32][120/412]	Loss 0.065 (0.08579)	InvT  26.15 ( 26.12)	Acc@1  98.83 ( 98.67)	Acc@3 100.00 ( 99.98)
Epoch: [32][140/412]	Loss 0.1305 (0.08704)	InvT  26.16 ( 26.13)	Acc@1  96.48 ( 98.62)	Acc@3 100.00 ( 99.98)
Epoch: [32][160/412]	Loss 0.1222 (0.08694)	InvT  26.17 ( 26.13)	Acc@1  98.05 ( 98.60)	Acc@3 100.00 ( 99.97)
Epoch: [32][180/412]	Loss 0.05304 (0.08642)	InvT  26.18 ( 26.14)	Acc@1  99.61 ( 98.61)	Acc@3 100.00 ( 99.97)
Epoch: [32][200/412]	Loss 0.06238 (0.08632)	InvT  26.19 ( 26.14)	Acc@1  99.22 ( 98.63)	Acc@3 100.00 ( 99.97)
Epoch: [32][220/412]	Loss 0.05513 (0.0861)	InvT  26.20 ( 26.15)	Acc@1  99.22 ( 98.66)	Acc@3 100.00 ( 99.97)
Epoch: [32][240/412]	Loss 0.08994 (0.08671)	InvT  26.21 ( 26.15)	Acc@1  98.44 ( 98.64)	Acc@3 100.00 ( 99.97)
Epoch: [32][260/412]	Loss 0.09896 (0.08622)	InvT  26.21 ( 26.16)	Acc@1  97.27 ( 98.65)	Acc@3 100.00 ( 99.97)
Epoch: [32][280/412]	Loss 0.0981 (0.08633)	InvT  26.22 ( 26.16)	Acc@1  98.83 ( 98.67)	Acc@3 100.00 ( 99.97)
Epoch: [32][300/412]	Loss 0.1232 (0.0868)	InvT  26.23 ( 26.16)	Acc@1  97.66 ( 98.66)	Acc@3 100.00 ( 99.97)
Epoch: [32][320/412]	Loss 0.06944 (0.08725)	InvT  26.24 ( 26.17)	Acc@1 100.00 ( 98.67)	Acc@3 100.00 ( 99.97)
Epoch: [32][340/412]	Loss 0.04012 (0.08707)	InvT  26.25 ( 26.17)	Acc@1  99.61 ( 98.67)	Acc@3 100.00 ( 99.97)
Epoch: [32][360/412]	Loss 0.09546 (0.08702)	InvT  26.26 ( 26.18)	Acc@1  98.05 ( 98.67)	Acc@3 100.00 ( 99.97)
Epoch: [32][380/412]	Loss 0.1373 (0.08739)	InvT  26.27 ( 26.18)	Acc@1  97.66 ( 98.66)	Acc@3 100.00 ( 99.97)
Epoch: [32][400/412]	Loss 0.1057 (0.08783)	InvT  26.28 ( 26.19)	Acc@1  97.66 ( 98.64)	Acc@3 100.00 ( 99.97)
Learning rate: 2.0317468081984054e-05
Epoch 32, valid metric: {"Acc@1": 37.0, "Acc@3": 52.1, "loss": 3.883}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch32.mdl
Epoch: [33][  0/412]	Loss 0.07271 (0.07271)	InvT  26.28 ( 26.28)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [33][ 20/412]	Loss 0.08201 (0.07604)	InvT  26.29 ( 26.29)	Acc@1  98.44 ( 99.03)	Acc@3 100.00 ( 99.98)
Epoch: [33][ 40/412]	Loss 0.04037 (0.08094)	InvT  26.30 ( 26.29)	Acc@1  99.61 ( 98.82)	Acc@3 100.00 ( 99.99)
Epoch: [33][ 60/412]	Loss 0.08455 (0.08151)	InvT  26.31 ( 26.30)	Acc@1  98.05 ( 98.75)	Acc@3 100.00 ( 99.99)
Epoch: [33][ 80/412]	Loss 0.1012 (0.08259)	InvT  26.32 ( 26.30)	Acc@1  99.22 ( 98.72)	Acc@3 100.00 ( 99.98)
Epoch: [33][100/412]	Loss 0.03952 (0.07944)	InvT  26.33 ( 26.30)	Acc@1 100.00 ( 98.78)	Acc@3 100.00 ( 99.98)
Epoch: [33][120/412]	Loss 0.09709 (0.0785)	InvT  26.34 ( 26.31)	Acc@1  98.05 ( 98.80)	Acc@3 100.00 ( 99.99)
Epoch: [33][140/412]	Loss 0.02836 (0.07844)	InvT  26.35 ( 26.31)	Acc@1 100.00 ( 98.81)	Acc@3 100.00 ( 99.99)
Epoch: [33][160/412]	Loss 0.06929 (0.07936)	InvT  26.35 ( 26.32)	Acc@1  99.22 ( 98.80)	Acc@3 100.00 ( 99.99)
Epoch: [33][180/412]	Loss 0.1201 (0.07992)	InvT  26.36 ( 26.32)	Acc@1  96.48 ( 98.79)	Acc@3 100.00 ( 99.99)
Epoch: [33][200/412]	Loss 0.06764 (0.07927)	InvT  26.37 ( 26.33)	Acc@1  98.83 ( 98.79)	Acc@3 100.00 ( 99.99)
Epoch: [33][220/412]	Loss 0.1096 (0.07934)	InvT  26.38 ( 26.33)	Acc@1  97.27 ( 98.79)	Acc@3 100.00 ( 99.99)
Epoch: [33][240/412]	Loss 0.04461 (0.07919)	InvT  26.39 ( 26.34)	Acc@1  99.61 ( 98.79)	Acc@3 100.00 ( 99.99)
Epoch: [33][260/412]	Loss 0.07995 (0.07988)	InvT  26.40 ( 26.34)	Acc@1  98.44 ( 98.78)	Acc@3 100.00 ( 99.99)
Epoch: [33][280/412]	Loss 0.06939 (0.08023)	InvT  26.41 ( 26.34)	Acc@1  98.83 ( 98.78)	Acc@3 100.00 ( 99.99)
Epoch: [33][300/412]	Loss 0.07572 (0.08104)	InvT  26.42 ( 26.35)	Acc@1  98.83 ( 98.76)	Acc@3 100.00 ( 99.99)
Epoch: [33][320/412]	Loss 0.06994 (0.08182)	InvT  26.42 ( 26.35)	Acc@1  99.22 ( 98.73)	Acc@3 100.00 ( 99.99)
Epoch: [33][340/412]	Loss 0.0828 (0.08294)	InvT  26.43 ( 26.36)	Acc@1  98.83 ( 98.71)	Acc@3 100.00 ( 99.99)
Epoch: [33][360/412]	Loss 0.06857 (0.08263)	InvT  26.44 ( 26.36)	Acc@1 100.00 ( 98.72)	Acc@3 100.00 ( 99.98)
Epoch: [33][380/412]	Loss 0.1021 (0.08283)	InvT  26.45 ( 26.37)	Acc@1  98.05 ( 98.72)	Acc@3  99.61 ( 99.98)
Epoch: [33][400/412]	Loss 0.06701 (0.08258)	InvT  26.46 ( 26.37)	Acc@1  99.22 ( 98.73)	Acc@3 100.00 ( 99.98)
Learning rate: 2.0015164114855942e-05
Epoch 33, valid metric: {"Acc@1": 37.2, "Acc@3": 52.3, "loss": 3.826}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch33.mdl
Epoch: [34][  0/412]	Loss 0.07702 (0.07702)	InvT  26.46 ( 26.46)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [34][ 20/412]	Loss 0.04153 (0.07108)	InvT  26.47 ( 26.47)	Acc@1  99.61 ( 98.98)	Acc@3 100.00 ( 99.98)
Epoch: [34][ 40/412]	Loss 0.0819 (0.07487)	InvT  26.48 ( 26.47)	Acc@1  98.44 ( 98.83)	Acc@3 100.00 ( 99.98)
Epoch: [34][ 60/412]	Loss 0.08055 (0.07802)	InvT  26.49 ( 26.48)	Acc@1  98.83 ( 98.71)	Acc@3 100.00 ( 99.99)
Epoch: [34][ 80/412]	Loss 0.05784 (0.07558)	InvT  26.50 ( 26.48)	Acc@1  98.83 ( 98.75)	Acc@3 100.00 ( 99.99)
Epoch: [34][100/412]	Loss 0.09529 (0.07594)	InvT  26.51 ( 26.49)	Acc@1  98.83 ( 98.73)	Acc@3 100.00 ( 99.99)
Epoch: [34][120/412]	Loss 0.07122 (0.07588)	InvT  26.51 ( 26.49)	Acc@1  98.44 ( 98.75)	Acc@3 100.00 ( 99.99)
Epoch: [34][140/412]	Loss 0.03325 (0.0759)	InvT  26.52 ( 26.49)	Acc@1 100.00 ( 98.77)	Acc@3 100.00 ( 99.99)
Epoch: [34][160/412]	Loss 0.1495 (0.07609)	InvT  26.53 ( 26.50)	Acc@1  98.44 ( 98.79)	Acc@3 100.00 ( 99.99)
Epoch: [34][180/412]	Loss 0.04954 (0.07529)	InvT  26.54 ( 26.50)	Acc@1  99.61 ( 98.81)	Acc@3 100.00 ( 99.99)
Epoch: [34][200/412]	Loss 0.06851 (0.07527)	InvT  26.55 ( 26.51)	Acc@1  98.44 ( 98.80)	Acc@3 100.00 ( 99.99)
Epoch: [34][220/412]	Loss 0.09383 (0.07567)	InvT  26.56 ( 26.51)	Acc@1  98.44 ( 98.80)	Acc@3 100.00 ( 99.99)
Epoch: [34][240/412]	Loss 0.07854 (0.07551)	InvT  26.57 ( 26.52)	Acc@1  98.44 ( 98.80)	Acc@3 100.00 ( 99.99)
Epoch: [34][260/412]	Loss 0.05116 (0.07546)	InvT  26.58 ( 26.52)	Acc@1  99.61 ( 98.80)	Acc@3 100.00 ( 99.99)
Epoch: [34][280/412]	Loss 0.1175 (0.0761)	InvT  26.59 ( 26.52)	Acc@1  98.83 ( 98.79)	Acc@3 100.00 ( 99.99)
Epoch: [34][300/412]	Loss 0.05148 (0.07703)	InvT  26.59 ( 26.53)	Acc@1  98.83 ( 98.78)	Acc@3 100.00 ( 99.99)
Epoch: [34][320/412]	Loss 0.06293 (0.07727)	InvT  26.60 ( 26.53)	Acc@1  99.22 ( 98.78)	Acc@3 100.00 ( 99.99)
Epoch: [34][340/412]	Loss 0.09203 (0.07739)	InvT  26.61 ( 26.54)	Acc@1  98.44 ( 98.78)	Acc@3 100.00 ( 99.99)
Epoch: [34][360/412]	Loss 0.07663 (0.0774)	InvT  26.62 ( 26.54)	Acc@1  98.44 ( 98.79)	Acc@3 100.00 ( 99.99)
Epoch: [34][380/412]	Loss 0.05638 (0.0777)	InvT  26.63 ( 26.55)	Acc@1  99.61 ( 98.79)	Acc@3 100.00 ( 99.99)
Epoch: [34][400/412]	Loss 0.06287 (0.07779)	InvT  26.64 ( 26.55)	Acc@1  98.83 ( 98.79)	Acc@3 100.00 ( 99.99)
Learning rate: 1.9712860147727828e-05
Epoch 34, valid metric: {"Acc@1": 37.8, "Acc@3": 52.4, "loss": 3.867}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch34.mdl
Epoch: [35][  0/412]	Loss 0.1046 (0.1046)	InvT  26.64 ( 26.64)	Acc@1  97.66 ( 97.66)	Acc@3 100.00 (100.00)
Epoch: [35][ 20/412]	Loss 0.1533 (0.07394)	InvT  26.65 ( 26.65)	Acc@1  97.27 ( 98.96)	Acc@3  99.61 ( 99.96)
Epoch: [35][ 40/412]	Loss 0.06478 (0.07095)	InvT  26.66 ( 26.65)	Acc@1  99.61 ( 99.07)	Acc@3 100.00 ( 99.98)
Epoch: [35][ 60/412]	Loss 0.04736 (0.06865)	InvT  26.67 ( 26.66)	Acc@1  99.61 ( 99.09)	Acc@3 100.00 ( 99.99)
Epoch: [35][ 80/412]	Loss 0.05586 (0.06682)	InvT  26.68 ( 26.66)	Acc@1  99.61 ( 99.11)	Acc@3 100.00 ( 99.99)
Epoch: [35][100/412]	Loss 0.05967 (0.06693)	InvT  26.69 ( 26.66)	Acc@1  99.22 ( 99.12)	Acc@3 100.00 ( 99.99)
Epoch: [35][120/412]	Loss 0.06429 (0.06831)	InvT  26.69 ( 26.67)	Acc@1  99.22 ( 99.08)	Acc@3 100.00 ( 99.99)
Epoch: [35][140/412]	Loss 0.05548 (0.06741)	InvT  26.70 ( 26.67)	Acc@1  98.83 ( 99.07)	Acc@3 100.00 ( 99.99)
Epoch: [35][160/412]	Loss 0.04654 (0.06824)	InvT  26.71 ( 26.68)	Acc@1  99.22 ( 99.03)	Acc@3 100.00 (100.00)
Epoch: [35][180/412]	Loss 0.0845 (0.0682)	InvT  26.72 ( 26.68)	Acc@1  99.22 ( 99.02)	Acc@3 100.00 ( 99.99)
Epoch: [35][200/412]	Loss 0.06251 (0.06867)	InvT  26.73 ( 26.69)	Acc@1  99.61 ( 99.00)	Acc@3 100.00 ( 99.99)
Epoch: [35][220/412]	Loss 0.02503 (0.06882)	InvT  26.74 ( 26.69)	Acc@1 100.00 ( 98.98)	Acc@3 100.00 ( 99.99)
Epoch: [35][240/412]	Loss 0.05696 (0.06898)	InvT  26.75 ( 26.70)	Acc@1  99.22 ( 98.98)	Acc@3 100.00 ( 99.99)
Epoch: [35][260/412]	Loss 0.07105 (0.06967)	InvT  26.76 ( 26.70)	Acc@1  99.22 ( 98.98)	Acc@3 100.00 ( 99.99)
Epoch: [35][280/412]	Loss 0.04102 (0.06978)	InvT  26.77 ( 26.70)	Acc@1 100.00 ( 98.96)	Acc@3 100.00 ( 99.99)
Epoch: [35][300/412]	Loss 0.06435 (0.06963)	InvT  26.77 ( 26.71)	Acc@1  98.83 ( 98.97)	Acc@3 100.00 ( 99.99)
Epoch: [35][320/412]	Loss 0.08256 (0.07101)	InvT  26.78 ( 26.71)	Acc@1  98.05 ( 98.94)	Acc@3 100.00 ( 99.99)
Epoch: [35][340/412]	Loss 0.04514 (0.07109)	InvT  26.79 ( 26.72)	Acc@1  99.61 ( 98.94)	Acc@3 100.00 ( 99.99)
Epoch: [35][360/412]	Loss 0.0561 (0.07177)	InvT  26.80 ( 26.72)	Acc@1  98.83 ( 98.93)	Acc@3 100.00 ( 99.99)
Epoch: [35][380/412]	Loss 0.0811 (0.07186)	InvT  26.81 ( 26.73)	Acc@1  98.44 ( 98.93)	Acc@3 100.00 ( 99.99)
Epoch: [35][400/412]	Loss 0.1078 (0.07219)	InvT  26.82 ( 26.73)	Acc@1  98.83 ( 98.92)	Acc@3 100.00 ( 99.99)
Learning rate: 1.9410556180599716e-05
Epoch 35, valid metric: {"Acc@1": 36.9, "Acc@3": 52.5, "loss": 3.839}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch35.mdl
Epoch: [36][  0/412]	Loss 0.0402 (0.0402)	InvT  26.82 ( 26.82)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
Epoch: [36][ 20/412]	Loss 0.05149 (0.06506)	InvT  26.83 ( 26.83)	Acc@1 100.00 ( 99.22)	Acc@3 100.00 ( 99.98)
Epoch: [36][ 40/412]	Loss 0.1021 (0.06868)	InvT  26.84 ( 26.83)	Acc@1  98.83 ( 99.04)	Acc@3 100.00 ( 99.98)
Epoch: [36][ 60/412]	Loss 0.06984 (0.06864)	InvT  26.85 ( 26.84)	Acc@1  99.22 ( 98.98)	Acc@3 100.00 ( 99.99)
Epoch: [36][ 80/412]	Loss 0.08917 (0.06854)	InvT  26.86 ( 26.84)	Acc@1  98.83 ( 98.99)	Acc@3 100.00 ( 99.98)
Epoch: [36][100/412]	Loss 0.07809 (0.06806)	InvT  26.87 ( 26.85)	Acc@1  99.22 ( 98.99)	Acc@3 100.00 ( 99.98)
Epoch: [36][120/412]	Loss 0.05446 (0.06751)	InvT  26.88 ( 26.85)	Acc@1  98.83 ( 99.01)	Acc@3 100.00 ( 99.98)
Epoch: [36][140/412]	Loss 0.05461 (0.06808)	InvT  26.88 ( 26.85)	Acc@1  98.83 ( 99.00)	Acc@3 100.00 ( 99.98)
Epoch: [36][160/412]	Loss 0.03829 (0.06888)	InvT  26.89 ( 26.86)	Acc@1 100.00 ( 99.00)	Acc@3 100.00 ( 99.98)
Epoch: [36][180/412]	Loss 0.09492 (0.0695)	InvT  26.90 ( 26.86)	Acc@1  98.05 ( 98.96)	Acc@3 100.00 ( 99.98)
Epoch: [36][200/412]	Loss 0.08662 (0.07006)	InvT  26.91 ( 26.87)	Acc@1  98.44 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [36][220/412]	Loss 0.05106 (0.07083)	InvT  26.92 ( 26.87)	Acc@1  99.22 ( 98.92)	Acc@3 100.00 ( 99.98)
Epoch: [36][240/412]	Loss 0.08043 (0.07031)	InvT  26.93 ( 26.87)	Acc@1  98.83 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [36][260/412]	Loss 0.05514 (0.06982)	InvT  26.93 ( 26.88)	Acc@1  98.83 ( 98.94)	Acc@3 100.00 ( 99.99)
Epoch: [36][280/412]	Loss 0.02736 (0.06951)	InvT  26.94 ( 26.88)	Acc@1 100.00 ( 98.94)	Acc@3 100.00 ( 99.99)
Epoch: [36][300/412]	Loss 0.07375 (0.07002)	InvT  26.95 ( 26.89)	Acc@1  98.44 ( 98.93)	Acc@3 100.00 ( 99.98)
Epoch: [36][320/412]	Loss 0.08449 (0.06976)	InvT  26.96 ( 26.89)	Acc@1  97.27 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [36][340/412]	Loss 0.06278 (0.06948)	InvT  26.97 ( 26.90)	Acc@1  99.22 ( 98.95)	Acc@3 100.00 ( 99.99)
Epoch: [36][360/412]	Loss 0.09294 (0.0697)	InvT  26.98 ( 26.90)	Acc@1  99.22 ( 98.95)	Acc@3 100.00 ( 99.99)
Epoch: [36][380/412]	Loss 0.1143 (0.06999)	InvT  26.99 ( 26.90)	Acc@1  98.44 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [36][400/412]	Loss 0.04925 (0.07017)	InvT  27.00 ( 26.91)	Acc@1  99.22 ( 98.93)	Acc@3 100.00 ( 99.98)
Learning rate: 1.9108252213471605e-05
Epoch 36, valid metric: {"Acc@1": 37.4, "Acc@3": 52.2, "loss": 3.903}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch36.mdl
Epoch: [37][  0/412]	Loss 0.03091 (0.03091)	InvT  27.00 ( 27.00)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
Epoch: [37][ 20/412]	Loss 0.05067 (0.0603)	InvT  27.01 ( 27.01)	Acc@1 100.00 ( 99.09)	Acc@3 100.00 ( 99.98)
Epoch: [37][ 40/412]	Loss 0.0647 (0.06569)	InvT  27.02 ( 27.01)	Acc@1  99.22 ( 99.05)	Acc@3 100.00 ( 99.98)
Epoch: [37][ 60/412]	Loss 0.02798 (0.06356)	InvT  27.03 ( 27.02)	Acc@1 100.00 ( 99.10)	Acc@3 100.00 ( 99.99)
Epoch: [37][ 80/412]	Loss 0.05485 (0.06442)	InvT  27.04 ( 27.02)	Acc@1  99.22 ( 99.07)	Acc@3 100.00 ( 99.99)
Epoch: [37][100/412]	Loss 0.07759 (0.06574)	InvT  27.05 ( 27.03)	Acc@1  98.83 ( 99.01)	Acc@3 100.00 ( 99.99)
Epoch: [37][120/412]	Loss 0.05357 (0.06566)	InvT  27.06 ( 27.03)	Acc@1  98.83 ( 99.00)	Acc@3 100.00 ( 99.99)
Epoch: [37][140/412]	Loss 0.05251 (0.06621)	InvT  27.06 ( 27.03)	Acc@1  98.44 ( 98.97)	Acc@3 100.00 ( 99.99)
Epoch: [37][160/412]	Loss 0.06891 (0.06613)	InvT  27.07 ( 27.04)	Acc@1  98.05 ( 98.98)	Acc@3 100.00 ( 99.99)
Epoch: [37][180/412]	Loss 0.08836 (0.06589)	InvT  27.08 ( 27.04)	Acc@1  98.05 ( 98.97)	Acc@3 100.00 ( 99.99)
Epoch: [37][200/412]	Loss 0.09682 (0.06609)	InvT  27.09 ( 27.05)	Acc@1  97.66 ( 98.96)	Acc@3 100.00 ( 99.99)
Epoch: [37][220/412]	Loss 0.0617 (0.06598)	InvT  27.10 ( 27.05)	Acc@1  98.44 ( 98.96)	Acc@3 100.00 ( 99.99)
Epoch: [37][240/412]	Loss 0.0438 (0.06588)	InvT  27.11 ( 27.06)	Acc@1  98.83 ( 98.95)	Acc@3 100.00 ( 99.99)
Epoch: [37][260/412]	Loss 0.05843 (0.06596)	InvT  27.12 ( 27.06)	Acc@1  98.83 ( 98.95)	Acc@3 100.00 ( 99.99)
Epoch: [37][280/412]	Loss 0.03019 (0.06585)	InvT  27.13 ( 27.06)	Acc@1 100.00 ( 98.96)	Acc@3 100.00 ( 99.99)
Epoch: [37][300/412]	Loss 0.1269 (0.0661)	InvT  27.14 ( 27.07)	Acc@1  98.44 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [37][320/412]	Loss 0.0738 (0.0659)	InvT  27.15 ( 27.07)	Acc@1  98.05 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [37][340/412]	Loss 0.08228 (0.06617)	InvT  27.16 ( 27.08)	Acc@1  98.05 ( 98.95)	Acc@3 100.00 ( 99.98)
Epoch: [37][360/412]	Loss 0.09748 (0.0668)	InvT  27.16 ( 27.08)	Acc@1  97.66 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [37][380/412]	Loss 0.05711 (0.06728)	InvT  27.17 ( 27.09)	Acc@1  99.22 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [37][400/412]	Loss 0.09558 (0.06708)	InvT  27.18 ( 27.09)	Acc@1  98.83 ( 98.95)	Acc@3 100.00 ( 99.99)
Learning rate: 1.880594824634349e-05
Epoch 37, valid metric: {"Acc@1": 38.8, "Acc@3": 52.0, "loss": 3.861}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch37.mdl
Epoch: [38][  0/412]	Loss 0.0616 (0.0616)	InvT  27.19 ( 27.19)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
Epoch: [38][ 20/412]	Loss 0.07426 (0.0629)	InvT  27.20 ( 27.19)	Acc@1  98.05 ( 99.00)	Acc@3 100.00 (100.00)
Epoch: [38][ 40/412]	Loss 0.06841 (0.06383)	InvT  27.21 ( 27.20)	Acc@1  99.22 ( 99.00)	Acc@3 100.00 ( 99.99)
Epoch: [38][ 60/412]	Loss 0.05973 (0.06236)	InvT  27.21 ( 27.20)	Acc@1  99.22 ( 99.01)	Acc@3 100.00 ( 99.99)
Epoch: [38][ 80/412]	Loss 0.06739 (0.06223)	InvT  27.22 ( 27.21)	Acc@1  99.22 ( 99.00)	Acc@3 100.00 ( 99.99)
Epoch: [38][100/412]	Loss 0.1143 (0.06065)	InvT  27.23 ( 27.21)	Acc@1  98.05 ( 99.04)	Acc@3 100.00 ( 99.99)
Epoch: [38][120/412]	Loss 0.08107 (0.06143)	InvT  27.24 ( 27.21)	Acc@1  98.44 ( 99.03)	Acc@3 100.00 ( 99.99)
Epoch: [38][140/412]	Loss 0.07329 (0.06125)	InvT  27.25 ( 27.22)	Acc@1  98.44 ( 99.04)	Acc@3 100.00 ( 99.99)
Epoch: [38][160/412]	Loss 0.06136 (0.06094)	InvT  27.25 ( 27.22)	Acc@1  98.83 ( 99.06)	Acc@3 100.00 ( 99.99)
Epoch: [38][180/412]	Loss 0.07293 (0.0614)	InvT  27.26 ( 27.23)	Acc@1  98.44 ( 99.05)	Acc@3 100.00 ( 99.99)
Epoch: [38][200/412]	Loss 0.03579 (0.06136)	InvT  27.27 ( 27.23)	Acc@1 100.00 ( 99.05)	Acc@3 100.00 ( 99.99)
Epoch: [38][220/412]	Loss 0.0497 (0.06153)	InvT  27.28 ( 27.23)	Acc@1  99.22 ( 99.05)	Acc@3 100.00 ( 99.99)
Epoch: [38][240/412]	Loss 0.05706 (0.06107)	InvT  27.29 ( 27.24)	Acc@1  99.22 ( 99.07)	Acc@3 100.00 ( 99.99)
Epoch: [38][260/412]	Loss 0.06771 (0.06065)	InvT  27.30 ( 27.24)	Acc@1  99.61 ( 99.09)	Acc@3 100.00 ( 99.99)
Epoch: [38][280/412]	Loss 0.042 (0.06151)	InvT  27.31 ( 27.25)	Acc@1  99.22 ( 99.07)	Acc@3 100.00 ( 99.99)
Epoch: [38][300/412]	Loss 0.05951 (0.06225)	InvT  27.31 ( 27.25)	Acc@1  98.83 ( 99.06)	Acc@3 100.00 ( 99.99)
Epoch: [38][320/412]	Loss 0.06963 (0.06265)	InvT  27.32 ( 27.26)	Acc@1  99.61 ( 99.06)	Acc@3 100.00 ( 99.99)
Epoch: [38][340/412]	Loss 0.03696 (0.06259)	InvT  27.33 ( 27.26)	Acc@1  99.61 ( 99.06)	Acc@3 100.00 ( 99.99)
Epoch: [38][360/412]	Loss 0.06085 (0.06314)	InvT  27.34 ( 27.26)	Acc@1  99.22 ( 99.05)	Acc@3 100.00 ( 99.99)
Epoch: [38][380/412]	Loss 0.1067 (0.06335)	InvT  27.35 ( 27.27)	Acc@1  97.66 ( 99.03)	Acc@3 100.00 ( 99.99)
Epoch: [38][400/412]	Loss 0.09429 (0.06346)	InvT  27.36 ( 27.27)	Acc@1  98.44 ( 99.03)	Acc@3 100.00 ( 99.99)
Learning rate: 1.850364427921538e-05
Epoch 38, valid metric: {"Acc@1": 38.2, "Acc@3": 51.5, "loss": 3.951}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch38.mdl
Epoch: [39][  0/412]	Loss 0.0644 (0.0644)	InvT  27.36 ( 27.36)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [39][ 20/412]	Loss 0.04606 (0.05583)	InvT  27.37 ( 27.37)	Acc@1 100.00 ( 99.24)	Acc@3 100.00 (100.00)
Epoch: [39][ 40/412]	Loss 0.04545 (0.05995)	InvT  27.38 ( 27.37)	Acc@1  98.83 ( 99.09)	Acc@3 100.00 (100.00)
Epoch: [39][ 60/412]	Loss 0.06984 (0.06219)	InvT  27.39 ( 27.38)	Acc@1  98.44 ( 99.05)	Acc@3 100.00 ( 99.99)
Epoch: [39][ 80/412]	Loss 0.05583 (0.06494)	InvT  27.40 ( 27.38)	Acc@1  98.83 ( 98.97)	Acc@3 100.00 (100.00)
Epoch: [39][100/412]	Loss 0.05501 (0.06398)	InvT  27.41 ( 27.39)	Acc@1  98.83 ( 99.01)	Acc@3 100.00 (100.00)
Epoch: [39][120/412]	Loss 0.08453 (0.06385)	InvT  27.41 ( 27.39)	Acc@1  98.44 ( 99.02)	Acc@3 100.00 ( 99.99)
Epoch: [39][140/412]	Loss 0.0763 (0.06311)	InvT  27.42 ( 27.39)	Acc@1  98.83 ( 99.01)	Acc@3 100.00 ( 99.99)
Epoch: [39][160/412]	Loss 0.08515 (0.06304)	InvT  27.43 ( 27.40)	Acc@1  99.22 ( 99.03)	Acc@3 100.00 (100.00)
Epoch: [39][180/412]	Loss 0.04674 (0.06241)	InvT  27.44 ( 27.40)	Acc@1  99.22 ( 99.05)	Acc@3 100.00 (100.00)
Epoch: [39][200/412]	Loss 0.02711 (0.06205)	InvT  27.45 ( 27.41)	Acc@1 100.00 ( 99.06)	Acc@3 100.00 ( 99.99)
Epoch: [39][220/412]	Loss 0.1041 (0.06209)	InvT  27.46 ( 27.41)	Acc@1  98.44 ( 99.05)	Acc@3 100.00 ( 99.99)
Epoch: [39][240/412]	Loss 0.1128 (0.06205)	InvT  27.47 ( 27.41)	Acc@1  98.83 ( 99.05)	Acc@3 100.00 ( 99.99)
Epoch: [39][260/412]	Loss 0.03646 (0.06257)	InvT  27.47 ( 27.42)	Acc@1 100.00 ( 99.04)	Acc@3 100.00 ( 99.99)
Epoch: [39][280/412]	Loss 0.0515 (0.06271)	InvT  27.48 ( 27.42)	Acc@1  99.61 ( 99.05)	Acc@3 100.00 ( 99.99)
Epoch: [39][300/412]	Loss 0.04409 (0.06358)	InvT  27.49 ( 27.43)	Acc@1  99.61 ( 99.03)	Acc@3 100.00 ( 99.99)
Epoch: [39][320/412]	Loss 0.07878 (0.06346)	InvT  27.50 ( 27.43)	Acc@1  98.83 ( 99.04)	Acc@3 100.00 ( 99.98)
Epoch: [39][340/412]	Loss 0.04867 (0.06358)	InvT  27.51 ( 27.44)	Acc@1  99.22 ( 99.04)	Acc@3 100.00 ( 99.98)
Epoch: [39][360/412]	Loss 0.07118 (0.06331)	InvT  27.52 ( 27.44)	Acc@1  99.61 ( 99.04)	Acc@3 100.00 ( 99.98)
Epoch: [39][380/412]	Loss 0.04907 (0.06291)	InvT  27.53 ( 27.44)	Acc@1  98.83 ( 99.05)	Acc@3 100.00 ( 99.99)
Epoch: [39][400/412]	Loss 0.04318 (0.06309)	InvT  27.54 ( 27.45)	Acc@1  98.83 ( 99.05)	Acc@3 100.00 ( 99.99)
Learning rate: 1.8201340312087268e-05
Epoch 39, valid metric: {"Acc@1": 37.7, "Acc@3": 52.7, "loss": 3.884}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch39.mdl
Epoch: [40][  0/412]	Loss 0.07816 (0.07816)	InvT  27.54 ( 27.54)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [40][ 20/412]	Loss 0.05936 (0.06194)	InvT  27.55 ( 27.55)	Acc@1  99.61 ( 99.03)	Acc@3 100.00 (100.00)
Epoch: [40][ 40/412]	Loss 0.04129 (0.05562)	InvT  27.56 ( 27.55)	Acc@1  99.61 ( 99.15)	Acc@3 100.00 (100.00)
Epoch: [40][ 60/412]	Loss 0.09441 (0.05754)	InvT  27.57 ( 27.55)	Acc@1  98.05 ( 99.12)	Acc@3 100.00 (100.00)
Epoch: [40][ 80/412]	Loss 0.05569 (0.05755)	InvT  27.57 ( 27.56)	Acc@1  99.61 ( 99.12)	Acc@3 100.00 (100.00)
Epoch: [40][100/412]	Loss 0.09796 (0.05868)	InvT  27.58 ( 27.56)	Acc@1  98.44 ( 99.09)	Acc@3 100.00 (100.00)
Epoch: [40][120/412]	Loss 0.04015 (0.05848)	InvT  27.59 ( 27.57)	Acc@1  99.61 ( 99.10)	Acc@3 100.00 (100.00)
Epoch: [40][140/412]	Loss 0.06793 (0.05808)	InvT  27.60 ( 27.57)	Acc@1  99.61 ( 99.10)	Acc@3 100.00 ( 99.99)
Epoch: [40][160/412]	Loss 0.09005 (0.05778)	InvT  27.61 ( 27.57)	Acc@1  98.83 ( 99.11)	Acc@3 100.00 (100.00)
Epoch: [40][180/412]	Loss 0.08559 (0.05978)	InvT  27.62 ( 27.58)	Acc@1  98.05 ( 99.07)	Acc@3 100.00 (100.00)
Epoch: [40][200/412]	Loss 0.02816 (0.05893)	InvT  27.62 ( 27.58)	Acc@1 100.00 ( 99.09)	Acc@3 100.00 (100.00)
Epoch: [40][220/412]	Loss 0.09791 (0.05945)	InvT  27.63 ( 27.59)	Acc@1  99.22 ( 99.09)	Acc@3 100.00 ( 99.99)
Epoch: [40][240/412]	Loss 0.06269 (0.05984)	InvT  27.64 ( 27.59)	Acc@1  98.83 ( 99.08)	Acc@3 100.00 ( 99.99)
Epoch: [40][260/412]	Loss 0.04687 (0.05996)	InvT  27.65 ( 27.60)	Acc@1  99.22 ( 99.09)	Acc@3 100.00 ( 99.99)
Epoch: [40][280/412]	Loss 0.0591 (0.0602)	InvT  27.66 ( 27.60)	Acc@1  98.83 ( 99.08)	Acc@3 100.00 ( 99.99)
Epoch: [40][300/412]	Loss 0.05804 (0.06027)	InvT  27.67 ( 27.60)	Acc@1  98.83 ( 99.07)	Acc@3 100.00 ( 99.99)
Epoch: [40][320/412]	Loss 0.03169 (0.06023)	InvT  27.68 ( 27.61)	Acc@1 100.00 ( 99.07)	Acc@3 100.00 ( 99.99)
Epoch: [40][340/412]	Loss 0.04617 (0.06032)	InvT  27.68 ( 27.61)	Acc@1  99.22 ( 99.07)	Acc@3 100.00 ( 99.99)
Epoch: [40][360/412]	Loss 0.06167 (0.06048)	InvT  27.69 ( 27.62)	Acc@1 100.00 ( 99.07)	Acc@3 100.00 ( 99.99)
Epoch: [40][380/412]	Loss 0.07562 (0.06043)	InvT  27.70 ( 27.62)	Acc@1  98.83 ( 99.07)	Acc@3 100.00 ( 99.99)
Epoch: [40][400/412]	Loss 0.0832 (0.0603)	InvT  27.71 ( 27.63)	Acc@1  99.61 ( 99.08)	Acc@3 100.00 ( 99.99)
Learning rate: 1.7899036344959156e-05
Epoch 40, valid metric: {"Acc@1": 39.5, "Acc@3": 53.0, "loss": 3.946}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch40.mdl
Epoch: [41][  0/412]	Loss 0.06696 (0.06696)	InvT  27.72 ( 27.72)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [41][ 20/412]	Loss 0.02696 (0.06556)	InvT  27.72 ( 27.72)	Acc@1  99.61 ( 98.90)	Acc@3 100.00 (100.00)
Epoch: [41][ 40/412]	Loss 0.04772 (0.05838)	InvT  27.73 ( 27.72)	Acc@1  98.83 ( 99.06)	Acc@3 100.00 (100.00)
Epoch: [41][ 60/412]	Loss 0.0293 (0.05747)	InvT  27.74 ( 27.73)	Acc@1  99.61 ( 99.08)	Acc@3 100.00 (100.00)
Epoch: [41][ 80/412]	Loss 0.05454 (0.05875)	InvT  27.75 ( 27.73)	Acc@1  98.83 ( 99.05)	Acc@3 100.00 (100.00)
Epoch: [41][100/412]	Loss 0.04343 (0.05864)	InvT  27.76 ( 27.74)	Acc@1  99.22 ( 99.06)	Acc@3 100.00 ( 99.99)
Epoch: [41][120/412]	Loss 0.1065 (0.05904)	InvT  27.77 ( 27.74)	Acc@1  99.22 ( 99.10)	Acc@3 100.00 ( 99.99)
Epoch: [41][140/412]	Loss 0.0451 (0.05832)	InvT  27.77 ( 27.75)	Acc@1  99.61 ( 99.11)	Acc@3 100.00 ( 99.99)
Epoch: [41][160/412]	Loss 0.03174 (0.05897)	InvT  27.78 ( 27.75)	Acc@1  99.61 ( 99.11)	Acc@3 100.00 ( 99.99)
Epoch: [41][180/412]	Loss 0.01986 (0.0584)	InvT  27.79 ( 27.75)	Acc@1 100.00 ( 99.13)	Acc@3 100.00 ( 99.99)
Epoch: [41][200/412]	Loss 0.07547 (0.05858)	InvT  27.80 ( 27.76)	Acc@1  98.44 ( 99.13)	Acc@3 100.00 ( 99.99)
Epoch: [41][220/412]	Loss 0.07309 (0.05972)	InvT  27.81 ( 27.76)	Acc@1  98.44 ( 99.11)	Acc@3 100.00 ( 99.99)
Epoch: [41][240/412]	Loss 0.08204 (0.05977)	InvT  27.81 ( 27.77)	Acc@1  98.83 ( 99.11)	Acc@3 100.00 ( 99.99)
Epoch: [41][260/412]	Loss 0.04693 (0.0595)	InvT  27.82 ( 27.77)	Acc@1  99.61 ( 99.11)	Acc@3 100.00 ( 99.99)
Epoch: [41][280/412]	Loss 0.07572 (0.05944)	InvT  27.83 ( 27.77)	Acc@1  98.44 ( 99.10)	Acc@3 100.00 ( 99.98)
Epoch: [41][300/412]	Loss 0.03336 (0.05914)	InvT  27.84 ( 27.78)	Acc@1  99.22 ( 99.09)	Acc@3 100.00 ( 99.99)
Epoch: [41][320/412]	Loss 0.0641 (0.05869)	InvT  27.85 ( 27.78)	Acc@1  99.22 ( 99.11)	Acc@3 100.00 ( 99.99)
Epoch: [41][340/412]	Loss 0.1244 (0.05895)	InvT  27.86 ( 27.79)	Acc@1  99.22 ( 99.10)	Acc@3 100.00 ( 99.99)
Epoch: [41][360/412]	Loss 0.04975 (0.05938)	InvT  27.86 ( 27.79)	Acc@1  99.22 ( 99.08)	Acc@3 100.00 ( 99.99)
Epoch: [41][380/412]	Loss 0.07035 (0.05995)	InvT  27.87 ( 27.79)	Acc@1  98.83 ( 99.07)	Acc@3 100.00 ( 99.99)
Epoch: [41][400/412]	Loss 0.09239 (0.06041)	InvT  27.88 ( 27.80)	Acc@1  96.88 ( 99.04)	Acc@3 100.00 ( 99.99)
Learning rate: 1.759673237783104e-05
Epoch 41, valid metric: {"Acc@1": 38.2, "Acc@3": 52.1, "loss": 3.981}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch41.mdl
Epoch: [42][  0/412]	Loss 0.05116 (0.05116)	InvT  27.89 ( 27.89)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
Epoch: [42][ 20/412]	Loss 0.03111 (0.05209)	InvT  27.89 ( 27.89)	Acc@1 100.00 ( 99.11)	Acc@3 100.00 ( 99.98)
Epoch: [42][ 40/412]	Loss 0.0588 (0.05185)	InvT  27.90 ( 27.89)	Acc@1  98.83 ( 99.14)	Acc@3 100.00 ( 99.99)
Epoch: [42][ 60/412]	Loss 0.03911 (0.05373)	InvT  27.91 ( 27.90)	Acc@1 100.00 ( 99.14)	Acc@3 100.00 ( 99.99)
Epoch: [42][ 80/412]	Loss 0.1251 (0.05634)	InvT  27.92 ( 27.90)	Acc@1  97.66 ( 99.11)	Acc@3 100.00 ( 99.99)
Epoch: [42][100/412]	Loss 0.04381 (0.05661)	InvT  27.93 ( 27.91)	Acc@1  98.83 ( 99.11)	Acc@3 100.00 ( 99.99)
Epoch: [42][120/412]	Loss 0.02994 (0.05572)	InvT  27.93 ( 27.91)	Acc@1 100.00 ( 99.15)	Acc@3 100.00 ( 99.99)
Epoch: [42][140/412]	Loss 0.04692 (0.05675)	InvT  27.94 ( 27.91)	Acc@1 100.00 ( 99.16)	Acc@3 100.00 ( 99.99)
Epoch: [42][160/412]	Loss 0.0431 (0.0569)	InvT  27.95 ( 27.92)	Acc@1  98.83 ( 99.14)	Acc@3 100.00 ( 99.99)
Epoch: [42][180/412]	Loss 0.0295 (0.05639)	InvT  27.96 ( 27.92)	Acc@1 100.00 ( 99.15)	Acc@3 100.00 ( 99.99)
Epoch: [42][200/412]	Loss 0.05745 (0.0568)	InvT  27.97 ( 27.93)	Acc@1  98.83 ( 99.13)	Acc@3 100.00 ( 99.99)
Epoch: [42][220/412]	Loss 0.06675 (0.05676)	InvT  27.98 ( 27.93)	Acc@1  98.05 ( 99.13)	Acc@3 100.00 ( 99.99)
Epoch: [42][240/412]	Loss 0.04412 (0.05655)	InvT  27.99 ( 27.94)	Acc@1  98.83 ( 99.14)	Acc@3 100.00 ( 99.99)
Epoch: [42][260/412]	Loss 0.04401 (0.0568)	InvT  27.99 ( 27.94)	Acc@1  99.22 ( 99.13)	Acc@3 100.00 ( 99.99)
Epoch: [42][280/412]	Loss 0.09562 (0.05646)	InvT  28.00 ( 27.94)	Acc@1  97.66 ( 99.13)	Acc@3 100.00 ( 99.99)
Epoch: [42][300/412]	Loss 0.03742 (0.05603)	InvT  28.01 ( 27.95)	Acc@1  99.22 ( 99.14)	Acc@3 100.00 ( 99.99)
Epoch: [42][320/412]	Loss 0.02525 (0.05569)	InvT  28.02 ( 27.95)	Acc@1 100.00 ( 99.14)	Acc@3 100.00 ( 99.99)
Epoch: [42][340/412]	Loss 0.02709 (0.05565)	InvT  28.03 ( 27.96)	Acc@1  99.61 ( 99.14)	Acc@3 100.00 ( 99.99)
Epoch: [42][360/412]	Loss 0.04664 (0.05581)	InvT  28.04 ( 27.96)	Acc@1  99.22 ( 99.13)	Acc@3 100.00 ( 99.99)
Epoch: [42][380/412]	Loss 0.04776 (0.05608)	InvT  28.04 ( 27.96)	Acc@1  98.83 ( 99.12)	Acc@3 100.00 ( 99.99)
Epoch: [42][400/412]	Loss 0.05971 (0.05583)	InvT  28.05 ( 27.97)	Acc@1  99.61 ( 99.13)	Acc@3 100.00 ( 99.99)
Learning rate: 1.729442841070293e-05
Epoch 42, valid metric: {"Acc@1": 38.0, "Acc@3": 52.1, "loss": 3.934}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch42.mdl
Epoch: [43][  0/412]	Loss 0.02768 (0.02768)	InvT  28.06 ( 28.06)	Acc@1 100.00 (100.00)	Acc@3 100.00 (100.00)
Epoch: [43][ 20/412]	Loss 0.09432 (0.04736)	InvT  28.07 ( 28.06)	Acc@1  98.83 ( 99.39)	Acc@3 100.00 (100.00)
Epoch: [43][ 40/412]	Loss 0.06723 (0.0491)	InvT  28.07 ( 28.07)	Acc@1  98.83 ( 99.27)	Acc@3 100.00 ( 99.98)
Epoch: [43][ 60/412]	Loss 0.04657 (0.05288)	InvT  28.08 ( 28.07)	Acc@1  99.22 ( 99.16)	Acc@3 100.00 ( 99.99)
Epoch: [43][ 80/412]	Loss 0.06495 (0.05283)	InvT  28.09 ( 28.07)	Acc@1  98.44 ( 99.16)	Acc@3 100.00 ( 99.99)
Epoch: [43][100/412]	Loss 0.1028 (0.05412)	InvT  28.10 ( 28.08)	Acc@1  97.66 ( 99.15)	Acc@3 100.00 ( 99.99)
Epoch: [43][120/412]	Loss 0.02796 (0.05408)	InvT  28.10 ( 28.08)	Acc@1 100.00 ( 99.15)	Acc@3 100.00 ( 99.99)
Epoch: [43][140/412]	Loss 0.04054 (0.05408)	InvT  28.11 ( 28.08)	Acc@1  99.61 ( 99.16)	Acc@3 100.00 ( 99.99)
Epoch: [43][160/412]	Loss 0.04906 (0.05305)	InvT  28.12 ( 28.09)	Acc@1  99.22 ( 99.18)	Acc@3 100.00 ( 99.99)
Epoch: [43][180/412]	Loss 0.04148 (0.0524)	InvT  28.13 ( 28.09)	Acc@1 100.00 ( 99.20)	Acc@3 100.00 ( 99.99)
Epoch: [43][200/412]	Loss 0.0499 (0.05245)	InvT  28.14 ( 28.10)	Acc@1  99.22 ( 99.19)	Acc@3 100.00 ( 99.99)
Epoch: [43][220/412]	Loss 0.03732 (0.05258)	InvT  28.15 ( 28.10)	Acc@1  99.61 ( 99.18)	Acc@3 100.00 ( 99.99)
Epoch: [43][240/412]	Loss 0.07096 (0.05213)	InvT  28.15 ( 28.10)	Acc@1  98.44 ( 99.20)	Acc@3 100.00 ( 99.99)
Epoch: [43][260/412]	Loss 0.07307 (0.05272)	InvT  28.16 ( 28.11)	Acc@1  98.44 ( 99.19)	Acc@3 100.00 ( 99.99)
Epoch: [43][280/412]	Loss 0.03306 (0.05281)	InvT  28.17 ( 28.11)	Acc@1  99.61 ( 99.18)	Acc@3 100.00 ( 99.99)
Epoch: [43][300/412]	Loss 0.04628 (0.05305)	InvT  28.18 ( 28.12)	Acc@1  98.83 ( 99.17)	Acc@3 100.00 ( 99.99)
Epoch: [43][320/412]	Loss 0.03167 (0.05306)	InvT  28.19 ( 28.12)	Acc@1  99.22 ( 99.17)	Acc@3 100.00 ( 99.99)
Epoch: [43][340/412]	Loss 0.04239 (0.05312)	InvT  28.20 ( 28.13)	Acc@1  99.22 ( 99.17)	Acc@3 100.00 ( 99.99)
Epoch: [43][360/412]	Loss 0.06231 (0.05321)	InvT  28.20 ( 28.13)	Acc@1  99.22 ( 99.18)	Acc@3 100.00 ( 99.99)
Epoch: [43][380/412]	Loss 0.02478 (0.05341)	InvT  28.21 ( 28.13)	Acc@1 100.00 ( 99.17)	Acc@3 100.00 ( 99.99)
Epoch: [43][400/412]	Loss 0.06152 (0.05376)	InvT  28.22 ( 28.14)	Acc@1  98.83 ( 99.17)	Acc@3 100.00 ( 99.99)
Learning rate: 1.699212444357482e-05
Epoch 43, valid metric: {"Acc@1": 38.4, "Acc@3": 52.4, "loss": 3.999}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch43.mdl
Epoch: [44][  0/412]	Loss 0.02503 (0.02503)	InvT  28.23 ( 28.23)	Acc@1 100.00 (100.00)	Acc@3 100.00 (100.00)
Epoch: [44][ 20/412]	Loss 0.03511 (0.04707)	InvT  28.24 ( 28.23)	Acc@1  99.22 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [44][ 40/412]	Loss 0.04597 (0.04856)	InvT  28.24 ( 28.24)	Acc@1  99.61 ( 99.31)	Acc@3 100.00 (100.00)
Epoch: [44][ 60/412]	Loss 0.02143 (0.04932)	InvT  28.25 ( 28.24)	Acc@1 100.00 ( 99.31)	Acc@3 100.00 (100.00)
Epoch: [44][ 80/412]	Loss 0.03149 (0.04776)	InvT  28.26 ( 28.24)	Acc@1 100.00 ( 99.34)	Acc@3 100.00 (100.00)
Epoch: [44][100/412]	Loss 0.04281 (0.04843)	InvT  28.27 ( 28.25)	Acc@1  98.83 ( 99.30)	Acc@3 100.00 (100.00)
Epoch: [44][120/412]	Loss 0.03774 (0.0476)	InvT  28.27 ( 28.25)	Acc@1  99.22 ( 99.31)	Acc@3 100.00 (100.00)
Epoch: [44][140/412]	Loss 0.05217 (0.04872)	InvT  28.28 ( 28.26)	Acc@1  99.61 ( 99.27)	Acc@3 100.00 (100.00)
Epoch: [44][160/412]	Loss 0.05467 (0.04848)	InvT  28.29 ( 28.26)	Acc@1  98.83 ( 99.26)	Acc@3 100.00 (100.00)
Epoch: [44][180/412]	Loss 0.05343 (0.04913)	InvT  28.30 ( 28.26)	Acc@1  99.22 ( 99.25)	Acc@3 100.00 (100.00)
Epoch: [44][200/412]	Loss 0.03598 (0.04933)	InvT  28.31 ( 28.27)	Acc@1  99.61 ( 99.25)	Acc@3 100.00 (100.00)
Epoch: [44][220/412]	Loss 0.08771 (0.05036)	InvT  28.31 ( 28.27)	Acc@1  98.05 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [44][240/412]	Loss 0.1068 (0.05056)	InvT  28.32 ( 28.27)	Acc@1  97.66 ( 99.20)	Acc@3 100.00 (100.00)
Epoch: [44][260/412]	Loss 0.0445 (0.05086)	InvT  28.33 ( 28.28)	Acc@1  99.22 ( 99.19)	Acc@3 100.00 (100.00)
Epoch: [44][280/412]	Loss 0.06769 (0.05122)	InvT  28.34 ( 28.28)	Acc@1  98.44 ( 99.19)	Acc@3 100.00 ( 99.99)
Epoch: [44][300/412]	Loss 0.05529 (0.05076)	InvT  28.35 ( 28.29)	Acc@1  98.83 ( 99.20)	Acc@3 100.00 ( 99.99)
Epoch: [44][320/412]	Loss 0.0358 (0.05095)	InvT  28.35 ( 28.29)	Acc@1  99.61 ( 99.20)	Acc@3 100.00 ( 99.99)
Epoch: [44][340/412]	Loss 0.04258 (0.05106)	InvT  28.36 ( 28.29)	Acc@1  99.61 ( 99.20)	Acc@3 100.00 ( 99.99)
Epoch: [44][360/412]	Loss 0.07295 (0.05107)	InvT  28.37 ( 28.30)	Acc@1  99.22 ( 99.20)	Acc@3 100.00 ( 99.99)
Epoch: [44][380/412]	Loss 0.02213 (0.05105)	InvT  28.38 ( 28.30)	Acc@1 100.00 ( 99.19)	Acc@3 100.00 ( 99.99)
Epoch: [44][400/412]	Loss 0.04655 (0.05096)	InvT  28.39 ( 28.31)	Acc@1  99.22 ( 99.20)	Acc@3 100.00 (100.00)
Learning rate: 1.6689820476446704e-05
Epoch 44, valid metric: {"Acc@1": 40.0, "Acc@3": 53.5, "loss": 3.96}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch44.mdl
Epoch: [45][  0/412]	Loss 0.04836 (0.04836)	InvT  28.39 ( 28.39)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [45][ 20/412]	Loss 0.03846 (0.06029)	InvT  28.40 ( 28.40)	Acc@1  99.61 ( 99.01)	Acc@3 100.00 ( 99.98)
Epoch: [45][ 40/412]	Loss 0.02749 (0.05406)	InvT  28.41 ( 28.40)	Acc@1  99.61 ( 99.16)	Acc@3 100.00 ( 99.97)
Epoch: [45][ 60/412]	Loss 0.06704 (0.05304)	InvT  28.42 ( 28.40)	Acc@1  99.22 ( 99.17)	Acc@3 100.00 ( 99.98)
Epoch: [45][ 80/412]	Loss 0.03856 (0.05133)	InvT  28.42 ( 28.41)	Acc@1  99.22 ( 99.18)	Acc@3 100.00 ( 99.99)
Epoch: [45][100/412]	Loss 0.04808 (0.05015)	InvT  28.43 ( 28.41)	Acc@1  99.61 ( 99.19)	Acc@3 100.00 ( 99.99)
Epoch: [45][120/412]	Loss 0.03542 (0.05026)	InvT  28.44 ( 28.42)	Acc@1  99.61 ( 99.19)	Acc@3 100.00 ( 99.99)
Epoch: [45][140/412]	Loss 0.04496 (0.05011)	InvT  28.45 ( 28.42)	Acc@1  99.22 ( 99.18)	Acc@3 100.00 ( 99.99)
Epoch: [45][160/412]	Loss 0.02692 (0.0499)	InvT  28.46 ( 28.42)	Acc@1 100.00 ( 99.19)	Acc@3 100.00 ( 99.99)
Epoch: [45][180/412]	Loss 0.0298 (0.05026)	InvT  28.46 ( 28.43)	Acc@1  99.61 ( 99.18)	Acc@3 100.00 ( 99.99)
Epoch: [45][200/412]	Loss 0.08194 (0.05005)	InvT  28.47 ( 28.43)	Acc@1  98.83 ( 99.18)	Acc@3 100.00 ( 99.99)
Epoch: [45][220/412]	Loss 0.04385 (0.05032)	InvT  28.48 ( 28.44)	Acc@1  99.22 ( 99.19)	Acc@3 100.00 ( 99.99)
Epoch: [45][240/412]	Loss 0.05574 (0.05005)	InvT  28.49 ( 28.44)	Acc@1  98.44 ( 99.19)	Acc@3 100.00 ( 99.99)
Epoch: [45][260/412]	Loss 0.07804 (0.04985)	InvT  28.50 ( 28.44)	Acc@1  98.05 ( 99.19)	Acc@3 100.00 ( 99.99)
Epoch: [45][280/412]	Loss 0.04996 (0.05003)	InvT  28.50 ( 28.45)	Acc@1  98.83 ( 99.19)	Acc@3 100.00 ( 99.99)
Epoch: [45][300/412]	Loss 0.0399 (0.05066)	InvT  28.51 ( 28.45)	Acc@1  99.61 ( 99.18)	Acc@3 100.00 ( 99.99)
Epoch: [45][320/412]	Loss 0.04627 (0.05125)	InvT  28.52 ( 28.46)	Acc@1  99.61 ( 99.17)	Acc@3 100.00 ( 99.99)
Epoch: [45][340/412]	Loss 0.04342 (0.05106)	InvT  28.53 ( 28.46)	Acc@1 100.00 ( 99.18)	Acc@3 100.00 ( 99.99)
Epoch: [45][360/412]	Loss 0.03538 (0.05086)	InvT  28.53 ( 28.46)	Acc@1  99.61 ( 99.18)	Acc@3 100.00 ( 99.99)
Epoch: [45][380/412]	Loss 0.04825 (0.05128)	InvT  28.54 ( 28.47)	Acc@1  99.22 ( 99.18)	Acc@3 100.00 ( 99.99)
Epoch: [45][400/412]	Loss 0.07374 (0.05174)	InvT  28.55 ( 28.47)	Acc@1  98.44 ( 99.17)	Acc@3 100.00 ( 99.99)
Learning rate: 1.6387516509318593e-05
Epoch 45, valid metric: {"Acc@1": 38.9, "Acc@3": 52.8, "loss": 4.009}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch45.mdl
Epoch: [46][  0/412]	Loss 0.06082 (0.06082)	InvT  28.55 ( 28.55)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [46][ 20/412]	Loss 0.06892 (0.04974)	InvT  28.56 ( 28.56)	Acc@1  98.44 ( 99.20)	Acc@3 100.00 ( 99.98)
Epoch: [46][ 40/412]	Loss 0.06296 (0.05106)	InvT  28.57 ( 28.56)	Acc@1  98.44 ( 99.16)	Acc@3 100.00 ( 99.97)
Epoch: [46][ 60/412]	Loss 0.04615 (0.05228)	InvT  28.58 ( 28.57)	Acc@1  98.83 ( 99.11)	Acc@3 100.00 ( 99.98)
Epoch: [46][ 80/412]	Loss 0.02048 (0.05305)	InvT  28.58 ( 28.57)	Acc@1 100.00 ( 99.09)	Acc@3 100.00 ( 99.98)
Epoch: [46][100/412]	Loss 0.03775 (0.05119)	InvT  28.59 ( 28.57)	Acc@1  99.61 ( 99.14)	Acc@3 100.00 ( 99.98)
Epoch: [46][120/412]	Loss 0.03571 (0.05029)	InvT  28.60 ( 28.58)	Acc@1  99.22 ( 99.17)	Acc@3 100.00 ( 99.99)
Epoch: [46][140/412]	Loss 0.04602 (0.04912)	InvT  28.61 ( 28.58)	Acc@1  98.83 ( 99.21)	Acc@3 100.00 ( 99.99)
Epoch: [46][160/412]	Loss 0.0419 (0.04888)	InvT  28.62 ( 28.59)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 ( 99.99)
Epoch: [46][180/412]	Loss 0.04662 (0.04872)	InvT  28.63 ( 28.59)	Acc@1  99.61 ( 99.23)	Acc@3 100.00 ( 99.99)
Epoch: [46][200/412]	Loss 0.02823 (0.04816)	InvT  28.63 ( 28.59)	Acc@1  99.61 ( 99.23)	Acc@3 100.00 ( 99.99)
Epoch: [46][220/412]	Loss 0.0778 (0.04906)	InvT  28.64 ( 28.60)	Acc@1  98.83 ( 99.22)	Acc@3 100.00 ( 99.99)
Epoch: [46][240/412]	Loss 0.04842 (0.04913)	InvT  28.65 ( 28.60)	Acc@1  99.22 ( 99.23)	Acc@3 100.00 ( 99.99)
Epoch: [46][260/412]	Loss 0.04998 (0.04902)	InvT  28.66 ( 28.61)	Acc@1  98.83 ( 99.23)	Acc@3 100.00 ( 99.99)
Epoch: [46][280/412]	Loss 0.04861 (0.04893)	InvT  28.67 ( 28.61)	Acc@1  99.61 ( 99.23)	Acc@3 100.00 ( 99.99)
Epoch: [46][300/412]	Loss 0.04341 (0.04896)	InvT  28.67 ( 28.61)	Acc@1  99.22 ( 99.23)	Acc@3 100.00 ( 99.99)
Epoch: [46][320/412]	Loss 0.03369 (0.04945)	InvT  28.68 ( 28.62)	Acc@1  99.61 ( 99.23)	Acc@3 100.00 ( 99.99)
Epoch: [46][340/412]	Loss 0.04216 (0.04945)	InvT  28.69 ( 28.62)	Acc@1  99.61 ( 99.23)	Acc@3 100.00 ( 99.99)
Epoch: [46][360/412]	Loss 0.01898 (0.04917)	InvT  28.70 ( 28.63)	Acc@1 100.00 ( 99.24)	Acc@3 100.00 ( 99.99)
Epoch: [46][380/412]	Loss 0.03802 (0.04939)	InvT  28.71 ( 28.63)	Acc@1  99.61 ( 99.24)	Acc@3 100.00 ( 99.99)
Epoch: [46][400/412]	Loss 0.04823 (0.04933)	InvT  28.71 ( 28.63)	Acc@1  99.61 ( 99.23)	Acc@3 100.00 ( 99.99)
Learning rate: 1.608521254219048e-05
Epoch 46, valid metric: {"Acc@1": 40.2, "Acc@3": 53.0, "loss": 4.06}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch46.mdl
Epoch: [47][  0/412]	Loss 0.05999 (0.05999)	InvT  28.72 ( 28.72)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [47][ 20/412]	Loss 0.02649 (0.04595)	InvT  28.73 ( 28.72)	Acc@1  99.22 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [47][ 40/412]	Loss 0.02236 (0.04596)	InvT  28.74 ( 28.73)	Acc@1 100.00 ( 99.30)	Acc@3 100.00 ( 99.99)
Epoch: [47][ 60/412]	Loss 0.05242 (0.0451)	InvT  28.74 ( 28.73)	Acc@1  98.83 ( 99.29)	Acc@3 100.00 ( 99.99)
Epoch: [47][ 80/412]	Loss 0.08274 (0.04744)	InvT  28.75 ( 28.73)	Acc@1  98.44 ( 99.27)	Acc@3 100.00 ( 99.99)
Epoch: [47][100/412]	Loss 0.05949 (0.04798)	InvT  28.76 ( 28.74)	Acc@1  99.22 ( 99.25)	Acc@3 100.00 ( 99.99)
Epoch: [47][120/412]	Loss 0.1008 (0.04892)	InvT  28.76 ( 28.74)	Acc@1  98.44 ( 99.24)	Acc@3  99.61 ( 99.99)
Epoch: [47][140/412]	Loss 0.0468 (0.04848)	InvT  28.77 ( 28.75)	Acc@1  98.83 ( 99.24)	Acc@3 100.00 ( 99.99)
Epoch: [47][160/412]	Loss 0.05567 (0.04835)	InvT  28.78 ( 28.75)	Acc@1  98.83 ( 99.24)	Acc@3 100.00 ( 99.99)
Epoch: [47][180/412]	Loss 0.02357 (0.04832)	InvT  28.79 ( 28.75)	Acc@1 100.00 ( 99.24)	Acc@3 100.00 ( 99.99)
Epoch: [47][200/412]	Loss 0.05884 (0.04796)	InvT  28.80 ( 28.76)	Acc@1 100.00 ( 99.24)	Acc@3 100.00 ( 99.99)
Epoch: [47][220/412]	Loss 0.04911 (0.04832)	InvT  28.80 ( 28.76)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 ( 99.99)
Epoch: [47][240/412]	Loss 0.04036 (0.04791)	InvT  28.81 ( 28.77)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 ( 99.99)
Epoch: [47][260/412]	Loss 0.02051 (0.04788)	InvT  28.82 ( 28.77)	Acc@1  99.61 ( 99.22)	Acc@3 100.00 ( 99.99)
Epoch: [47][280/412]	Loss 0.03882 (0.04739)	InvT  28.83 ( 28.77)	Acc@1  99.61 ( 99.24)	Acc@3 100.00 ( 99.99)
Epoch: [47][300/412]	Loss 0.08042 (0.04722)	InvT  28.84 ( 28.78)	Acc@1  98.44 ( 99.25)	Acc@3 100.00 ( 99.99)
Epoch: [47][320/412]	Loss 0.01911 (0.04712)	InvT  28.84 ( 28.78)	Acc@1 100.00 ( 99.25)	Acc@3 100.00 ( 99.99)
Epoch: [47][340/412]	Loss 0.06628 (0.04766)	InvT  28.85 ( 28.78)	Acc@1  98.05 ( 99.23)	Acc@3 100.00 ( 99.99)
Epoch: [47][360/412]	Loss 0.02021 (0.04755)	InvT  28.86 ( 28.79)	Acc@1  99.22 ( 99.23)	Acc@3 100.00 ( 99.99)
Epoch: [47][380/412]	Loss 0.04123 (0.04735)	InvT  28.87 ( 28.79)	Acc@1  99.22 ( 99.23)	Acc@3 100.00 ( 99.99)
Epoch: [47][400/412]	Loss 0.06635 (0.04742)	InvT  28.88 ( 28.80)	Acc@1  98.44 ( 99.23)	Acc@3 100.00 ( 99.99)
Learning rate: 1.578290857506237e-05
Epoch 47, valid metric: {"Acc@1": 38.7, "Acc@3": 51.9, "loss": 4.017}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch47.mdl
Epoch: [48][  0/412]	Loss 0.07096 (0.07096)	InvT  28.88 ( 28.88)	Acc@1  97.66 ( 97.66)	Acc@3 100.00 (100.00)
Epoch: [48][ 20/412]	Loss 0.0707 (0.04557)	InvT  28.89 ( 28.88)	Acc@1  98.83 ( 99.26)	Acc@3 100.00 (100.00)
Epoch: [48][ 40/412]	Loss 0.01756 (0.04426)	InvT  28.89 ( 28.89)	Acc@1 100.00 ( 99.34)	Acc@3 100.00 ( 99.99)
Epoch: [48][ 60/412]	Loss 0.02739 (0.04143)	InvT  28.90 ( 28.89)	Acc@1  99.61 ( 99.39)	Acc@3 100.00 ( 99.99)
Epoch: [48][ 80/412]	Loss 0.0104 (0.04041)	InvT  28.91 ( 28.89)	Acc@1 100.00 ( 99.38)	Acc@3 100.00 (100.00)
Epoch: [48][100/412]	Loss 0.04736 (0.04038)	InvT  28.92 ( 28.90)	Acc@1  98.44 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [48][120/412]	Loss 0.06069 (0.04066)	InvT  28.92 ( 28.90)	Acc@1  98.83 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [48][140/412]	Loss 0.02978 (0.04009)	InvT  28.93 ( 28.91)	Acc@1 100.00 ( 99.38)	Acc@3 100.00 (100.00)
Epoch: [48][160/412]	Loss 0.06333 (0.03986)	InvT  28.94 ( 28.91)	Acc@1  99.22 ( 99.39)	Acc@3  99.61 (100.00)
Epoch: [48][180/412]	Loss 0.03778 (0.04059)	InvT  28.95 ( 28.91)	Acc@1  99.61 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [48][200/412]	Loss 0.05492 (0.0406)	InvT  28.96 ( 28.92)	Acc@1  98.83 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [48][220/412]	Loss 0.0241 (0.04025)	InvT  28.96 ( 28.92)	Acc@1  99.61 ( 99.38)	Acc@3 100.00 (100.00)
Epoch: [48][240/412]	Loss 0.03139 (0.04049)	InvT  28.97 ( 28.93)	Acc@1 100.00 ( 99.37)	Acc@3 100.00 (100.00)
Epoch: [48][260/412]	Loss 0.03566 (0.04028)	InvT  28.98 ( 28.93)	Acc@1  98.05 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [48][280/412]	Loss 0.0199 (0.04074)	InvT  28.99 ( 28.93)	Acc@1 100.00 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [48][300/412]	Loss 0.03906 (0.04062)	InvT  29.00 ( 28.94)	Acc@1  99.22 ( 99.37)	Acc@3 100.00 (100.00)
Epoch: [48][320/412]	Loss 0.05903 (0.04112)	InvT  29.00 ( 28.94)	Acc@1  99.22 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [48][340/412]	Loss 0.02657 (0.04163)	InvT  29.01 ( 28.94)	Acc@1 100.00 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [48][360/412]	Loss 0.05901 (0.04199)	InvT  29.02 ( 28.95)	Acc@1  98.44 ( 99.34)	Acc@3 100.00 (100.00)
Epoch: [48][380/412]	Loss 0.06473 (0.04214)	InvT  29.03 ( 28.95)	Acc@1  99.22 ( 99.34)	Acc@3 100.00 (100.00)
Epoch: [48][400/412]	Loss 0.0584 (0.0423)	InvT  29.03 ( 28.96)	Acc@1  99.61 ( 99.33)	Acc@3 100.00 (100.00)
Learning rate: 1.5480604607934255e-05
Epoch 48, valid metric: {"Acc@1": 39.3, "Acc@3": 52.5, "loss": 4.028}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch48.mdl
Epoch: [49][  0/412]	Loss 0.0436 (0.0436)	InvT  29.04 ( 29.04)	Acc@1  98.83 ( 98.83)	Acc@3 100.00 (100.00)
Epoch: [49][ 20/412]	Loss 0.01803 (0.03471)	InvT  29.05 ( 29.04)	Acc@1 100.00 ( 99.50)	Acc@3 100.00 (100.00)
Epoch: [49][ 40/412]	Loss 0.02771 (0.03984)	InvT  29.05 ( 29.05)	Acc@1  99.61 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [49][ 60/412]	Loss 0.02735 (0.04103)	InvT  29.06 ( 29.05)	Acc@1 100.00 ( 99.36)	Acc@3 100.00 ( 99.99)
Epoch: [49][ 80/412]	Loss 0.04943 (0.04117)	InvT  29.07 ( 29.05)	Acc@1  99.22 ( 99.34)	Acc@3 100.00 (100.00)
Epoch: [49][100/412]	Loss 0.02408 (0.04113)	InvT  29.07 ( 29.06)	Acc@1 100.00 ( 99.33)	Acc@3 100.00 (100.00)
Epoch: [49][120/412]	Loss 0.03952 (0.04072)	InvT  29.08 ( 29.06)	Acc@1  99.22 ( 99.34)	Acc@3 100.00 (100.00)
Epoch: [49][140/412]	Loss 0.05294 (0.04073)	InvT  29.09 ( 29.06)	Acc@1  99.22 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [49][160/412]	Loss 0.04583 (0.04123)	InvT  29.10 ( 29.07)	Acc@1  99.22 ( 99.34)	Acc@3 100.00 (100.00)
Epoch: [49][180/412]	Loss 0.04581 (0.04106)	InvT  29.10 ( 29.07)	Acc@1  98.83 ( 99.35)	Acc@3  99.61 (100.00)
Epoch: [49][200/412]	Loss 0.0571 (0.04067)	InvT  29.11 ( 29.07)	Acc@1  98.44 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [49][220/412]	Loss 0.01855 (0.04067)	InvT  29.12 ( 29.08)	Acc@1 100.00 ( 99.35)	Acc@3 100.00 ( 99.99)
Epoch: [49][240/412]	Loss 0.01593 (0.04061)	InvT  29.13 ( 29.08)	Acc@1 100.00 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [49][260/412]	Loss 0.0442 (0.04062)	InvT  29.14 ( 29.09)	Acc@1  99.22 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [49][280/412]	Loss 0.04132 (0.04037)	InvT  29.14 ( 29.09)	Acc@1  99.61 ( 99.37)	Acc@3 100.00 (100.00)
Epoch: [49][300/412]	Loss 0.03757 (0.04051)	InvT  29.15 ( 29.09)	Acc@1  99.61 ( 99.37)	Acc@3 100.00 (100.00)
Epoch: [49][320/412]	Loss 0.01848 (0.04089)	InvT  29.16 ( 29.10)	Acc@1 100.00 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [49][340/412]	Loss 0.05473 (0.04073)	InvT  29.17 ( 29.10)	Acc@1  98.44 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [49][360/412]	Loss 0.0454 (0.04079)	InvT  29.17 ( 29.10)	Acc@1  99.61 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [49][380/412]	Loss 0.03914 (0.0407)	InvT  29.18 ( 29.11)	Acc@1  99.61 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [49][400/412]	Loss 0.023 (0.04068)	InvT  29.19 ( 29.11)	Acc@1 100.00 ( 99.36)	Acc@3 100.00 (100.00)
Learning rate: 1.5178300640806144e-05
Epoch 49, valid metric: {"Acc@1": 38.7, "Acc@3": 51.7, "loss": 4.093}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch49.mdl
Epoch: [50][  0/412]	Loss 0.0314 (0.0314)	InvT  29.19 ( 29.19)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
Epoch: [50][ 20/412]	Loss 0.0348 (0.03601)	InvT  29.20 ( 29.20)	Acc@1  99.61 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [50][ 40/412]	Loss 0.03442 (0.04003)	InvT  29.21 ( 29.20)	Acc@1  99.22 ( 99.28)	Acc@3 100.00 (100.00)
Epoch: [50][ 60/412]	Loss 0.03045 (0.03996)	InvT  29.21 ( 29.20)	Acc@1 100.00 ( 99.30)	Acc@3 100.00 (100.00)
Epoch: [50][ 80/412]	Loss 0.0379 (0.03947)	InvT  29.22 ( 29.21)	Acc@1  99.22 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [50][100/412]	Loss 0.03848 (0.03934)	InvT  29.23 ( 29.21)	Acc@1  99.22 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [50][120/412]	Loss 0.07582 (0.03943)	InvT  29.24 ( 29.21)	Acc@1  98.44 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [50][140/412]	Loss 0.07707 (0.03958)	InvT  29.24 ( 29.22)	Acc@1  99.22 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [50][160/412]	Loss 0.01772 (0.03997)	InvT  29.25 ( 29.22)	Acc@1 100.00 ( 99.33)	Acc@3 100.00 (100.00)
Epoch: [50][180/412]	Loss 0.03105 (0.0398)	InvT  29.26 ( 29.23)	Acc@1  99.22 ( 99.33)	Acc@3 100.00 (100.00)
Epoch: [50][200/412]	Loss 0.03559 (0.04)	InvT  29.26 ( 29.23)	Acc@1 100.00 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [50][220/412]	Loss 0.07191 (0.04056)	InvT  29.27 ( 29.23)	Acc@1  98.44 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [50][240/412]	Loss 0.04389 (0.0406)	InvT  29.28 ( 29.24)	Acc@1  98.83 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [50][260/412]	Loss 0.02334 (0.04083)	InvT  29.29 ( 29.24)	Acc@1  99.61 ( 99.34)	Acc@3 100.00 (100.00)
Epoch: [50][280/412]	Loss 0.01731 (0.04079)	InvT  29.29 ( 29.24)	Acc@1 100.00 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [50][300/412]	Loss 0.02858 (0.0408)	InvT  29.30 ( 29.25)	Acc@1 100.00 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [50][320/412]	Loss 0.03241 (0.0405)	InvT  29.31 ( 29.25)	Acc@1  99.61 ( 99.37)	Acc@3 100.00 (100.00)
Epoch: [50][340/412]	Loss 0.05748 (0.04042)	InvT  29.32 ( 29.25)	Acc@1  98.83 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [50][360/412]	Loss 0.02189 (0.04012)	InvT  29.33 ( 29.26)	Acc@1  99.61 ( 99.37)	Acc@3 100.00 (100.00)
Epoch: [50][380/412]	Loss 0.03355 (0.03996)	InvT  29.33 ( 29.26)	Acc@1  99.61 ( 99.37)	Acc@3 100.00 (100.00)
Epoch: [50][400/412]	Loss 0.04979 (0.04018)	InvT  29.34 ( 29.27)	Acc@1  99.22 ( 99.37)	Acc@3 100.00 (100.00)
Learning rate: 1.4875996673678031e-05
Epoch 50, valid metric: {"Acc@1": 38.7, "Acc@3": 51.8, "loss": 4.094}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch50.mdl
Epoch: [51][  0/412]	Loss 0.03889 (0.03889)	InvT  29.35 ( 29.35)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [51][ 20/412]	Loss 0.09512 (0.03999)	InvT  29.35 ( 29.35)	Acc@1  98.05 ( 99.35)	Acc@3 100.00 (100.00)
Epoch: [51][ 40/412]	Loss 0.0595 (0.04049)	InvT  29.36 ( 29.35)	Acc@1  98.44 ( 99.29)	Acc@3 100.00 (100.00)
Epoch: [51][ 60/412]	Loss 0.06149 (0.03737)	InvT  29.37 ( 29.36)	Acc@1  98.44 ( 99.39)	Acc@3 100.00 (100.00)
Epoch: [51][ 80/412]	Loss 0.03014 (0.03631)	InvT  29.38 ( 29.36)	Acc@1  99.61 ( 99.42)	Acc@3 100.00 (100.00)
Epoch: [51][100/412]	Loss 0.02638 (0.03594)	InvT  29.38 ( 29.36)	Acc@1  99.61 ( 99.44)	Acc@3 100.00 (100.00)
Epoch: [51][120/412]	Loss 0.03305 (0.0362)	InvT  29.39 ( 29.37)	Acc@1  99.22 ( 99.43)	Acc@3  99.61 ( 99.99)
Epoch: [51][140/412]	Loss 0.06255 (0.03648)	InvT  29.40 ( 29.37)	Acc@1  98.44 ( 99.42)	Acc@3 100.00 ( 99.99)
Epoch: [51][160/412]	Loss 0.03532 (0.03636)	InvT  29.41 ( 29.38)	Acc@1  98.83 ( 99.43)	Acc@3 100.00 (100.00)
Epoch: [51][180/412]	Loss 0.0234 (0.03694)	InvT  29.41 ( 29.38)	Acc@1  99.61 ( 99.42)	Acc@3 100.00 (100.00)
Epoch: [51][200/412]	Loss 0.02078 (0.0374)	InvT  29.42 ( 29.38)	Acc@1  99.61 ( 99.40)	Acc@3 100.00 (100.00)
Epoch: [51][220/412]	Loss 0.05151 (0.03789)	InvT  29.43 ( 29.39)	Acc@1  99.61 ( 99.39)	Acc@3 100.00 (100.00)
Epoch: [51][240/412]	Loss 0.04426 (0.03804)	InvT  29.44 ( 29.39)	Acc@1  98.83 ( 99.37)	Acc@3 100.00 (100.00)
Epoch: [51][260/412]	Loss 0.04551 (0.03835)	InvT  29.44 ( 29.39)	Acc@1  99.22 ( 99.36)	Acc@3 100.00 (100.00)
Epoch: [51][280/412]	Loss 0.02394 (0.03815)	InvT  29.45 ( 29.40)	Acc@1 100.00 ( 99.37)	Acc@3 100.00 (100.00)
Epoch: [51][300/412]	Loss 0.01696 (0.03785)	InvT  29.46 ( 29.40)	Acc@1 100.00 ( 99.38)	Acc@3 100.00 (100.00)
Epoch: [51][320/412]	Loss 0.03929 (0.03812)	InvT  29.46 ( 29.41)	Acc@1  99.61 ( 99.38)	Acc@3 100.00 (100.00)
Epoch: [51][340/412]	Loss 0.02471 (0.0386)	InvT  29.47 ( 29.41)	Acc@1  99.22 ( 99.37)	Acc@3 100.00 ( 99.99)
Epoch: [51][360/412]	Loss 0.02747 (0.03878)	InvT  29.48 ( 29.41)	Acc@1  99.61 ( 99.37)	Acc@3 100.00 ( 99.99)
Epoch: [51][380/412]	Loss 0.02307 (0.03883)	InvT  29.49 ( 29.42)	Acc@1  99.61 ( 99.37)	Acc@3 100.00 ( 99.99)
Epoch: [51][400/412]	Loss 0.02504 (0.03908)	InvT  29.49 ( 29.42)	Acc@1  99.22 ( 99.37)	Acc@3 100.00 (100.00)
Learning rate: 1.457369270654992e-05
Epoch 51, valid metric: {"Acc@1": 37.7, "Acc@3": 51.1, "loss": 4.058}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch51.mdl
Epoch: [52][  0/412]	Loss 0.0513 (0.0513)	InvT  29.50 ( 29.50)	Acc@1  98.83 ( 98.83)	Acc@3 100.00 (100.00)
Epoch: [52][ 20/412]	Loss 0.0429 (0.033)	InvT  29.51 ( 29.50)	Acc@1  99.61 ( 99.46)	Acc@3 100.00 ( 99.98)
Epoch: [52][ 40/412]	Loss 0.01263 (0.03504)	InvT  29.51 ( 29.51)	Acc@1 100.00 ( 99.48)	Acc@3 100.00 ( 99.99)
Epoch: [52][ 60/412]	Loss 0.01939 (0.03662)	InvT  29.52 ( 29.51)	Acc@1  99.61 ( 99.43)	Acc@3 100.00 ( 99.99)
Epoch: [52][ 80/412]	Loss 0.02712 (0.03682)	InvT  29.53 ( 29.51)	Acc@1  98.83 ( 99.41)	Acc@3 100.00 (100.00)
Epoch: [52][100/412]	Loss 0.04511 (0.03648)	InvT  29.54 ( 29.52)	Acc@1  98.83 ( 99.42)	Acc@3 100.00 (100.00)
Epoch: [52][120/412]	Loss 0.02891 (0.03636)	InvT  29.54 ( 29.52)	Acc@1  99.61 ( 99.42)	Acc@3 100.00 (100.00)
Epoch: [52][140/412]	Loss 0.03837 (0.036)	InvT  29.55 ( 29.52)	Acc@1  98.83 ( 99.43)	Acc@3 100.00 (100.00)
Epoch: [52][160/412]	Loss 0.02535 (0.03583)	InvT  29.56 ( 29.53)	Acc@1  99.61 ( 99.43)	Acc@3 100.00 (100.00)
Epoch: [52][180/412]	Loss 0.05096 (0.03538)	InvT  29.57 ( 29.53)	Acc@1  99.22 ( 99.44)	Acc@3 100.00 (100.00)
Epoch: [52][200/412]	Loss 0.08472 (0.03618)	InvT  29.57 ( 29.54)	Acc@1  98.44 ( 99.43)	Acc@3 100.00 (100.00)
Epoch: [52][220/412]	Loss 0.03088 (0.036)	InvT  29.58 ( 29.54)	Acc@1  99.61 ( 99.43)	Acc@3 100.00 (100.00)
Epoch: [52][240/412]	Loss 0.03361 (0.0365)	InvT  29.59 ( 29.54)	Acc@1  99.22 ( 99.42)	Acc@3 100.00 (100.00)
Epoch: [52][260/412]	Loss 0.1077 (0.03727)	InvT  29.59 ( 29.55)	Acc@1  98.83 ( 99.41)	Acc@3 100.00 (100.00)
Epoch: [52][280/412]	Loss 0.04141 (0.03718)	InvT  29.60 ( 29.55)	Acc@1  98.83 ( 99.41)	Acc@3 100.00 (100.00)
Epoch: [52][300/412]	Loss 0.04251 (0.03764)	InvT  29.61 ( 29.55)	Acc@1  98.83 ( 99.40)	Acc@3 100.00 (100.00)
Epoch: [52][320/412]	Loss 0.02514 (0.03792)	InvT  29.61 ( 29.56)	Acc@1 100.00 ( 99.40)	Acc@3 100.00 (100.00)
Epoch: [52][340/412]	Loss 0.01358 (0.03794)	InvT  29.62 ( 29.56)	Acc@1 100.00 ( 99.39)	Acc@3 100.00 (100.00)
Epoch: [52][360/412]	Loss 0.04783 (0.03764)	InvT  29.63 ( 29.56)	Acc@1  99.61 ( 99.40)	Acc@3 100.00 (100.00)
Epoch: [52][380/412]	Loss 0.05561 (0.03778)	InvT  29.64 ( 29.57)	Acc@1  99.22 ( 99.40)	Acc@3 100.00 (100.00)
Epoch: [52][400/412]	Loss 0.02327 (0.03802)	InvT  29.65 ( 29.57)	Acc@1  99.22 ( 99.39)	Acc@3 100.00 (100.00)
Learning rate: 1.4271388739421807e-05
Epoch 52, valid metric: {"Acc@1": 38.2, "Acc@3": 51.9, "loss": 4.136}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch52.mdl
Epoch: [53][  0/412]	Loss 0.02853 (0.02853)	InvT  29.65 ( 29.65)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [53][ 20/412]	Loss 0.05002 (0.03487)	InvT  29.66 ( 29.65)	Acc@1  98.83 ( 99.40)	Acc@3 100.00 ( 99.98)
Epoch: [53][ 40/412]	Loss 0.03938 (0.03701)	InvT  29.66 ( 29.66)	Acc@1  98.44 ( 99.34)	Acc@3 100.00 ( 99.99)
Epoch: [53][ 60/412]	Loss 0.09297 (0.03675)	InvT  29.67 ( 29.66)	Acc@1  98.83 ( 99.41)	Acc@3 100.00 ( 99.99)
Epoch: [53][ 80/412]	Loss 0.0303 (0.03637)	InvT  29.68 ( 29.66)	Acc@1 100.00 ( 99.42)	Acc@3 100.00 (100.00)
Epoch: [53][100/412]	Loss 0.02967 (0.03624)	InvT  29.69 ( 29.67)	Acc@1  99.61 ( 99.42)	Acc@3 100.00 (100.00)
Epoch: [53][120/412]	Loss 0.02988 (0.0361)	InvT  29.69 ( 29.67)	Acc@1  99.22 ( 99.42)	Acc@3 100.00 (100.00)
Epoch: [53][140/412]	Loss 0.05337 (0.0365)	InvT  29.70 ( 29.68)	Acc@1  98.44 ( 99.39)	Acc@3 100.00 ( 99.99)
Epoch: [53][160/412]	Loss 0.06879 (0.03694)	InvT  29.71 ( 29.68)	Acc@1  98.44 ( 99.39)	Acc@3 100.00 (100.00)
Epoch: [53][180/412]	Loss 0.03208 (0.03647)	InvT  29.72 ( 29.68)	Acc@1  99.61 ( 99.41)	Acc@3 100.00 (100.00)
Epoch: [53][200/412]	Loss 0.08019 (0.03678)	InvT  29.72 ( 29.69)	Acc@1  99.61 ( 99.40)	Acc@3 100.00 (100.00)
Epoch: [53][220/412]	Loss 0.035 (0.03657)	InvT  29.73 ( 29.69)	Acc@1  99.61 ( 99.41)	Acc@3 100.00 (100.00)
Epoch: [53][240/412]	Loss 0.02407 (0.03685)	InvT  29.74 ( 29.69)	Acc@1  99.61 ( 99.41)	Acc@3 100.00 (100.00)
Epoch: [53][260/412]	Loss 0.04235 (0.03762)	InvT  29.74 ( 29.70)	Acc@1  99.61 ( 99.40)	Acc@3 100.00 (100.00)
Epoch: [53][280/412]	Loss 0.05093 (0.03781)	InvT  29.75 ( 29.70)	Acc@1  99.22 ( 99.39)	Acc@3 100.00 (100.00)
Epoch: [53][300/412]	Loss 0.03053 (0.03771)	InvT  29.76 ( 29.70)	Acc@1  99.61 ( 99.39)	Acc@3 100.00 (100.00)
Epoch: [53][320/412]	Loss 0.04883 (0.03807)	InvT  29.77 ( 29.71)	Acc@1  98.83 ( 99.37)	Acc@3 100.00 (100.00)
Epoch: [53][340/412]	Loss 0.03361 (0.03817)	InvT  29.77 ( 29.71)	Acc@1  99.22 ( 99.37)	Acc@3 100.00 (100.00)
Epoch: [53][360/412]	Loss 0.02332 (0.03812)	InvT  29.78 ( 29.72)	Acc@1 100.00 ( 99.38)	Acc@3 100.00 (100.00)
Epoch: [53][380/412]	Loss 0.05418 (0.03801)	InvT  29.79 ( 29.72)	Acc@1  99.22 ( 99.38)	Acc@3 100.00 (100.00)
Epoch: [53][400/412]	Loss 0.0393 (0.03799)	InvT  29.79 ( 29.72)	Acc@1  98.83 ( 99.38)	Acc@3 100.00 (100.00)
Learning rate: 1.3969084772293695e-05
Epoch 53, valid metric: {"Acc@1": 39.0, "Acc@3": 51.6, "loss": 4.113}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch53.mdl
Epoch: [54][  0/412]	Loss 0.02204 (0.02204)	InvT  29.80 ( 29.80)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
Epoch: [54][ 20/412]	Loss 0.02652 (0.03566)	InvT  29.81 ( 29.80)	Acc@1  99.61 ( 99.52)	Acc@3 100.00 ( 99.98)
Epoch: [54][ 40/412]	Loss 0.03672 (0.03474)	InvT  29.81 ( 29.81)	Acc@1  98.83 ( 99.47)	Acc@3 100.00 ( 99.99)
Epoch: [54][ 60/412]	Loss 0.01776 (0.03457)	InvT  29.82 ( 29.81)	Acc@1  99.61 ( 99.44)	Acc@3 100.00 ( 99.99)
Epoch: [54][ 80/412]	Loss 0.04497 (0.0342)	InvT  29.83 ( 29.81)	Acc@1  98.83 ( 99.48)	Acc@3 100.00 ( 99.99)
Epoch: [54][100/412]	Loss 0.03464 (0.03414)	InvT  29.83 ( 29.82)	Acc@1  99.61 ( 99.48)	Acc@3 100.00 ( 99.99)
Epoch: [54][120/412]	Loss 0.05742 (0.03637)	InvT  29.84 ( 29.82)	Acc@1  99.61 ( 99.42)	Acc@3  99.61 ( 99.99)
Epoch: [54][140/412]	Loss 0.07819 (0.03704)	InvT  29.85 ( 29.82)	Acc@1  98.05 ( 99.40)	Acc@3  99.61 ( 99.98)
Epoch: [54][160/412]	Loss 0.05972 (0.03713)	InvT  29.85 ( 29.83)	Acc@1  98.05 ( 99.39)	Acc@3 100.00 ( 99.99)
Epoch: [54][180/412]	Loss 0.05962 (0.03739)	InvT  29.86 ( 29.83)	Acc@1  98.83 ( 99.38)	Acc@3  99.61 ( 99.98)
Epoch: [54][200/412]	Loss 0.03413 (0.03674)	InvT  29.87 ( 29.83)	Acc@1  99.22 ( 99.40)	Acc@3 100.00 ( 99.99)
Epoch: [54][220/412]	Loss 0.01439 (0.03689)	InvT  29.87 ( 29.84)	Acc@1 100.00 ( 99.41)	Acc@3 100.00 ( 99.99)
Epoch: [54][240/412]	Loss 0.01491 (0.03666)	InvT  29.88 ( 29.84)	Acc@1  99.61 ( 99.41)	Acc@3 100.00 ( 99.99)
Epoch: [54][260/412]	Loss 0.05066 (0.03668)	InvT  29.89 ( 29.84)	Acc@1  99.22 ( 99.41)	Acc@3 100.00 ( 99.99)
Epoch: [54][280/412]	Loss 0.04069 (0.03646)	InvT  29.89 ( 29.85)	Acc@1  99.22 ( 99.42)	Acc@3 100.00 ( 99.99)
Epoch: [54][300/412]	Loss 0.02315 (0.03628)	InvT  29.90 ( 29.85)	Acc@1 100.00 ( 99.42)	Acc@3 100.00 ( 99.99)
Epoch: [54][320/412]	Loss 0.03696 (0.0359)	InvT  29.91 ( 29.85)	Acc@1  99.61 ( 99.43)	Acc@3 100.00 ( 99.99)
Epoch: [54][340/412]	Loss 0.05117 (0.03598)	InvT  29.92 ( 29.86)	Acc@1  99.61 ( 99.43)	Acc@3 100.00 ( 99.99)
Epoch: [54][360/412]	Loss 0.06034 (0.03628)	InvT  29.92 ( 29.86)	Acc@1  98.83 ( 99.42)	Acc@3 100.00 ( 99.99)
Epoch: [54][380/412]	Loss 0.03656 (0.0363)	InvT  29.93 ( 29.86)	Acc@1  99.61 ( 99.42)	Acc@3 100.00 ( 99.99)
Epoch: [54][400/412]	Loss 0.05874 (0.03633)	InvT  29.94 ( 29.87)	Acc@1  98.83 ( 99.41)	Acc@3 100.00 ( 99.99)
Learning rate: 1.3666780805165582e-05
Epoch 54, valid metric: {"Acc@1": 37.1, "Acc@3": 52.3, "loss": 4.112}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch54.mdl
Epoch: [55][  0/412]	Loss 0.0386 (0.0386)	InvT  29.94 ( 29.94)	Acc@1  99.61 ( 99.61)	Acc@3 100.00 (100.00)
Epoch: [55][ 20/412]	Loss 0.02881 (0.03681)	InvT  29.95 ( 29.95)	Acc@1 100.00 ( 99.46)	Acc@3 100.00 (100.00)
Epoch: [55][ 40/412]	Loss 0.02674 (0.0384)	InvT  29.96 ( 29.95)	Acc@1  99.22 ( 99.39)	Acc@3 100.00 (100.00)
Epoch: [55][ 60/412]	Loss 0.01515 (0.03698)	InvT  29.96 ( 29.95)	Acc@1 100.00 ( 99.46)	Acc@3 100.00 (100.00)
Epoch: [55][ 80/412]	Loss 0.01599 (0.0367)	InvT  29.97 ( 29.96)	Acc@1 100.00 ( 99.44)	Acc@3 100.00 (100.00)
Epoch: [55][100/412]	Loss 0.05162 (0.03596)	InvT  29.98 ( 29.96)	Acc@1  99.22 ( 99.45)	Acc@3 100.00 (100.00)
Epoch: [55][120/412]	Loss 0.03611 (0.03586)	InvT  29.98 ( 29.96)	Acc@1  99.61 ( 99.43)	Acc@3 100.00 (100.00)
Epoch: [55][140/412]	Loss 0.07823 (0.03645)	InvT  29.99 ( 29.97)	Acc@1  99.61 ( 99.40)	Acc@3 100.00 (100.00)
Epoch: [55][160/412]	Loss 0.04642 (0.03609)	InvT  30.00 ( 29.97)	Acc@1  99.61 ( 99.41)	Acc@3 100.00 (100.00)
Epoch: [55][180/412]	Loss 0.0927 (0.03671)	InvT  30.00 ( 29.97)	Acc@1  98.05 ( 99.38)	Acc@3 100.00 (100.00)
Epoch: [55][200/412]	Loss 0.0263 (0.03687)	InvT  30.01 ( 29.98)	Acc@1 100.00 ( 99.39)	Acc@3 100.00 (100.00)
Epoch: [55][220/412]	Loss 0.03155 (0.03699)	InvT  30.02 ( 29.98)	Acc@1  99.22 ( 99.39)	Acc@3 100.00 (100.00)
Epoch: [55][240/412]	Loss 0.0187 (0.03651)	InvT  30.02 ( 29.98)	Acc@1  99.61 ( 99.41)	Acc@3 100.00 (100.00)
Epoch: [55][260/412]	Loss 0.03978 (0.03679)	InvT  30.03 ( 29.99)	Acc@1  99.61 ( 99.41)	Acc@3 100.00 (100.00)
Epoch: [55][280/412]	Loss 0.05396 (0.03703)	InvT  30.04 ( 29.99)	Acc@1  99.22 ( 99.41)	Acc@3 100.00 (100.00)
Epoch: [55][300/412]	Loss 0.01463 (0.03671)	InvT  30.04 ( 29.99)	Acc@1 100.00 ( 99.41)	Acc@3 100.00 (100.00)
Epoch: [55][320/412]	Loss 0.04477 (0.03639)	InvT  30.05 ( 30.00)	Acc@1  98.83 ( 99.42)	Acc@3 100.00 (100.00)
Epoch: [55][340/412]	Loss 0.04787 (0.03654)	InvT  30.06 ( 30.00)	Acc@1  98.83 ( 99.42)	Acc@3 100.00 (100.00)
Epoch: [55][360/412]	Loss 0.03127 (0.03639)	InvT  30.06 ( 30.00)	Acc@1  99.61 ( 99.42)	Acc@3 100.00 (100.00)
Epoch: [55][380/412]	Loss 0.05203 (0.03641)	InvT  30.07 ( 30.01)	Acc@1  99.22 ( 99.42)	Acc@3 100.00 (100.00)
Epoch: [55][400/412]	Loss 0.04106 (0.03654)	InvT  30.08 ( 30.01)	Acc@1  99.61 ( 99.42)	Acc@3 100.00 (100.00)
Learning rate: 1.336447683803747e-05
Epoch 55, valid metric: {"Acc@1": 37.9, "Acc@3": 51.2, "loss": 4.129}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch55.mdl
Lost patience at 55 epoch. Best epoch at 46
=> creating model
CustomBertModel(
  (hr_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (tail_bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
)
log_inv_t: 1.0
hr_bert.embeddings.word_embeddings.weight: 91812096
hr_bert.embeddings.position_embeddings.weight: 393216
hr_bert.embeddings.token_type_embeddings.weight: 1536
hr_bert.embeddings.LayerNorm.weight: 768
hr_bert.embeddings.LayerNorm.bias: 768
hr_bert.encoder.layer.0.attention.self.query.weight: 589824
hr_bert.encoder.layer.0.attention.self.query.bias: 768
hr_bert.encoder.layer.0.attention.self.key.weight: 589824
hr_bert.encoder.layer.0.attention.self.key.bias: 768
hr_bert.encoder.layer.0.attention.self.value.weight: 589824
hr_bert.encoder.layer.0.attention.self.value.bias: 768
hr_bert.encoder.layer.0.attention.output.dense.weight: 589824
hr_bert.encoder.layer.0.attention.output.dense.bias: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.0.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.0.intermediate.dense.bias: 3072
hr_bert.encoder.layer.0.output.dense.weight: 2359296
hr_bert.encoder.layer.0.output.dense.bias: 768
hr_bert.encoder.layer.0.output.LayerNorm.weight: 768
hr_bert.encoder.layer.0.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.attention.self.query.weight: 589824
hr_bert.encoder.layer.1.attention.self.query.bias: 768
hr_bert.encoder.layer.1.attention.self.key.weight: 589824
hr_bert.encoder.layer.1.attention.self.key.bias: 768
hr_bert.encoder.layer.1.attention.self.value.weight: 589824
hr_bert.encoder.layer.1.attention.self.value.bias: 768
hr_bert.encoder.layer.1.attention.output.dense.weight: 589824
hr_bert.encoder.layer.1.attention.output.dense.bias: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.1.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.1.intermediate.dense.bias: 3072
hr_bert.encoder.layer.1.output.dense.weight: 2359296
hr_bert.encoder.layer.1.output.dense.bias: 768
hr_bert.encoder.layer.1.output.LayerNorm.weight: 768
hr_bert.encoder.layer.1.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.attention.self.query.weight: 589824
hr_bert.encoder.layer.2.attention.self.query.bias: 768
hr_bert.encoder.layer.2.attention.self.key.weight: 589824
hr_bert.encoder.layer.2.attention.self.key.bias: 768
hr_bert.encoder.layer.2.attention.self.value.weight: 589824
hr_bert.encoder.layer.2.attention.self.value.bias: 768
hr_bert.encoder.layer.2.attention.output.dense.weight: 589824
hr_bert.encoder.layer.2.attention.output.dense.bias: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.2.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.2.intermediate.dense.bias: 3072
hr_bert.encoder.layer.2.output.dense.weight: 2359296
hr_bert.encoder.layer.2.output.dense.bias: 768
hr_bert.encoder.layer.2.output.LayerNorm.weight: 768
hr_bert.encoder.layer.2.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.attention.self.query.weight: 589824
hr_bert.encoder.layer.3.attention.self.query.bias: 768
hr_bert.encoder.layer.3.attention.self.key.weight: 589824
hr_bert.encoder.layer.3.attention.self.key.bias: 768
hr_bert.encoder.layer.3.attention.self.value.weight: 589824
hr_bert.encoder.layer.3.attention.self.value.bias: 768
hr_bert.encoder.layer.3.attention.output.dense.weight: 589824
hr_bert.encoder.layer.3.attention.output.dense.bias: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.3.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.3.intermediate.dense.bias: 3072
hr_bert.encoder.layer.3.output.dense.weight: 2359296
hr_bert.encoder.layer.3.output.dense.bias: 768
hr_bert.encoder.layer.3.output.LayerNorm.weight: 768
hr_bert.encoder.layer.3.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.attention.self.query.weight: 589824
hr_bert.encoder.layer.4.attention.self.query.bias: 768
hr_bert.encoder.layer.4.attention.self.key.weight: 589824
hr_bert.encoder.layer.4.attention.self.key.bias: 768
hr_bert.encoder.layer.4.attention.self.value.weight: 589824
hr_bert.encoder.layer.4.attention.self.value.bias: 768
hr_bert.encoder.layer.4.attention.output.dense.weight: 589824
hr_bert.encoder.layer.4.attention.output.dense.bias: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.4.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.4.intermediate.dense.bias: 3072
hr_bert.encoder.layer.4.output.dense.weight: 2359296
hr_bert.encoder.layer.4.output.dense.bias: 768
hr_bert.encoder.layer.4.output.LayerNorm.weight: 768
hr_bert.encoder.layer.4.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.attention.self.query.weight: 589824
hr_bert.encoder.layer.5.attention.self.query.bias: 768
hr_bert.encoder.layer.5.attention.self.key.weight: 589824
hr_bert.encoder.layer.5.attention.self.key.bias: 768
hr_bert.encoder.layer.5.attention.self.value.weight: 589824
hr_bert.encoder.layer.5.attention.self.value.bias: 768
hr_bert.encoder.layer.5.attention.output.dense.weight: 589824
hr_bert.encoder.layer.5.attention.output.dense.bias: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.5.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.5.intermediate.dense.bias: 3072
hr_bert.encoder.layer.5.output.dense.weight: 2359296
hr_bert.encoder.layer.5.output.dense.bias: 768
hr_bert.encoder.layer.5.output.LayerNorm.weight: 768
hr_bert.encoder.layer.5.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.attention.self.query.weight: 589824
hr_bert.encoder.layer.6.attention.self.query.bias: 768
hr_bert.encoder.layer.6.attention.self.key.weight: 589824
hr_bert.encoder.layer.6.attention.self.key.bias: 768
hr_bert.encoder.layer.6.attention.self.value.weight: 589824
hr_bert.encoder.layer.6.attention.self.value.bias: 768
hr_bert.encoder.layer.6.attention.output.dense.weight: 589824
hr_bert.encoder.layer.6.attention.output.dense.bias: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.6.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.6.intermediate.dense.bias: 3072
hr_bert.encoder.layer.6.output.dense.weight: 2359296
hr_bert.encoder.layer.6.output.dense.bias: 768
hr_bert.encoder.layer.6.output.LayerNorm.weight: 768
hr_bert.encoder.layer.6.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.attention.self.query.weight: 589824
hr_bert.encoder.layer.7.attention.self.query.bias: 768
hr_bert.encoder.layer.7.attention.self.key.weight: 589824
hr_bert.encoder.layer.7.attention.self.key.bias: 768
hr_bert.encoder.layer.7.attention.self.value.weight: 589824
hr_bert.encoder.layer.7.attention.self.value.bias: 768
hr_bert.encoder.layer.7.attention.output.dense.weight: 589824
hr_bert.encoder.layer.7.attention.output.dense.bias: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.7.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.7.intermediate.dense.bias: 3072
hr_bert.encoder.layer.7.output.dense.weight: 2359296
hr_bert.encoder.layer.7.output.dense.bias: 768
hr_bert.encoder.layer.7.output.LayerNorm.weight: 768
hr_bert.encoder.layer.7.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.attention.self.query.weight: 589824
hr_bert.encoder.layer.8.attention.self.query.bias: 768
hr_bert.encoder.layer.8.attention.self.key.weight: 589824
hr_bert.encoder.layer.8.attention.self.key.bias: 768
hr_bert.encoder.layer.8.attention.self.value.weight: 589824
hr_bert.encoder.layer.8.attention.self.value.bias: 768
hr_bert.encoder.layer.8.attention.output.dense.weight: 589824
hr_bert.encoder.layer.8.attention.output.dense.bias: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.8.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.8.intermediate.dense.bias: 3072
hr_bert.encoder.layer.8.output.dense.weight: 2359296
hr_bert.encoder.layer.8.output.dense.bias: 768
hr_bert.encoder.layer.8.output.LayerNorm.weight: 768
hr_bert.encoder.layer.8.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.attention.self.query.weight: 589824
hr_bert.encoder.layer.9.attention.self.query.bias: 768
hr_bert.encoder.layer.9.attention.self.key.weight: 589824
hr_bert.encoder.layer.9.attention.self.key.bias: 768
hr_bert.encoder.layer.9.attention.self.value.weight: 589824
hr_bert.encoder.layer.9.attention.self.value.bias: 768
hr_bert.encoder.layer.9.attention.output.dense.weight: 589824
hr_bert.encoder.layer.9.attention.output.dense.bias: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.9.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.9.intermediate.dense.bias: 3072
hr_bert.encoder.layer.9.output.dense.weight: 2359296
hr_bert.encoder.layer.9.output.dense.bias: 768
hr_bert.encoder.layer.9.output.LayerNorm.weight: 768
hr_bert.encoder.layer.9.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.attention.self.query.weight: 589824
hr_bert.encoder.layer.10.attention.self.query.bias: 768
hr_bert.encoder.layer.10.attention.self.key.weight: 589824
hr_bert.encoder.layer.10.attention.self.key.bias: 768
hr_bert.encoder.layer.10.attention.self.value.weight: 589824
hr_bert.encoder.layer.10.attention.self.value.bias: 768
hr_bert.encoder.layer.10.attention.output.dense.weight: 589824
hr_bert.encoder.layer.10.attention.output.dense.bias: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.10.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.10.intermediate.dense.bias: 3072
hr_bert.encoder.layer.10.output.dense.weight: 2359296
hr_bert.encoder.layer.10.output.dense.bias: 768
hr_bert.encoder.layer.10.output.LayerNorm.weight: 768
hr_bert.encoder.layer.10.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.attention.self.query.weight: 589824
hr_bert.encoder.layer.11.attention.self.query.bias: 768
hr_bert.encoder.layer.11.attention.self.key.weight: 589824
hr_bert.encoder.layer.11.attention.self.key.bias: 768
hr_bert.encoder.layer.11.attention.self.value.weight: 589824
hr_bert.encoder.layer.11.attention.self.value.bias: 768
hr_bert.encoder.layer.11.attention.output.dense.weight: 589824
hr_bert.encoder.layer.11.attention.output.dense.bias: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
hr_bert.encoder.layer.11.intermediate.dense.weight: 2359296
hr_bert.encoder.layer.11.intermediate.dense.bias: 3072
hr_bert.encoder.layer.11.output.dense.weight: 2359296
hr_bert.encoder.layer.11.output.dense.bias: 768
hr_bert.encoder.layer.11.output.LayerNorm.weight: 768
hr_bert.encoder.layer.11.output.LayerNorm.bias: 768
hr_bert.pooler.dense.weight: 589824
hr_bert.pooler.dense.bias: 768
tail_bert.embeddings.word_embeddings.weight: 91812096
tail_bert.embeddings.position_embeddings.weight: 393216
tail_bert.embeddings.token_type_embeddings.weight: 1536
tail_bert.embeddings.LayerNorm.weight: 768
tail_bert.embeddings.LayerNorm.bias: 768
tail_bert.encoder.layer.0.attention.self.query.weight: 589824
tail_bert.encoder.layer.0.attention.self.query.bias: 768
tail_bert.encoder.layer.0.attention.self.key.weight: 589824
tail_bert.encoder.layer.0.attention.self.key.bias: 768
tail_bert.encoder.layer.0.attention.self.value.weight: 589824
tail_bert.encoder.layer.0.attention.self.value.bias: 768
tail_bert.encoder.layer.0.attention.output.dense.weight: 589824
tail_bert.encoder.layer.0.attention.output.dense.bias: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.0.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.0.intermediate.dense.bias: 3072
tail_bert.encoder.layer.0.output.dense.weight: 2359296
tail_bert.encoder.layer.0.output.dense.bias: 768
tail_bert.encoder.layer.0.output.LayerNorm.weight: 768
tail_bert.encoder.layer.0.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.attention.self.query.weight: 589824
tail_bert.encoder.layer.1.attention.self.query.bias: 768
tail_bert.encoder.layer.1.attention.self.key.weight: 589824
tail_bert.encoder.layer.1.attention.self.key.bias: 768
tail_bert.encoder.layer.1.attention.self.value.weight: 589824
tail_bert.encoder.layer.1.attention.self.value.bias: 768
tail_bert.encoder.layer.1.attention.output.dense.weight: 589824
tail_bert.encoder.layer.1.attention.output.dense.bias: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.1.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.1.intermediate.dense.bias: 3072
tail_bert.encoder.layer.1.output.dense.weight: 2359296
tail_bert.encoder.layer.1.output.dense.bias: 768
tail_bert.encoder.layer.1.output.LayerNorm.weight: 768
tail_bert.encoder.layer.1.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.attention.self.query.weight: 589824
tail_bert.encoder.layer.2.attention.self.query.bias: 768
tail_bert.encoder.layer.2.attention.self.key.weight: 589824
tail_bert.encoder.layer.2.attention.self.key.bias: 768
tail_bert.encoder.layer.2.attention.self.value.weight: 589824
tail_bert.encoder.layer.2.attention.self.value.bias: 768
tail_bert.encoder.layer.2.attention.output.dense.weight: 589824
tail_bert.encoder.layer.2.attention.output.dense.bias: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.2.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.2.intermediate.dense.bias: 3072
tail_bert.encoder.layer.2.output.dense.weight: 2359296
tail_bert.encoder.layer.2.output.dense.bias: 768
tail_bert.encoder.layer.2.output.LayerNorm.weight: 768
tail_bert.encoder.layer.2.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.attention.self.query.weight: 589824
tail_bert.encoder.layer.3.attention.self.query.bias: 768
tail_bert.encoder.layer.3.attention.self.key.weight: 589824
tail_bert.encoder.layer.3.attention.self.key.bias: 768
tail_bert.encoder.layer.3.attention.self.value.weight: 589824
tail_bert.encoder.layer.3.attention.self.value.bias: 768
tail_bert.encoder.layer.3.attention.output.dense.weight: 589824
tail_bert.encoder.layer.3.attention.output.dense.bias: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.3.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.3.intermediate.dense.bias: 3072
tail_bert.encoder.layer.3.output.dense.weight: 2359296
tail_bert.encoder.layer.3.output.dense.bias: 768
tail_bert.encoder.layer.3.output.LayerNorm.weight: 768
tail_bert.encoder.layer.3.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.attention.self.query.weight: 589824
tail_bert.encoder.layer.4.attention.self.query.bias: 768
tail_bert.encoder.layer.4.attention.self.key.weight: 589824
tail_bert.encoder.layer.4.attention.self.key.bias: 768
tail_bert.encoder.layer.4.attention.self.value.weight: 589824
tail_bert.encoder.layer.4.attention.self.value.bias: 768
tail_bert.encoder.layer.4.attention.output.dense.weight: 589824
tail_bert.encoder.layer.4.attention.output.dense.bias: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.4.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.4.intermediate.dense.bias: 3072
tail_bert.encoder.layer.4.output.dense.weight: 2359296
tail_bert.encoder.layer.4.output.dense.bias: 768
tail_bert.encoder.layer.4.output.LayerNorm.weight: 768
tail_bert.encoder.layer.4.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.attention.self.query.weight: 589824
tail_bert.encoder.layer.5.attention.self.query.bias: 768
tail_bert.encoder.layer.5.attention.self.key.weight: 589824
tail_bert.encoder.layer.5.attention.self.key.bias: 768
tail_bert.encoder.layer.5.attention.self.value.weight: 589824
tail_bert.encoder.layer.5.attention.self.value.bias: 768
tail_bert.encoder.layer.5.attention.output.dense.weight: 589824
tail_bert.encoder.layer.5.attention.output.dense.bias: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.5.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.5.intermediate.dense.bias: 3072
tail_bert.encoder.layer.5.output.dense.weight: 2359296
tail_bert.encoder.layer.5.output.dense.bias: 768
tail_bert.encoder.layer.5.output.LayerNorm.weight: 768
tail_bert.encoder.layer.5.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.attention.self.query.weight: 589824
tail_bert.encoder.layer.6.attention.self.query.bias: 768
tail_bert.encoder.layer.6.attention.self.key.weight: 589824
tail_bert.encoder.layer.6.attention.self.key.bias: 768
tail_bert.encoder.layer.6.attention.self.value.weight: 589824
tail_bert.encoder.layer.6.attention.self.value.bias: 768
tail_bert.encoder.layer.6.attention.output.dense.weight: 589824
tail_bert.encoder.layer.6.attention.output.dense.bias: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.6.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.6.intermediate.dense.bias: 3072
tail_bert.encoder.layer.6.output.dense.weight: 2359296
tail_bert.encoder.layer.6.output.dense.bias: 768
tail_bert.encoder.layer.6.output.LayerNorm.weight: 768
tail_bert.encoder.layer.6.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.attention.self.query.weight: 589824
tail_bert.encoder.layer.7.attention.self.query.bias: 768
tail_bert.encoder.layer.7.attention.self.key.weight: 589824
tail_bert.encoder.layer.7.attention.self.key.bias: 768
tail_bert.encoder.layer.7.attention.self.value.weight: 589824
tail_bert.encoder.layer.7.attention.self.value.bias: 768
tail_bert.encoder.layer.7.attention.output.dense.weight: 589824
tail_bert.encoder.layer.7.attention.output.dense.bias: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.7.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.7.intermediate.dense.bias: 3072
tail_bert.encoder.layer.7.output.dense.weight: 2359296
tail_bert.encoder.layer.7.output.dense.bias: 768
tail_bert.encoder.layer.7.output.LayerNorm.weight: 768
tail_bert.encoder.layer.7.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.attention.self.query.weight: 589824
tail_bert.encoder.layer.8.attention.self.query.bias: 768
tail_bert.encoder.layer.8.attention.self.key.weight: 589824
tail_bert.encoder.layer.8.attention.self.key.bias: 768
tail_bert.encoder.layer.8.attention.self.value.weight: 589824
tail_bert.encoder.layer.8.attention.self.value.bias: 768
tail_bert.encoder.layer.8.attention.output.dense.weight: 589824
tail_bert.encoder.layer.8.attention.output.dense.bias: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.8.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.8.intermediate.dense.bias: 3072
tail_bert.encoder.layer.8.output.dense.weight: 2359296
tail_bert.encoder.layer.8.output.dense.bias: 768
tail_bert.encoder.layer.8.output.LayerNorm.weight: 768
tail_bert.encoder.layer.8.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.attention.self.query.weight: 589824
tail_bert.encoder.layer.9.attention.self.query.bias: 768
tail_bert.encoder.layer.9.attention.self.key.weight: 589824
tail_bert.encoder.layer.9.attention.self.key.bias: 768
tail_bert.encoder.layer.9.attention.self.value.weight: 589824
tail_bert.encoder.layer.9.attention.self.value.bias: 768
tail_bert.encoder.layer.9.attention.output.dense.weight: 589824
tail_bert.encoder.layer.9.attention.output.dense.bias: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.9.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.9.intermediate.dense.bias: 3072
tail_bert.encoder.layer.9.output.dense.weight: 2359296
tail_bert.encoder.layer.9.output.dense.bias: 768
tail_bert.encoder.layer.9.output.LayerNorm.weight: 768
tail_bert.encoder.layer.9.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.attention.self.query.weight: 589824
tail_bert.encoder.layer.10.attention.self.query.bias: 768
tail_bert.encoder.layer.10.attention.self.key.weight: 589824
tail_bert.encoder.layer.10.attention.self.key.bias: 768
tail_bert.encoder.layer.10.attention.self.value.weight: 589824
tail_bert.encoder.layer.10.attention.self.value.bias: 768
tail_bert.encoder.layer.10.attention.output.dense.weight: 589824
tail_bert.encoder.layer.10.attention.output.dense.bias: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.10.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.10.intermediate.dense.bias: 3072
tail_bert.encoder.layer.10.output.dense.weight: 2359296
tail_bert.encoder.layer.10.output.dense.bias: 768
tail_bert.encoder.layer.10.output.LayerNorm.weight: 768
tail_bert.encoder.layer.10.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.attention.self.query.weight: 589824
tail_bert.encoder.layer.11.attention.self.query.bias: 768
tail_bert.encoder.layer.11.attention.self.key.weight: 589824
tail_bert.encoder.layer.11.attention.self.key.bias: 768
tail_bert.encoder.layer.11.attention.self.value.weight: 589824
tail_bert.encoder.layer.11.attention.self.value.bias: 768
tail_bert.encoder.layer.11.attention.output.dense.weight: 589824
tail_bert.encoder.layer.11.attention.output.dense.bias: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.attention.output.LayerNorm.bias: 768
tail_bert.encoder.layer.11.intermediate.dense.weight: 2359296
tail_bert.encoder.layer.11.intermediate.dense.bias: 3072
tail_bert.encoder.layer.11.output.dense.weight: 2359296
tail_bert.encoder.layer.11.output.dense.bias: 768
tail_bert.encoder.layer.11.output.LayerNorm.weight: 768
tail_bert.encoder.layer.11.output.LayerNorm.bias: 768
tail_bert.pooler.dense.weight: 589824
tail_bert.pooler.dense.bias: 768
Number of parameters: 355.0M
In test mode: False
Load 52847 examples from ./data/union+trans_en2hi/train.txt.json
In test mode: False
Load 500 examples from ./data/union+trans_en2hi/valid.txt.json
Total training steps: 41286, warmup steps: 400
Args={
    "pretrained_model": "bert-base-multilingual-cased",
    "task": "mopenkb",
    "train_path": "./data/union+trans_en2hi/train.txt.json",
    "valid_path": "./data/union+trans_en2hi/valid.txt.json",
    "model_dir": "./checkpoint/union+trans_en2hi",
    "continue_training": null,
    "patience": 10,
    "warmup": 400,
    "max_to_keep": 0,
    "grad_clip": 10.0,
    "pooling": "mean",
    "dropout": 0.1,
    "use_amp": true,
    "t": 0.05,
    "use_link_graph": false,
    "eval_every_n_step": 10000,
    "pre_batch": 0,
    "pre_batch_weight": 0.5,
    "additive_margin": 0.02,
    "finetune_t": true,
    "max_num_tokens": 50,
    "use_self_negative": true,
    "workers": 3,
    "epochs": 100,
    "batch_size": 256,
    "lr": 3e-05,
    "lr_scheduler": "linear",
    "weight_decay": 0.0001,
    "print_freq": 20,
    "seed": 1234,
    "is_test": false,
    "rerank_n_hop": 2,
    "neighbor_weight": 0.0,
    "eval_model_path": ""
}
Epoch: [0][  0/412]	Loss 11.86 (11.86)	InvT  20.00 ( 20.00)	Acc@1   0.39 (  0.39)	Acc@3   3.12 (  3.12)
Epoch: [0][ 20/412]	Loss 11.5 (11.78)	InvT  20.00 ( 20.00)	Acc@1   0.78 (  0.87)	Acc@3   3.52 (  4.06)
Epoch: [0][ 40/412]	Loss 10.19 (11.2)	InvT  20.00 ( 20.00)	Acc@1   0.78 (  0.79)	Acc@3   5.08 (  5.35)
Epoch: [0][ 60/412]	Loss 9.053 (10.67)	InvT  20.00 ( 20.00)	Acc@1   2.73 (  1.01)	Acc@3  15.23 (  6.79)
Epoch: [0][ 80/412]	Loss 8.327 (10.19)	InvT  20.00 ( 20.00)	Acc@1   1.95 (  1.40)	Acc@3  16.41 (  8.56)
Epoch: [0][100/412]	Loss 7.844 (9.772)	InvT  19.99 ( 20.00)	Acc@1  10.16 (  2.40)	Acc@3  23.44 ( 10.75)
Epoch: [0][120/412]	Loss 7.349 (9.418)	InvT  19.99 ( 20.00)	Acc@1   9.77 (  3.44)	Acc@3  26.95 ( 13.03)
Epoch: [0][140/412]	Loss 6.546 (9.102)	InvT  19.99 ( 20.00)	Acc@1  15.23 (  4.69)	Acc@3  37.89 ( 15.43)
Epoch: [0][160/412]	Loss 6.778 (8.82)	InvT  19.99 ( 20.00)	Acc@1  16.41 (  5.97)	Acc@3  32.03 ( 17.77)
Epoch: [0][180/412]	Loss 6.747 (8.576)	InvT  19.99 ( 19.99)	Acc@1  14.84 (  7.09)	Acc@3  34.38 ( 19.83)
Epoch: [0][200/412]	Loss 5.973 (8.349)	InvT  19.98 ( 19.99)	Acc@1  23.83 (  8.36)	Acc@3  43.36 ( 21.89)
Epoch: [0][220/412]	Loss 6.173 (8.152)	InvT  19.98 ( 19.99)	Acc@1  20.70 (  9.52)	Acc@3  42.58 ( 23.73)
Epoch: [0][240/412]	Loss 5.841 (7.973)	InvT  19.98 ( 19.99)	Acc@1  27.73 ( 10.66)	Acc@3  46.48 ( 25.40)
Epoch: [0][260/412]	Loss 5.927 (7.805)	InvT  19.97 ( 19.99)	Acc@1  26.17 ( 11.77)	Acc@3  45.31 ( 27.07)
Epoch: [0][280/412]	Loss 5.515 (7.655)	InvT  19.97 ( 19.99)	Acc@1  25.00 ( 12.86)	Acc@3  48.44 ( 28.56)
Epoch: [0][300/412]	Loss 5.371 (7.511)	InvT  19.96 ( 19.99)	Acc@1  27.73 ( 13.88)	Acc@3  54.69 ( 30.05)
Epoch: [0][320/412]	Loss 5.352 (7.381)	InvT  19.96 ( 19.99)	Acc@1  29.69 ( 14.85)	Acc@3  51.17 ( 31.34)
Epoch: [0][340/412]	Loss 5.492 (7.261)	InvT  19.95 ( 19.98)	Acc@1  28.12 ( 15.71)	Acc@3  53.91 ( 32.51)
Epoch: [0][360/412]	Loss 5.156 (7.148)	InvT  19.95 ( 19.98)	Acc@1  31.25 ( 16.59)	Acc@3  52.73 ( 33.70)
Epoch: [0][380/412]	Loss 5.179 (7.043)	InvT  19.94 ( 19.98)	Acc@1  27.34 ( 17.36)	Acc@3  59.38 ( 34.83)
Epoch: [0][400/412]	Loss 5.391 (6.945)	InvT  19.94 ( 19.98)	Acc@1  26.95 ( 18.12)	Acc@3  54.30 ( 35.82)
Learning rate: 2.9991195030083647e-05
Epoch 0, valid metric: {"Acc@1": 19.6, "Acc@3": 32.4, "loss": 3.972}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch0.mdl
Epoch: [1][  0/412]	Loss 4.627 (4.627)	InvT  19.93 ( 19.93)	Acc@1  36.33 ( 36.33)	Acc@3  57.81 ( 57.81)
Epoch: [1][ 20/412]	Loss 4.58 (4.609)	InvT  19.93 ( 19.93)	Acc@1  39.06 ( 38.21)	Acc@3  57.81 ( 60.79)
Epoch: [1][ 40/412]	Loss 4.663 (4.581)	InvT  19.92 ( 19.93)	Acc@1  40.62 ( 38.54)	Acc@3  62.89 ( 61.47)
Epoch: [1][ 60/412]	Loss 4.431 (4.577)	InvT  19.92 ( 19.92)	Acc@1  43.36 ( 38.52)	Acc@3  63.28 ( 61.23)
Epoch: [1][ 80/412]	Loss 4.311 (4.57)	InvT  19.91 ( 19.92)	Acc@1  44.53 ( 38.75)	Acc@3  64.45 ( 61.39)
Epoch: [1][100/412]	Loss 4.465 (4.536)	InvT  19.91 ( 19.92)	Acc@1  41.02 ( 39.08)	Acc@3  60.94 ( 61.69)
Epoch: [1][120/412]	Loss 4.388 (4.511)	InvT  19.90 ( 19.92)	Acc@1  42.19 ( 39.36)	Acc@3  64.84 ( 61.97)
Epoch: [1][140/412]	Loss 4.464 (4.497)	InvT  19.89 ( 19.91)	Acc@1  39.06 ( 39.46)	Acc@3  62.11 ( 62.02)
Epoch: [1][160/412]	Loss 3.983 (4.474)	InvT  19.89 ( 19.91)	Acc@1  44.53 ( 39.75)	Acc@3  67.58 ( 62.23)
Epoch: [1][180/412]	Loss 4.031 (4.453)	InvT  19.88 ( 19.91)	Acc@1  43.36 ( 40.07)	Acc@3  65.62 ( 62.49)
Epoch: [1][200/412]	Loss 4.034 (4.434)	InvT  19.88 ( 19.91)	Acc@1  44.53 ( 40.30)	Acc@3  67.58 ( 62.72)
Epoch: [1][220/412]	Loss 4.058 (4.416)	InvT  19.87 ( 19.90)	Acc@1  39.45 ( 40.45)	Acc@3  65.23 ( 62.87)
Epoch: [1][240/412]	Loss 4.06 (4.398)	InvT  19.87 ( 19.90)	Acc@1  44.53 ( 40.66)	Acc@3  67.58 ( 63.08)
Epoch: [1][260/412]	Loss 3.993 (4.383)	InvT  19.86 ( 19.90)	Acc@1  46.88 ( 40.84)	Acc@3  67.58 ( 63.26)
Epoch: [1][280/412]	Loss 4.479 (4.374)	InvT  19.85 ( 19.89)	Acc@1  39.45 ( 40.91)	Acc@3  64.06 ( 63.32)
Epoch: [1][300/412]	Loss 4.202 (4.359)	InvT  19.85 ( 19.89)	Acc@1  44.53 ( 41.03)	Acc@3  63.28 ( 63.43)
Epoch: [1][320/412]	Loss 3.847 (4.346)	InvT  19.84 ( 19.89)	Acc@1  44.53 ( 41.16)	Acc@3  66.41 ( 63.54)
Epoch: [1][340/412]	Loss 3.95 (4.332)	InvT  19.83 ( 19.89)	Acc@1  48.05 ( 41.32)	Acc@3  65.23 ( 63.67)
Epoch: [1][360/412]	Loss 4.072 (4.319)	InvT  19.83 ( 19.88)	Acc@1  43.75 ( 41.41)	Acc@3  68.36 ( 63.85)
Epoch: [1][380/412]	Loss 3.855 (4.304)	InvT  19.82 ( 19.88)	Acc@1  42.97 ( 41.57)	Acc@3  64.45 ( 63.96)
Epoch: [1][400/412]	Loss 3.821 (4.289)	InvT  19.82 ( 19.88)	Acc@1  47.66 ( 41.75)	Acc@3  67.97 ( 64.12)
Learning rate: 2.9688891062955536e-05
Epoch 1, valid metric: {"Acc@1": 24.2, "Acc@3": 40.0, "loss": 3.595}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch1.mdl
Epoch: [2][  0/412]	Loss 3.535 (3.535)	InvT  19.81 ( 19.81)	Acc@1  52.34 ( 52.34)	Acc@3  68.36 ( 68.36)
Epoch: [2][ 20/412]	Loss 3.01 (3.352)	InvT  19.81 ( 19.81)	Acc@1  55.86 ( 52.86)	Acc@3  76.56 ( 74.83)
Epoch: [2][ 40/412]	Loss 3.527 (3.323)	InvT  19.82 ( 19.81)	Acc@1  48.83 ( 52.82)	Acc@3  72.66 ( 74.88)
Epoch: [2][ 60/412]	Loss 3.665 (3.304)	InvT  19.82 ( 19.82)	Acc@1  46.88 ( 53.14)	Acc@3  69.92 ( 74.99)
Epoch: [2][ 80/412]	Loss 3.219 (3.316)	InvT  19.82 ( 19.82)	Acc@1  54.30 ( 52.90)	Acc@3  76.17 ( 74.86)
Epoch: [2][100/412]	Loss 3.566 (3.306)	InvT  19.82 ( 19.82)	Acc@1  51.95 ( 53.15)	Acc@3  69.92 ( 74.86)
Epoch: [2][120/412]	Loss 3.458 (3.315)	InvT  19.82 ( 19.82)	Acc@1  53.52 ( 53.09)	Acc@3  75.39 ( 74.80)
Epoch: [2][140/412]	Loss 3.226 (3.315)	InvT  19.82 ( 19.82)	Acc@1  53.91 ( 53.11)	Acc@3  77.34 ( 74.75)
Epoch: [2][160/412]	Loss 3.447 (3.31)	InvT  19.82 ( 19.82)	Acc@1  55.47 ( 53.29)	Acc@3  72.66 ( 74.75)
Epoch: [2][180/412]	Loss 2.617 (3.305)	InvT  19.82 ( 19.82)	Acc@1  60.55 ( 53.38)	Acc@3  84.38 ( 74.81)
Epoch: [2][200/412]	Loss 3.532 (3.308)	InvT  19.82 ( 19.82)	Acc@1  52.73 ( 53.40)	Acc@3  73.83 ( 74.79)
Epoch: [2][220/412]	Loss 2.818 (3.295)	InvT  19.82 ( 19.82)	Acc@1  62.50 ( 53.52)	Acc@3  78.52 ( 74.90)
Epoch: [2][240/412]	Loss 3.15 (3.288)	InvT  19.82 ( 19.82)	Acc@1  55.47 ( 53.54)	Acc@3  76.56 ( 74.94)
Epoch: [2][260/412]	Loss 3.143 (3.291)	InvT  19.82 ( 19.82)	Acc@1  55.08 ( 53.54)	Acc@3  76.17 ( 74.93)
Epoch: [2][280/412]	Loss 3.09 (3.29)	InvT  19.82 ( 19.82)	Acc@1  58.98 ( 53.52)	Acc@3  77.73 ( 74.93)
Epoch: [2][300/412]	Loss 3.179 (3.284)	InvT  19.82 ( 19.82)	Acc@1  53.52 ( 53.57)	Acc@3  76.95 ( 75.02)
Epoch: [2][320/412]	Loss 3.168 (3.279)	InvT  19.82 ( 19.82)	Acc@1  55.08 ( 53.65)	Acc@3  76.95 ( 75.03)
Epoch: [2][340/412]	Loss 2.908 (3.279)	InvT  19.82 ( 19.82)	Acc@1  57.42 ( 53.72)	Acc@3  76.95 ( 75.06)
Epoch: [2][360/412]	Loss 3.116 (3.275)	InvT  19.82 ( 19.82)	Acc@1  57.03 ( 53.75)	Acc@3  76.95 ( 75.09)
Epoch: [2][380/412]	Loss 3.472 (3.274)	InvT  19.82 ( 19.82)	Acc@1  52.34 ( 53.76)	Acc@3  73.05 ( 75.11)
Epoch: [2][400/412]	Loss 3.01 (3.275)	InvT  19.82 ( 19.82)	Acc@1  57.03 ( 53.76)	Acc@3  78.52 ( 75.14)
Learning rate: 2.938658709582742e-05
Epoch 2, valid metric: {"Acc@1": 26.9, "Acc@3": 43.6, "loss": 3.441}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch2.mdl
Epoch: [3][  0/412]	Loss 2.713 (2.713)	InvT  19.82 ( 19.82)	Acc@1  60.55 ( 60.55)	Acc@3  82.42 ( 82.42)
Epoch: [3][ 20/412]	Loss 2.202 (2.583)	InvT  19.83 ( 19.82)	Acc@1  66.80 ( 61.87)	Acc@3  89.06 ( 82.59)
Epoch: [3][ 40/412]	Loss 2.424 (2.513)	InvT  19.84 ( 19.83)	Acc@1  65.62 ( 62.54)	Acc@3  84.38 ( 83.39)
Epoch: [3][ 60/412]	Loss 2.128 (2.53)	InvT  19.85 ( 19.83)	Acc@1  66.02 ( 62.24)	Acc@3  88.28 ( 83.07)
Epoch: [3][ 80/412]	Loss 2.648 (2.545)	InvT  19.86 ( 19.84)	Acc@1  57.03 ( 62.36)	Acc@3  83.59 ( 82.96)
Epoch: [3][100/412]	Loss 2.645 (2.559)	InvT  19.87 ( 19.84)	Acc@1  58.98 ( 62.30)	Acc@3  82.42 ( 82.87)
Epoch: [3][120/412]	Loss 2.325 (2.565)	InvT  19.87 ( 19.85)	Acc@1  67.97 ( 62.35)	Acc@3  82.81 ( 82.84)
Epoch: [3][140/412]	Loss 2.415 (2.564)	InvT  19.88 ( 19.85)	Acc@1  65.62 ( 62.31)	Acc@3  82.42 ( 82.88)
Epoch: [3][160/412]	Loss 2.461 (2.568)	InvT  19.89 ( 19.86)	Acc@1  65.23 ( 62.34)	Acc@3  83.20 ( 82.82)
Epoch: [3][180/412]	Loss 2.553 (2.565)	InvT  19.90 ( 19.86)	Acc@1  60.55 ( 62.41)	Acc@3  83.59 ( 82.83)
Epoch: [3][200/412]	Loss 2.608 (2.57)	InvT  19.91 ( 19.86)	Acc@1  60.55 ( 62.32)	Acc@3  82.81 ( 82.78)
Epoch: [3][220/412]	Loss 2.42 (2.575)	InvT  19.91 ( 19.87)	Acc@1  65.23 ( 62.18)	Acc@3  85.16 ( 82.72)
Epoch: [3][240/412]	Loss 2.525 (2.579)	InvT  19.92 ( 19.87)	Acc@1  62.50 ( 62.10)	Acc@3  82.42 ( 82.65)
Epoch: [3][260/412]	Loss 2.355 (2.585)	InvT  19.93 ( 19.88)	Acc@1  64.06 ( 62.01)	Acc@3  85.94 ( 82.55)
Epoch: [3][280/412]	Loss 2.468 (2.585)	InvT  19.93 ( 19.88)	Acc@1  62.89 ( 61.98)	Acc@3  85.16 ( 82.52)
Epoch: [3][300/412]	Loss 2.536 (2.588)	InvT  19.94 ( 19.88)	Acc@1  62.11 ( 61.96)	Acc@3  84.77 ( 82.48)
Epoch: [3][320/412]	Loss 2.298 (2.589)	InvT  19.95 ( 19.89)	Acc@1  66.02 ( 61.95)	Acc@3  84.38 ( 82.48)
Epoch: [3][340/412]	Loss 2.564 (2.591)	InvT  19.95 ( 19.89)	Acc@1  60.16 ( 61.92)	Acc@3  80.08 ( 82.45)
Epoch: [3][360/412]	Loss 2.347 (2.596)	InvT  19.96 ( 19.89)	Acc@1  62.50 ( 61.87)	Acc@3  83.98 ( 82.39)
Epoch: [3][380/412]	Loss 2.663 (2.6)	InvT  19.96 ( 19.90)	Acc@1  60.55 ( 61.84)	Acc@3  80.86 ( 82.38)
Epoch: [3][400/412]	Loss 2.814 (2.602)	InvT  19.97 ( 19.90)	Acc@1  58.98 ( 61.80)	Acc@3  79.69 ( 82.33)
Learning rate: 2.908428312869931e-05
Epoch 3, valid metric: {"Acc@1": 29.9, "Acc@3": 45.0, "loss": 3.37}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch3.mdl
Epoch: [4][  0/412]	Loss 1.931 (1.931)	InvT  19.97 ( 19.97)	Acc@1  70.31 ( 70.31)	Acc@3  88.67 ( 88.67)
Epoch: [4][ 20/412]	Loss 2.012 (2.029)	InvT  19.99 ( 19.98)	Acc@1  66.80 ( 69.27)	Acc@3  88.67 ( 87.85)
Epoch: [4][ 40/412]	Loss 2.171 (2.029)	InvT  20.00 ( 19.99)	Acc@1  68.75 ( 69.34)	Acc@3  86.72 ( 88.12)
Epoch: [4][ 60/412]	Loss 1.967 (2.027)	InvT  20.02 ( 20.00)	Acc@1  68.75 ( 69.28)	Acc@3  88.28 ( 88.22)
Epoch: [4][ 80/412]	Loss 2.266 (2.024)	InvT  20.04 ( 20.00)	Acc@1  65.62 ( 69.26)	Acc@3  87.50 ( 88.25)
Epoch: [4][100/412]	Loss 1.817 (2.027)	InvT  20.05 ( 20.01)	Acc@1  75.39 ( 69.31)	Acc@3  89.06 ( 88.10)
Epoch: [4][120/412]	Loss 2.281 (2.04)	InvT  20.06 ( 20.02)	Acc@1  65.62 ( 69.14)	Acc@3  83.98 ( 87.86)
Epoch: [4][140/412]	Loss 2.104 (2.049)	InvT  20.07 ( 20.03)	Acc@1  66.02 ( 69.01)	Acc@3  87.89 ( 87.81)
Epoch: [4][160/412]	Loss 1.844 (2.056)	InvT  20.09 ( 20.03)	Acc@1  73.05 ( 68.89)	Acc@3  88.67 ( 87.81)
Epoch: [4][180/412]	Loss 2.207 (2.064)	InvT  20.10 ( 20.04)	Acc@1  68.75 ( 68.80)	Acc@3  85.16 ( 87.75)
Epoch: [4][200/412]	Loss 2.369 (2.068)	InvT  20.11 ( 20.05)	Acc@1  65.23 ( 68.71)	Acc@3  84.38 ( 87.68)
Epoch: [4][220/412]	Loss 1.859 (2.072)	InvT  20.12 ( 20.05)	Acc@1  71.88 ( 68.71)	Acc@3  90.23 ( 87.69)
Epoch: [4][240/412]	Loss 2.302 (2.076)	InvT  20.13 ( 20.06)	Acc@1  68.75 ( 68.71)	Acc@3  86.33 ( 87.64)
Epoch: [4][260/412]	Loss 2.1 (2.075)	InvT  20.14 ( 20.07)	Acc@1  68.36 ( 68.76)	Acc@3  83.98 ( 87.59)
Epoch: [4][280/412]	Loss 2.113 (2.073)	InvT  20.16 ( 20.07)	Acc@1  69.92 ( 68.75)	Acc@3  87.11 ( 87.57)
Epoch: [4][300/412]	Loss 1.882 (2.076)	InvT  20.17 ( 20.08)	Acc@1  69.92 ( 68.72)	Acc@3  89.84 ( 87.58)
Epoch: [4][320/412]	Loss 2.122 (2.08)	InvT  20.18 ( 20.08)	Acc@1  69.53 ( 68.68)	Acc@3  87.11 ( 87.53)
Epoch: [4][340/412]	Loss 2.162 (2.085)	InvT  20.19 ( 20.09)	Acc@1  69.53 ( 68.63)	Acc@3  85.94 ( 87.46)
Epoch: [4][360/412]	Loss 1.991 (2.086)	InvT  20.20 ( 20.10)	Acc@1  72.66 ( 68.62)	Acc@3  87.89 ( 87.45)
Epoch: [4][380/412]	Loss 2.415 (2.089)	InvT  20.21 ( 20.10)	Acc@1  62.89 ( 68.59)	Acc@3  83.59 ( 87.40)
Epoch: [4][400/412]	Loss 2.158 (2.092)	InvT  20.22 ( 20.11)	Acc@1  69.53 ( 68.56)	Acc@3  89.06 ( 87.35)
Learning rate: 2.87819791615712e-05
Epoch 4, valid metric: {"Acc@1": 30.5, "Acc@3": 46.4, "loss": 3.387}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch4.mdl
Epoch: [5][  0/412]	Loss 1.761 (1.761)	InvT  20.23 ( 20.23)	Acc@1  73.83 ( 73.83)	Acc@3  90.23 ( 90.23)
Epoch: [5][ 20/412]	Loss 1.544 (1.676)	InvT  20.24 ( 20.23)	Acc@1  75.78 ( 74.00)	Acc@3  91.41 ( 91.50)
Epoch: [5][ 40/412]	Loss 1.548 (1.651)	InvT  20.26 ( 20.24)	Acc@1  77.73 ( 74.67)	Acc@3  95.31 ( 91.64)
Epoch: [5][ 60/412]	Loss 1.597 (1.64)	InvT  20.28 ( 20.25)	Acc@1  75.39 ( 74.78)	Acc@3  91.41 ( 91.69)
Epoch: [5][ 80/412]	Loss 1.528 (1.641)	InvT  20.30 ( 20.26)	Acc@1  75.39 ( 74.72)	Acc@3  92.19 ( 91.69)
Epoch: [5][100/412]	Loss 1.822 (1.646)	InvT  20.31 ( 20.27)	Acc@1  73.44 ( 74.57)	Acc@3  88.28 ( 91.63)
Epoch: [5][120/412]	Loss 1.383 (1.656)	InvT  20.33 ( 20.28)	Acc@1  78.12 ( 74.54)	Acc@3  94.14 ( 91.51)
Epoch: [5][140/412]	Loss 1.47 (1.657)	InvT  20.34 ( 20.29)	Acc@1  77.73 ( 74.47)	Acc@3  92.97 ( 91.50)
Epoch: [5][160/412]	Loss 1.967 (1.657)	InvT  20.36 ( 20.29)	Acc@1  69.53 ( 74.45)	Acc@3  89.84 ( 91.49)
Epoch: [5][180/412]	Loss 1.598 (1.655)	InvT  20.37 ( 20.30)	Acc@1  76.56 ( 74.43)	Acc@3  91.80 ( 91.48)
Epoch: [5][200/412]	Loss 1.701 (1.66)	InvT  20.39 ( 20.31)	Acc@1  69.14 ( 74.37)	Acc@3  89.84 ( 91.39)
Epoch: [5][220/412]	Loss 1.831 (1.669)	InvT  20.40 ( 20.32)	Acc@1  70.70 ( 74.26)	Acc@3  87.89 ( 91.33)
Epoch: [5][240/412]	Loss 1.593 (1.671)	InvT  20.41 ( 20.33)	Acc@1  77.34 ( 74.21)	Acc@3  90.23 ( 91.28)
Epoch: [5][260/412]	Loss 1.484 (1.673)	InvT  20.43 ( 20.33)	Acc@1  76.17 ( 74.18)	Acc@3  93.75 ( 91.23)
Epoch: [5][280/412]	Loss 1.804 (1.676)	InvT  20.44 ( 20.34)	Acc@1  73.83 ( 74.16)	Acc@3  89.84 ( 91.22)
Epoch: [5][300/412]	Loss 1.791 (1.678)	InvT  20.45 ( 20.35)	Acc@1  75.00 ( 74.11)	Acc@3  87.50 ( 91.19)
Epoch: [5][320/412]	Loss 1.773 (1.678)	InvT  20.46 ( 20.35)	Acc@1  73.44 ( 74.11)	Acc@3  91.41 ( 91.19)
Epoch: [5][340/412]	Loss 1.554 (1.677)	InvT  20.48 ( 20.36)	Acc@1  78.12 ( 74.07)	Acc@3  92.97 ( 91.19)
Epoch: [5][360/412]	Loss 1.676 (1.678)	InvT  20.49 ( 20.37)	Acc@1  78.12 ( 74.08)	Acc@3  88.28 ( 91.20)
Epoch: [5][380/412]	Loss 1.599 (1.681)	InvT  20.50 ( 20.37)	Acc@1  75.78 ( 74.04)	Acc@3  91.41 ( 91.19)
Epoch: [5][400/412]	Loss 2.166 (1.686)	InvT  20.51 ( 20.38)	Acc@1  67.58 ( 73.98)	Acc@3  87.50 ( 91.14)
Learning rate: 2.8479675194443087e-05
Epoch 5, valid metric: {"Acc@1": 32.2, "Acc@3": 48.8, "loss": 3.32}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch5.mdl
Epoch: [6][  0/412]	Loss 1.295 (1.295)	InvT  20.52 ( 20.52)	Acc@1  77.34 ( 77.34)	Acc@3  96.48 ( 96.48)
Epoch: [6][ 20/412]	Loss 1.283 (1.24)	InvT  20.53 ( 20.52)	Acc@1  81.25 ( 79.43)	Acc@3  92.97 ( 95.41)
Epoch: [6][ 40/412]	Loss 1.184 (1.256)	InvT  20.55 ( 20.53)	Acc@1  80.08 ( 79.59)	Acc@3  94.14 ( 95.06)
Epoch: [6][ 60/412]	Loss 1.234 (1.255)	InvT  20.57 ( 20.54)	Acc@1  78.91 ( 80.04)	Acc@3  96.09 ( 95.06)
Epoch: [6][ 80/412]	Loss 1.468 (1.268)	InvT  20.59 ( 20.55)	Acc@1  78.12 ( 80.08)	Acc@3  94.92 ( 94.99)
Epoch: [6][100/412]	Loss 1.114 (1.269)	InvT  20.61 ( 20.56)	Acc@1  85.16 ( 80.24)	Acc@3  95.70 ( 95.05)
Epoch: [6][120/412]	Loss 1.129 (1.281)	InvT  20.62 ( 20.57)	Acc@1  83.98 ( 80.05)	Acc@3  96.48 ( 94.89)
Epoch: [6][140/412]	Loss 1.344 (1.292)	InvT  20.64 ( 20.58)	Acc@1  80.08 ( 79.81)	Acc@3  92.97 ( 94.72)
Epoch: [6][160/412]	Loss 1.527 (1.297)	InvT  20.65 ( 20.59)	Acc@1  75.78 ( 79.78)	Acc@3  92.97 ( 94.61)
Epoch: [6][180/412]	Loss 1.54 (1.309)	InvT  20.67 ( 20.59)	Acc@1  78.12 ( 79.60)	Acc@3  92.97 ( 94.52)
Epoch: [6][200/412]	Loss 1.404 (1.315)	InvT  20.68 ( 20.60)	Acc@1  75.78 ( 79.49)	Acc@3  93.75 ( 94.44)
Epoch: [6][220/412]	Loss 1.352 (1.319)	InvT  20.69 ( 20.61)	Acc@1  80.08 ( 79.45)	Acc@3  94.14 ( 94.41)
Epoch: [6][240/412]	Loss 1.508 (1.324)	InvT  20.71 ( 20.62)	Acc@1  77.73 ( 79.40)	Acc@3  92.19 ( 94.32)
Epoch: [6][260/412]	Loss 1.276 (1.331)	InvT  20.72 ( 20.62)	Acc@1  81.64 ( 79.29)	Acc@3  93.75 ( 94.26)
Epoch: [6][280/412]	Loss 1.44 (1.331)	InvT  20.73 ( 20.63)	Acc@1  80.47 ( 79.28)	Acc@3  92.97 ( 94.25)
Epoch: [6][300/412]	Loss 1.167 (1.334)	InvT  20.75 ( 20.64)	Acc@1  83.98 ( 79.22)	Acc@3  95.31 ( 94.22)
Epoch: [6][320/412]	Loss 1.204 (1.34)	InvT  20.76 ( 20.65)	Acc@1  81.64 ( 79.13)	Acc@3  94.53 ( 94.16)
Epoch: [6][340/412]	Loss 1.323 (1.346)	InvT  20.77 ( 20.65)	Acc@1  78.91 ( 79.04)	Acc@3  95.31 ( 94.12)
Epoch: [6][360/412]	Loss 1.562 (1.349)	InvT  20.78 ( 20.66)	Acc@1  73.05 ( 79.00)	Acc@3  92.97 ( 94.09)
Epoch: [6][380/412]	Loss 1.186 (1.35)	InvT  20.80 ( 20.67)	Acc@1  81.25 ( 78.96)	Acc@3  94.53 ( 94.07)
Epoch: [6][400/412]	Loss 1.578 (1.356)	InvT  20.81 ( 20.67)	Acc@1  76.56 ( 78.86)	Acc@3  92.58 ( 94.04)
Learning rate: 2.8177371227314972e-05
Epoch 6, valid metric: {"Acc@1": 32.4, "Acc@3": 49.2, "loss": 3.34}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch6.mdl
Epoch: [7][  0/412]	Loss 1.032 (1.032)	InvT  20.82 ( 20.82)	Acc@1  84.38 ( 84.38)	Acc@3  96.09 ( 96.09)
Epoch: [7][ 20/412]	Loss 0.9153 (1.026)	InvT  20.83 ( 20.82)	Acc@1  83.59 ( 83.39)	Acc@3  98.44 ( 96.80)
Epoch: [7][ 40/412]	Loss 1.064 (1.021)	InvT  20.85 ( 20.83)	Acc@1  81.25 ( 83.50)	Acc@3  95.31 ( 96.72)
Epoch: [7][ 60/412]	Loss 1.046 (1.035)	InvT  20.86 ( 20.84)	Acc@1  85.55 ( 83.46)	Acc@3  98.44 ( 96.61)
Epoch: [7][ 80/412]	Loss 0.923 (1.034)	InvT  20.88 ( 20.85)	Acc@1  83.98 ( 83.53)	Acc@3  97.66 ( 96.59)
Epoch: [7][100/412]	Loss 1.134 (1.039)	InvT  20.90 ( 20.86)	Acc@1  82.81 ( 83.55)	Acc@3  94.53 ( 96.46)
Epoch: [7][120/412]	Loss 1.127 (1.049)	InvT  20.91 ( 20.86)	Acc@1  81.25 ( 83.35)	Acc@3  94.53 ( 96.38)
Epoch: [7][140/412]	Loss 1.23 (1.059)	InvT  20.92 ( 20.87)	Acc@1  81.25 ( 83.27)	Acc@3  94.53 ( 96.33)
Epoch: [7][160/412]	Loss 1.496 (1.065)	InvT  20.94 ( 20.88)	Acc@1  79.30 ( 83.17)	Acc@3  91.80 ( 96.24)
Epoch: [7][180/412]	Loss 1.159 (1.068)	InvT  20.95 ( 20.89)	Acc@1  81.64 ( 83.10)	Acc@3  97.27 ( 96.23)
Epoch: [7][200/412]	Loss 0.9415 (1.075)	InvT  20.97 ( 20.89)	Acc@1  85.16 ( 83.01)	Acc@3  97.66 ( 96.17)
Epoch: [7][220/412]	Loss 1.157 (1.078)	InvT  20.98 ( 20.90)	Acc@1  78.52 ( 82.96)	Acc@3  96.48 ( 96.14)
Epoch: [7][240/412]	Loss 1.056 (1.078)	InvT  21.00 ( 20.91)	Acc@1  84.77 ( 82.98)	Acc@3  96.48 ( 96.12)
Epoch: [7][260/412]	Loss 1.283 (1.08)	InvT  21.01 ( 20.92)	Acc@1  78.91 ( 82.91)	Acc@3  96.09 ( 96.13)
Epoch: [7][280/412]	Loss 1.2 (1.081)	InvT  21.02 ( 20.92)	Acc@1  79.69 ( 82.86)	Acc@3  95.31 ( 96.12)
Epoch: [7][300/412]	Loss 1.083 (1.086)	InvT  21.04 ( 20.93)	Acc@1  81.64 ( 82.74)	Acc@3  93.36 ( 96.08)
Epoch: [7][320/412]	Loss 1.279 (1.09)	InvT  21.05 ( 20.94)	Acc@1  79.30 ( 82.66)	Acc@3  94.53 ( 96.05)
Epoch: [7][340/412]	Loss 1.097 (1.095)	InvT  21.06 ( 20.94)	Acc@1  83.20 ( 82.58)	Acc@3  97.27 ( 96.02)
Epoch: [7][360/412]	Loss 1.087 (1.097)	InvT  21.07 ( 20.95)	Acc@1  83.98 ( 82.59)	Acc@3  96.88 ( 96.01)
Epoch: [7][380/412]	Loss 1.225 (1.101)	InvT  21.08 ( 20.96)	Acc@1  80.08 ( 82.48)	Acc@3  94.92 ( 95.99)
Epoch: [7][400/412]	Loss 1.32 (1.106)	InvT  21.09 ( 20.96)	Acc@1  79.30 ( 82.44)	Acc@3  94.92 ( 95.95)
Learning rate: 2.787506726018686e-05
Epoch 7, valid metric: {"Acc@1": 32.0, "Acc@3": 48.1, "loss": 3.432}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch7.mdl
Epoch: [8][  0/412]	Loss 0.8315 (0.8315)	InvT  21.10 ( 21.10)	Acc@1  85.94 ( 85.94)	Acc@3  98.05 ( 98.05)
Epoch: [8][ 20/412]	Loss 0.9132 (0.8302)	InvT  21.12 ( 21.11)	Acc@1  84.77 ( 86.63)	Acc@3  96.09 ( 97.90)
Epoch: [8][ 40/412]	Loss 0.8832 (0.8498)	InvT  21.13 ( 21.12)	Acc@1  86.72 ( 86.39)	Acc@3  98.05 ( 97.54)
Epoch: [8][ 60/412]	Loss 0.9377 (0.8576)	InvT  21.15 ( 21.13)	Acc@1  86.72 ( 86.34)	Acc@3  96.48 ( 97.61)
Epoch: [8][ 80/412]	Loss 1.016 (0.8642)	InvT  21.16 ( 21.13)	Acc@1  83.98 ( 86.28)	Acc@3  98.44 ( 97.64)
Epoch: [8][100/412]	Loss 0.8485 (0.8692)	InvT  21.18 ( 21.14)	Acc@1  83.20 ( 86.19)	Acc@3  99.22 ( 97.60)
Epoch: [8][120/412]	Loss 1.187 (0.8685)	InvT  21.19 ( 21.15)	Acc@1  83.98 ( 86.25)	Acc@3  97.66 ( 97.59)
Epoch: [8][140/412]	Loss 0.8108 (0.8734)	InvT  21.21 ( 21.16)	Acc@1  87.89 ( 86.17)	Acc@3  98.44 ( 97.54)
Epoch: [8][160/412]	Loss 1.002 (0.8737)	InvT  21.22 ( 21.16)	Acc@1  83.20 ( 86.16)	Acc@3  96.09 ( 97.56)
Epoch: [8][180/412]	Loss 0.9738 (0.8818)	InvT  21.23 ( 21.17)	Acc@1  84.38 ( 85.96)	Acc@3  96.88 ( 97.53)
Epoch: [8][200/412]	Loss 0.7306 (0.881)	InvT  21.25 ( 21.18)	Acc@1  87.11 ( 85.95)	Acc@3  98.05 ( 97.50)
Epoch: [8][220/412]	Loss 1.071 (0.8843)	InvT  21.26 ( 21.18)	Acc@1  83.20 ( 85.90)	Acc@3  95.70 ( 97.48)
Epoch: [8][240/412]	Loss 1.112 (0.8883)	InvT  21.28 ( 21.19)	Acc@1  80.86 ( 85.85)	Acc@3  97.27 ( 97.46)
Epoch: [8][260/412]	Loss 1.048 (0.8902)	InvT  21.29 ( 21.20)	Acc@1  83.59 ( 85.76)	Acc@3  95.31 ( 97.43)
Epoch: [8][280/412]	Loss 0.8567 (0.8888)	InvT  21.30 ( 21.21)	Acc@1  87.50 ( 85.81)	Acc@3  95.70 ( 97.43)
Epoch: [8][300/412]	Loss 0.944 (0.8921)	InvT  21.31 ( 21.21)	Acc@1  83.59 ( 85.72)	Acc@3  96.09 ( 97.41)
Epoch: [8][320/412]	Loss 1.044 (0.8984)	InvT  21.33 ( 21.22)	Acc@1  81.25 ( 85.60)	Acc@3  96.09 ( 97.38)
Epoch: [8][340/412]	Loss 0.8291 (0.8992)	InvT  21.34 ( 21.23)	Acc@1  87.50 ( 85.61)	Acc@3  98.44 ( 97.37)
Epoch: [8][360/412]	Loss 0.8582 (0.9006)	InvT  21.35 ( 21.23)	Acc@1  87.89 ( 85.63)	Acc@3  97.27 ( 97.37)
Epoch: [8][380/412]	Loss 1.062 (0.9026)	InvT  21.36 ( 21.24)	Acc@1  81.25 ( 85.58)	Acc@3  96.09 ( 97.34)
Epoch: [8][400/412]	Loss 0.798 (0.903)	InvT  21.37 ( 21.25)	Acc@1  85.16 ( 85.55)	Acc@3  98.83 ( 97.35)
Learning rate: 2.757276329305875e-05
Epoch 8, valid metric: {"Acc@1": 33.2, "Acc@3": 49.2, "loss": 3.392}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch8.mdl
Epoch: [9][  0/412]	Loss 0.7855 (0.7855)	InvT  21.38 ( 21.38)	Acc@1  86.72 ( 86.72)	Acc@3  98.05 ( 98.05)
Epoch: [9][ 20/412]	Loss 0.8414 (0.6909)	InvT  21.40 ( 21.39)	Acc@1  87.11 ( 89.19)	Acc@3  97.27 ( 98.62)
Epoch: [9][ 40/412]	Loss 0.6797 (0.6869)	InvT  21.41 ( 21.40)	Acc@1  90.62 ( 89.40)	Acc@3  98.05 ( 98.67)
Epoch: [9][ 60/412]	Loss 0.6967 (0.7037)	InvT  21.43 ( 21.40)	Acc@1  87.50 ( 88.83)	Acc@3  98.05 ( 98.58)
Epoch: [9][ 80/412]	Loss 0.6871 (0.7023)	InvT  21.44 ( 21.41)	Acc@1  88.67 ( 88.85)	Acc@3  99.22 ( 98.55)
Epoch: [9][100/412]	Loss 0.7263 (0.7081)	InvT  21.45 ( 21.42)	Acc@1  86.33 ( 88.73)	Acc@3  98.05 ( 98.51)
Epoch: [9][120/412]	Loss 0.8513 (0.7091)	InvT  21.47 ( 21.42)	Acc@1  85.16 ( 88.72)	Acc@3  98.05 ( 98.47)
Epoch: [9][140/412]	Loss 0.7091 (0.7162)	InvT  21.48 ( 21.43)	Acc@1  88.67 ( 88.51)	Acc@3  98.05 ( 98.44)
Epoch: [9][160/412]	Loss 0.852 (0.7207)	InvT  21.49 ( 21.44)	Acc@1  85.16 ( 88.42)	Acc@3  98.05 ( 98.42)
Epoch: [9][180/412]	Loss 0.7857 (0.7258)	InvT  21.51 ( 21.45)	Acc@1  85.94 ( 88.27)	Acc@3  98.44 ( 98.33)
Epoch: [9][200/412]	Loss 0.7211 (0.7251)	InvT  21.52 ( 21.45)	Acc@1  90.23 ( 88.26)	Acc@3  97.66 ( 98.33)
Epoch: [9][220/412]	Loss 0.743 (0.728)	InvT  21.53 ( 21.46)	Acc@1  89.84 ( 88.27)	Acc@3  98.83 ( 98.31)
Epoch: [9][240/412]	Loss 0.6613 (0.7284)	InvT  21.55 ( 21.47)	Acc@1  89.45 ( 88.25)	Acc@3  99.61 ( 98.30)
Epoch: [9][260/412]	Loss 0.6695 (0.732)	InvT  21.56 ( 21.47)	Acc@1  90.23 ( 88.20)	Acc@3  98.05 ( 98.27)
Epoch: [9][280/412]	Loss 0.8045 (0.7337)	InvT  21.57 ( 21.48)	Acc@1  88.67 ( 88.18)	Acc@3  97.27 ( 98.25)
Epoch: [9][300/412]	Loss 0.6654 (0.7353)	InvT  21.58 ( 21.49)	Acc@1  88.28 ( 88.13)	Acc@3  99.22 ( 98.26)
Epoch: [9][320/412]	Loss 0.7538 (0.737)	InvT  21.60 ( 21.49)	Acc@1  89.84 ( 88.11)	Acc@3  96.48 ( 98.26)
Epoch: [9][340/412]	Loss 0.7306 (0.7376)	InvT  21.61 ( 21.50)	Acc@1  88.28 ( 88.10)	Acc@3  98.05 ( 98.26)
Epoch: [9][360/412]	Loss 0.8335 (0.7392)	InvT  21.62 ( 21.51)	Acc@1  87.50 ( 88.10)	Acc@3  96.88 ( 98.24)
Epoch: [9][380/412]	Loss 0.7622 (0.7408)	InvT  21.63 ( 21.51)	Acc@1  87.89 ( 88.07)	Acc@3  98.05 ( 98.23)
Epoch: [9][400/412]	Loss 0.7384 (0.7423)	InvT  21.64 ( 21.52)	Acc@1  91.02 ( 88.03)	Acc@3  98.83 ( 98.22)
Learning rate: 2.7270459325930635e-05
Epoch 9, valid metric: {"Acc@1": 34.9, "Acc@3": 50.3, "loss": 3.416}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch9.mdl
Epoch: [10][  0/412]	Loss 0.5346 (0.5346)	InvT  21.65 ( 21.65)	Acc@1  91.41 ( 91.41)	Acc@3  98.44 ( 98.44)
Epoch: [10][ 20/412]	Loss 0.5489 (0.5776)	InvT  21.66 ( 21.66)	Acc@1  91.41 ( 91.37)	Acc@3  99.22 ( 99.01)
Epoch: [10][ 40/412]	Loss 0.4986 (0.5679)	InvT  21.68 ( 21.66)	Acc@1  92.97 ( 91.25)	Acc@3  99.61 ( 99.02)
Epoch: [10][ 60/412]	Loss 0.5193 (0.5696)	InvT  21.69 ( 21.67)	Acc@1  91.41 ( 91.18)	Acc@3  98.83 ( 99.06)
Epoch: [10][ 80/412]	Loss 0.6129 (0.5693)	InvT  21.71 ( 21.68)	Acc@1  90.23 ( 91.14)	Acc@3  96.88 ( 99.04)
Epoch: [10][100/412]	Loss 0.6484 (0.5708)	InvT  21.72 ( 21.69)	Acc@1  90.23 ( 91.09)	Acc@3  98.44 ( 99.02)
Epoch: [10][120/412]	Loss 0.5676 (0.5733)	InvT  21.73 ( 21.69)	Acc@1  89.84 ( 90.95)	Acc@3  99.22 ( 99.04)
Epoch: [10][140/412]	Loss 0.4521 (0.5778)	InvT  21.75 ( 21.70)	Acc@1  92.58 ( 90.87)	Acc@3 100.00 ( 99.01)
Epoch: [10][160/412]	Loss 0.6581 (0.5842)	InvT  21.76 ( 21.71)	Acc@1  88.67 ( 90.75)	Acc@3  98.83 ( 98.99)
Epoch: [10][180/412]	Loss 0.591 (0.5885)	InvT  21.77 ( 21.71)	Acc@1  93.75 ( 90.69)	Acc@3  98.83 ( 98.97)
Epoch: [10][200/412]	Loss 0.7056 (0.5941)	InvT  21.78 ( 21.72)	Acc@1  89.45 ( 90.59)	Acc@3  98.83 ( 98.95)
Epoch: [10][220/412]	Loss 0.5544 (0.5984)	InvT  21.80 ( 21.73)	Acc@1  89.45 ( 90.49)	Acc@3  98.83 ( 98.92)
Epoch: [10][240/412]	Loss 0.5871 (0.6009)	InvT  21.81 ( 21.73)	Acc@1  91.02 ( 90.47)	Acc@3  98.83 ( 98.92)
Epoch: [10][260/412]	Loss 0.6615 (0.6022)	InvT  21.82 ( 21.74)	Acc@1  89.45 ( 90.39)	Acc@3  98.83 ( 98.92)
Epoch: [10][280/412]	Loss 0.4742 (0.6051)	InvT  21.83 ( 21.74)	Acc@1  92.58 ( 90.32)	Acc@3 100.00 ( 98.89)
Epoch: [10][300/412]	Loss 0.6594 (0.6078)	InvT  21.85 ( 21.75)	Acc@1  87.50 ( 90.26)	Acc@3  98.83 ( 98.89)
Epoch: [10][320/412]	Loss 0.5238 (0.6082)	InvT  21.86 ( 21.76)	Acc@1  89.84 ( 90.25)	Acc@3  99.22 ( 98.89)
Epoch: [10][340/412]	Loss 0.697 (0.6109)	InvT  21.87 ( 21.76)	Acc@1  87.11 ( 90.19)	Acc@3  98.05 ( 98.88)
Epoch: [10][360/412]	Loss 0.7396 (0.6118)	InvT  21.88 ( 21.77)	Acc@1  88.28 ( 90.17)	Acc@3  96.88 ( 98.89)
Epoch: [10][380/412]	Loss 0.5945 (0.6147)	InvT  21.89 ( 21.78)	Acc@1  89.84 ( 90.09)	Acc@3  98.83 ( 98.88)
Epoch: [10][400/412]	Loss 0.7116 (0.6161)	InvT  21.90 ( 21.78)	Acc@1  87.89 ( 90.07)	Acc@3  98.44 ( 98.87)
Learning rate: 2.6968155358802524e-05
Epoch 10, valid metric: {"Acc@1": 33.8, "Acc@3": 50.4, "loss": 3.444}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch10.mdl
Epoch: [11][  0/412]	Loss 0.4304 (0.4304)	InvT  21.91 ( 21.91)	Acc@1  94.92 ( 94.92)	Acc@3 100.00 (100.00)
Epoch: [11][ 20/412]	Loss 0.3686 (0.4557)	InvT  21.92 ( 21.92)	Acc@1  95.70 ( 93.56)	Acc@3  99.61 ( 99.46)
Epoch: [11][ 40/412]	Loss 0.4494 (0.4638)	InvT  21.94 ( 21.92)	Acc@1  92.58 ( 93.28)	Acc@3  98.83 ( 99.42)
Epoch: [11][ 60/412]	Loss 0.4978 (0.4684)	InvT  21.95 ( 21.93)	Acc@1  92.97 ( 93.03)	Acc@3  99.22 ( 99.40)
Epoch: [11][ 80/412]	Loss 0.591 (0.4752)	InvT  21.96 ( 21.94)	Acc@1  89.45 ( 92.73)	Acc@3  99.22 ( 99.41)
Epoch: [11][100/412]	Loss 0.4066 (0.4736)	InvT  21.98 ( 21.94)	Acc@1  94.14 ( 92.69)	Acc@3  99.61 ( 99.42)
Epoch: [11][120/412]	Loss 0.4365 (0.4835)	InvT  21.99 ( 21.95)	Acc@1  94.14 ( 92.45)	Acc@3  99.22 ( 99.40)
Epoch: [11][140/412]	Loss 0.4721 (0.4895)	InvT  22.00 ( 21.96)	Acc@1  92.58 ( 92.30)	Acc@3  99.61 ( 99.40)
Epoch: [11][160/412]	Loss 0.5046 (0.4915)	InvT  22.01 ( 21.96)	Acc@1  91.80 ( 92.24)	Acc@3  98.44 ( 99.40)
Epoch: [11][180/412]	Loss 0.6552 (0.494)	InvT  22.02 ( 21.97)	Acc@1  88.28 ( 92.19)	Acc@3  98.83 ( 99.40)
Epoch: [11][200/412]	Loss 0.6886 (0.5001)	InvT  22.04 ( 21.98)	Acc@1  88.28 ( 92.09)	Acc@3  98.83 ( 99.37)
Epoch: [11][220/412]	Loss 0.4459 (0.5034)	InvT  22.05 ( 21.98)	Acc@1  92.58 ( 92.06)	Acc@3  99.61 ( 99.36)
Epoch: [11][240/412]	Loss 0.5216 (0.504)	InvT  22.06 ( 21.99)	Acc@1  91.80 ( 92.04)	Acc@3  99.22 ( 99.35)
Epoch: [11][260/412]	Loss 0.6485 (0.506)	InvT  22.07 ( 21.99)	Acc@1  89.45 ( 92.00)	Acc@3  97.27 ( 99.33)
Epoch: [11][280/412]	Loss 0.6084 (0.5069)	InvT  22.08 ( 22.00)	Acc@1  89.84 ( 92.00)	Acc@3  99.22 ( 99.33)
Epoch: [11][300/412]	Loss 0.5716 (0.5099)	InvT  22.10 ( 22.01)	Acc@1  90.23 ( 91.96)	Acc@3  99.61 ( 99.30)
Epoch: [11][320/412]	Loss 0.5594 (0.5116)	InvT  22.11 ( 22.01)	Acc@1  90.62 ( 91.90)	Acc@3  99.22 ( 99.30)
Epoch: [11][340/412]	Loss 0.7179 (0.514)	InvT  22.12 ( 22.02)	Acc@1  86.33 ( 91.85)	Acc@3  98.05 ( 99.30)
Epoch: [11][360/412]	Loss 0.5257 (0.5137)	InvT  22.13 ( 22.02)	Acc@1  90.62 ( 91.85)	Acc@3 100.00 ( 99.29)
Epoch: [11][380/412]	Loss 0.5713 (0.5162)	InvT  22.14 ( 22.03)	Acc@1  92.19 ( 91.80)	Acc@3  99.61 ( 99.28)
Epoch: [11][400/412]	Loss 0.6452 (0.5183)	InvT  22.15 ( 22.04)	Acc@1  88.28 ( 91.75)	Acc@3  98.44 ( 99.27)
Learning rate: 2.6665851391674412e-05
Epoch 11, valid metric: {"Acc@1": 34.2, "Acc@3": 50.8, "loss": 3.45}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch11.mdl
Epoch: [12][  0/412]	Loss 0.4468 (0.4468)	InvT  22.16 ( 22.16)	Acc@1  92.97 ( 92.97)	Acc@3 100.00 (100.00)
Epoch: [12][ 20/412]	Loss 0.5063 (0.4009)	InvT  22.17 ( 22.17)	Acc@1  91.41 ( 93.71)	Acc@3  99.22 ( 99.68)
Epoch: [12][ 40/412]	Loss 0.3636 (0.4059)	InvT  22.18 ( 22.17)	Acc@1  94.14 ( 93.68)	Acc@3 100.00 ( 99.68)
Epoch: [12][ 60/412]	Loss 0.398 (0.4096)	InvT  22.20 ( 22.18)	Acc@1  92.58 ( 93.60)	Acc@3 100.00 ( 99.73)
Epoch: [12][ 80/412]	Loss 0.4162 (0.4165)	InvT  22.21 ( 22.18)	Acc@1  94.53 ( 93.45)	Acc@3  99.22 ( 99.69)
Epoch: [12][100/412]	Loss 0.4475 (0.4184)	InvT  22.22 ( 22.19)	Acc@1  93.36 ( 93.47)	Acc@3 100.00 ( 99.68)
Epoch: [12][120/412]	Loss 0.4065 (0.4243)	InvT  22.23 ( 22.20)	Acc@1  92.58 ( 93.38)	Acc@3  99.61 ( 99.63)
Epoch: [12][140/412]	Loss 0.4662 (0.4266)	InvT  22.24 ( 22.20)	Acc@1  94.92 ( 93.32)	Acc@3  99.61 ( 99.63)
Epoch: [12][160/412]	Loss 0.3906 (0.4289)	InvT  22.25 ( 22.21)	Acc@1  94.14 ( 93.26)	Acc@3  99.22 ( 99.62)
Epoch: [12][180/412]	Loss 0.4131 (0.4292)	InvT  22.27 ( 22.21)	Acc@1  94.53 ( 93.31)	Acc@3  99.61 ( 99.59)
Epoch: [12][200/412]	Loss 0.6288 (0.4345)	InvT  22.28 ( 22.22)	Acc@1  89.84 ( 93.20)	Acc@3  98.83 ( 99.57)
Epoch: [12][220/412]	Loss 0.378 (0.4363)	InvT  22.29 ( 22.22)	Acc@1  94.92 ( 93.15)	Acc@3  99.22 ( 99.54)
Epoch: [12][240/412]	Loss 0.4894 (0.4378)	InvT  22.30 ( 22.23)	Acc@1  91.80 ( 93.10)	Acc@3  98.83 ( 99.53)
Epoch: [12][260/412]	Loss 0.6091 (0.4393)	InvT  22.31 ( 22.24)	Acc@1  88.67 ( 93.07)	Acc@3  99.22 ( 99.52)
Epoch: [12][280/412]	Loss 0.4259 (0.4401)	InvT  22.32 ( 22.24)	Acc@1  92.97 ( 93.05)	Acc@3  99.22 ( 99.51)
Epoch: [12][300/412]	Loss 0.4477 (0.4417)	InvT  22.33 ( 22.25)	Acc@1  92.58 ( 93.02)	Acc@3 100.00 ( 99.50)
Epoch: [12][320/412]	Loss 0.3909 (0.4428)	InvT  22.34 ( 22.25)	Acc@1  93.36 ( 92.96)	Acc@3 100.00 ( 99.51)
Epoch: [12][340/412]	Loss 0.3679 (0.4433)	InvT  22.36 ( 22.26)	Acc@1  92.97 ( 92.96)	Acc@3  99.22 ( 99.50)
Epoch: [12][360/412]	Loss 0.3675 (0.4431)	InvT  22.37 ( 22.26)	Acc@1  94.53 ( 92.94)	Acc@3  99.61 ( 99.51)
Epoch: [12][380/412]	Loss 0.4098 (0.4444)	InvT  22.38 ( 22.27)	Acc@1  92.97 ( 92.91)	Acc@3 100.00 ( 99.50)
Epoch: [12][400/412]	Loss 0.547 (0.4454)	InvT  22.39 ( 22.28)	Acc@1  89.84 ( 92.93)	Acc@3  98.44 ( 99.50)
Learning rate: 2.63635474245463e-05
Epoch 12, valid metric: {"Acc@1": 34.6, "Acc@3": 51.0, "loss": 3.456}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch12.mdl
Epoch: [13][  0/412]	Loss 0.3583 (0.3583)	InvT  22.40 ( 22.40)	Acc@1  94.92 ( 94.92)	Acc@3  99.22 ( 99.22)
Epoch: [13][ 20/412]	Loss 0.3287 (0.3557)	InvT  22.41 ( 22.40)	Acc@1  94.53 ( 94.42)	Acc@3 100.00 ( 99.61)
Epoch: [13][ 40/412]	Loss 0.3592 (0.3459)	InvT  22.42 ( 22.41)	Acc@1  94.92 ( 94.56)	Acc@3  99.61 ( 99.70)
Epoch: [13][ 60/412]	Loss 0.2698 (0.344)	InvT  22.43 ( 22.41)	Acc@1  95.31 ( 94.63)	Acc@3  99.61 ( 99.70)
Epoch: [13][ 80/412]	Loss 0.3176 (0.3408)	InvT  22.44 ( 22.42)	Acc@1  94.92 ( 94.66)	Acc@3  98.83 ( 99.71)
Epoch: [13][100/412]	Loss 0.3385 (0.3437)	InvT  22.45 ( 22.42)	Acc@1  92.97 ( 94.59)	Acc@3 100.00 ( 99.72)
Epoch: [13][120/412]	Loss 0.2393 (0.3431)	InvT  22.46 ( 22.43)	Acc@1  96.88 ( 94.61)	Acc@3 100.00 ( 99.73)
Epoch: [13][140/412]	Loss 0.4359 (0.3485)	InvT  22.47 ( 22.44)	Acc@1  91.02 ( 94.48)	Acc@3 100.00 ( 99.71)
Epoch: [13][160/412]	Loss 0.3567 (0.3493)	InvT  22.48 ( 22.44)	Acc@1  94.14 ( 94.50)	Acc@3  99.61 ( 99.72)
Epoch: [13][180/412]	Loss 0.3433 (0.3509)	InvT  22.49 ( 22.45)	Acc@1  93.36 ( 94.48)	Acc@3  99.61 ( 99.71)
Epoch: [13][200/412]	Loss 0.2881 (0.3537)	InvT  22.51 ( 22.45)	Acc@1  97.27 ( 94.44)	Acc@3 100.00 ( 99.71)
Epoch: [13][220/412]	Loss 0.4408 (0.355)	InvT  22.52 ( 22.46)	Acc@1  94.14 ( 94.43)	Acc@3  99.61 ( 99.71)
Epoch: [13][240/412]	Loss 0.3365 (0.3567)	InvT  22.53 ( 22.46)	Acc@1  94.92 ( 94.39)	Acc@3 100.00 ( 99.71)
Epoch: [13][260/412]	Loss 0.3808 (0.3599)	InvT  22.54 ( 22.47)	Acc@1  94.14 ( 94.34)	Acc@3  99.22 ( 99.70)
Epoch: [13][280/412]	Loss 0.41 (0.3618)	InvT  22.55 ( 22.47)	Acc@1  93.75 ( 94.30)	Acc@3  99.61 ( 99.70)
Epoch: [13][300/412]	Loss 0.3916 (0.3647)	InvT  22.56 ( 22.48)	Acc@1  94.14 ( 94.23)	Acc@3  99.22 ( 99.68)
Epoch: [13][320/412]	Loss 0.35 (0.3667)	InvT  22.57 ( 22.48)	Acc@1  93.75 ( 94.20)	Acc@3  99.61 ( 99.68)
Epoch: [13][340/412]	Loss 0.4137 (0.3684)	InvT  22.58 ( 22.49)	Acc@1  94.14 ( 94.19)	Acc@3  99.22 ( 99.67)
Epoch: [13][360/412]	Loss 0.3799 (0.3695)	InvT  22.59 ( 22.50)	Acc@1  94.53 ( 94.16)	Acc@3 100.00 ( 99.68)
Epoch: [13][380/412]	Loss 0.3231 (0.3704)	InvT  22.60 ( 22.50)	Acc@1  95.70 ( 94.16)	Acc@3  99.61 ( 99.67)
Epoch: [13][400/412]	Loss 0.3698 (0.3721)	InvT  22.62 ( 22.51)	Acc@1  95.31 ( 94.13)	Acc@3  99.61 ( 99.67)
Learning rate: 2.6061243457418186e-05
Epoch 13, valid metric: {"Acc@1": 36.6, "Acc@3": 51.5, "loss": 3.47}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch13.mdl
Epoch: [14][  0/412]	Loss 0.2507 (0.2507)	InvT  22.62 ( 22.62)	Acc@1  95.31 ( 95.31)	Acc@3  99.61 ( 99.61)
Epoch: [14][ 20/412]	Loss 0.401 (0.3127)	InvT  22.63 ( 22.63)	Acc@1  91.80 ( 94.74)	Acc@3  99.22 ( 99.76)
Epoch: [14][ 40/412]	Loss 0.3614 (0.3083)	InvT  22.64 ( 22.63)	Acc@1  93.75 ( 95.09)	Acc@3 100.00 ( 99.77)
Epoch: [14][ 60/412]	Loss 0.1967 (0.3036)	InvT  22.65 ( 22.64)	Acc@1  98.44 ( 95.28)	Acc@3 100.00 ( 99.81)
Epoch: [14][ 80/412]	Loss 0.2276 (0.3061)	InvT  22.66 ( 22.64)	Acc@1  96.88 ( 95.25)	Acc@3 100.00 ( 99.80)
Epoch: [14][100/412]	Loss 0.2722 (0.3057)	InvT  22.67 ( 22.65)	Acc@1  95.70 ( 95.22)	Acc@3 100.00 ( 99.81)
Epoch: [14][120/412]	Loss 0.3224 (0.3062)	InvT  22.68 ( 22.65)	Acc@1  95.31 ( 95.24)	Acc@3  99.61 ( 99.81)
Epoch: [14][140/412]	Loss 0.2907 (0.3092)	InvT  22.69 ( 22.66)	Acc@1  96.09 ( 95.18)	Acc@3  99.61 ( 99.81)
Epoch: [14][160/412]	Loss 0.2786 (0.3108)	InvT  22.70 ( 22.66)	Acc@1  96.09 ( 95.13)	Acc@3  99.61 ( 99.80)
Epoch: [14][180/412]	Loss 0.3755 (0.3139)	InvT  22.71 ( 22.67)	Acc@1  94.92 ( 95.12)	Acc@3  99.61 ( 99.79)
Epoch: [14][200/412]	Loss 0.3949 (0.315)	InvT  22.72 ( 22.67)	Acc@1  94.53 ( 95.09)	Acc@3  99.61 ( 99.78)
Epoch: [14][220/412]	Loss 0.3651 (0.3171)	InvT  22.74 ( 22.68)	Acc@1  94.14 ( 95.08)	Acc@3 100.00 ( 99.78)
Epoch: [14][240/412]	Loss 0.3011 (0.3181)	InvT  22.75 ( 22.68)	Acc@1  96.09 ( 95.07)	Acc@3  99.61 ( 99.77)
Epoch: [14][260/412]	Loss 0.3109 (0.3179)	InvT  22.76 ( 22.69)	Acc@1  95.70 ( 95.11)	Acc@3 100.00 ( 99.78)
Epoch: [14][280/412]	Loss 0.3446 (0.3199)	InvT  22.77 ( 22.69)	Acc@1  92.97 ( 95.07)	Acc@3 100.00 ( 99.78)
Epoch: [14][300/412]	Loss 0.2892 (0.3204)	InvT  22.78 ( 22.70)	Acc@1  94.92 ( 95.05)	Acc@3 100.00 ( 99.78)
Epoch: [14][320/412]	Loss 0.3648 (0.321)	InvT  22.79 ( 22.70)	Acc@1  93.75 ( 95.03)	Acc@3  99.61 ( 99.78)
Epoch: [14][340/412]	Loss 0.2945 (0.3241)	InvT  22.80 ( 22.71)	Acc@1  96.09 ( 94.97)	Acc@3  99.61 ( 99.78)
Epoch: [14][360/412]	Loss 0.3573 (0.3248)	InvT  22.81 ( 22.71)	Acc@1  94.53 ( 94.97)	Acc@3  99.22 ( 99.77)
Epoch: [14][380/412]	Loss 0.3372 (0.3264)	InvT  22.82 ( 22.72)	Acc@1  94.53 ( 94.95)	Acc@3 100.00 ( 99.77)
Epoch: [14][400/412]	Loss 0.2626 (0.3276)	InvT  22.83 ( 22.73)	Acc@1  96.88 ( 94.93)	Acc@3 100.00 ( 99.77)
Learning rate: 2.5758939490290075e-05
Epoch 14, valid metric: {"Acc@1": 35.7, "Acc@3": 52.2, "loss": 3.534}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch14.mdl
Epoch: [15][  0/412]	Loss 0.2212 (0.2212)	InvT  22.84 ( 22.84)	Acc@1  95.70 ( 95.70)	Acc@3 100.00 (100.00)
Epoch: [15][ 20/412]	Loss 0.2494 (0.2673)	InvT  22.85 ( 22.84)	Acc@1  96.88 ( 96.00)	Acc@3 100.00 ( 99.91)
Epoch: [15][ 40/412]	Loss 0.2683 (0.2787)	InvT  22.86 ( 22.85)	Acc@1  95.70 ( 95.67)	Acc@3 100.00 ( 99.82)
Epoch: [15][ 60/412]	Loss 0.3055 (0.2803)	InvT  22.87 ( 22.85)	Acc@1  95.31 ( 95.74)	Acc@3  99.61 ( 99.80)
Epoch: [15][ 80/412]	Loss 0.27 (0.2851)	InvT  22.88 ( 22.86)	Acc@1  96.09 ( 95.57)	Acc@3  99.61 ( 99.82)
Epoch: [15][100/412]	Loss 0.4215 (0.2873)	InvT  22.89 ( 22.86)	Acc@1  92.97 ( 95.52)	Acc@3 100.00 ( 99.84)
Epoch: [15][120/412]	Loss 0.2117 (0.2842)	InvT  22.90 ( 22.87)	Acc@1  97.27 ( 95.63)	Acc@3 100.00 ( 99.85)
Epoch: [15][140/412]	Loss 0.3107 (0.2825)	InvT  22.91 ( 22.87)	Acc@1  95.31 ( 95.66)	Acc@3  99.61 ( 99.86)
Epoch: [15][160/412]	Loss 0.2782 (0.2825)	InvT  22.92 ( 22.88)	Acc@1  96.48 ( 95.64)	Acc@3  99.22 ( 99.84)
Epoch: [15][180/412]	Loss 0.2542 (0.2829)	InvT  22.93 ( 22.88)	Acc@1  95.70 ( 95.61)	Acc@3 100.00 ( 99.84)
Epoch: [15][200/412]	Loss 0.2847 (0.284)	InvT  22.94 ( 22.89)	Acc@1  94.92 ( 95.63)	Acc@3  99.61 ( 99.83)
Epoch: [15][220/412]	Loss 0.2776 (0.2845)	InvT  22.95 ( 22.89)	Acc@1  94.92 ( 95.60)	Acc@3 100.00 ( 99.83)
Epoch: [15][240/412]	Loss 0.3168 (0.2842)	InvT  22.96 ( 22.90)	Acc@1  96.09 ( 95.61)	Acc@3  99.61 ( 99.83)
Epoch: [15][260/412]	Loss 0.2347 (0.2849)	InvT  22.97 ( 22.90)	Acc@1  96.09 ( 95.59)	Acc@3 100.00 ( 99.82)
Epoch: [15][280/412]	Loss 0.2958 (0.2838)	InvT  22.98 ( 22.91)	Acc@1  95.70 ( 95.63)	Acc@3 100.00 ( 99.82)
Epoch: [15][300/412]	Loss 0.329 (0.2842)	InvT  22.99 ( 22.91)	Acc@1  96.48 ( 95.63)	Acc@3 100.00 ( 99.82)
Epoch: [15][320/412]	Loss 0.2278 (0.283)	InvT  23.00 ( 22.92)	Acc@1  97.27 ( 95.64)	Acc@3  99.61 ( 99.83)
Epoch: [15][340/412]	Loss 0.2865 (0.2834)	InvT  23.01 ( 22.92)	Acc@1  96.88 ( 95.63)	Acc@3 100.00 ( 99.83)
Epoch: [15][360/412]	Loss 0.2026 (0.2836)	InvT  23.02 ( 22.93)	Acc@1  98.05 ( 95.63)	Acc@3 100.00 ( 99.83)
Epoch: [15][380/412]	Loss 0.1895 (0.2844)	InvT  23.03 ( 22.93)	Acc@1  96.48 ( 95.62)	Acc@3  99.61 ( 99.83)
Epoch: [15][400/412]	Loss 0.3144 (0.2857)	InvT  23.04 ( 22.94)	Acc@1  95.70 ( 95.60)	Acc@3  99.61 ( 99.83)
Learning rate: 2.5456635523161963e-05
Epoch 15, valid metric: {"Acc@1": 36.8, "Acc@3": 51.4, "loss": 3.442}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch15.mdl
Epoch: [16][  0/412]	Loss 0.2418 (0.2418)	InvT  23.05 ( 23.05)	Acc@1  96.88 ( 96.88)	Acc@3 100.00 (100.00)
Epoch: [16][ 20/412]	Loss 0.213 (0.2403)	InvT  23.05 ( 23.05)	Acc@1  96.48 ( 96.39)	Acc@3 100.00 ( 99.94)
Epoch: [16][ 40/412]	Loss 0.2261 (0.2491)	InvT  23.06 ( 23.05)	Acc@1  97.66 ( 96.22)	Acc@3  99.61 ( 99.92)
Epoch: [16][ 60/412]	Loss 0.366 (0.2473)	InvT  23.07 ( 23.06)	Acc@1  94.14 ( 96.29)	Acc@3  99.61 ( 99.94)
Epoch: [16][ 80/412]	Loss 0.2923 (0.2439)	InvT  23.08 ( 23.06)	Acc@1  95.31 ( 96.29)	Acc@3  99.22 ( 99.92)
Epoch: [16][100/412]	Loss 0.2 (0.2427)	InvT  23.09 ( 23.07)	Acc@1  97.27 ( 96.28)	Acc@3 100.00 ( 99.93)
Epoch: [16][120/412]	Loss 0.2478 (0.2425)	InvT  23.10 ( 23.07)	Acc@1  96.88 ( 96.29)	Acc@3 100.00 ( 99.93)
Epoch: [16][140/412]	Loss 0.2756 (0.2416)	InvT  23.11 ( 23.08)	Acc@1  94.92 ( 96.30)	Acc@3  99.61 ( 99.91)
Epoch: [16][160/412]	Loss 0.2955 (0.2422)	InvT  23.12 ( 23.08)	Acc@1  94.92 ( 96.31)	Acc@3  99.22 ( 99.90)
Epoch: [16][180/412]	Loss 0.2256 (0.2429)	InvT  23.13 ( 23.09)	Acc@1  96.09 ( 96.32)	Acc@3 100.00 ( 99.90)
Epoch: [16][200/412]	Loss 0.2993 (0.2438)	InvT  23.14 ( 23.09)	Acc@1  94.14 ( 96.30)	Acc@3 100.00 ( 99.90)
Epoch: [16][220/412]	Loss 0.226 (0.2453)	InvT  23.15 ( 23.10)	Acc@1  97.27 ( 96.27)	Acc@3 100.00 ( 99.90)
Epoch: [16][240/412]	Loss 0.2775 (0.2467)	InvT  23.16 ( 23.10)	Acc@1  96.09 ( 96.25)	Acc@3 100.00 ( 99.90)
Epoch: [16][260/412]	Loss 0.2709 (0.2466)	InvT  23.17 ( 23.11)	Acc@1  95.70 ( 96.26)	Acc@3 100.00 ( 99.91)
Epoch: [16][280/412]	Loss 0.2053 (0.2459)	InvT  23.18 ( 23.11)	Acc@1  97.66 ( 96.27)	Acc@3  99.61 ( 99.90)
Epoch: [16][300/412]	Loss 0.2965 (0.2474)	InvT  23.19 ( 23.12)	Acc@1  94.92 ( 96.23)	Acc@3 100.00 ( 99.91)
Epoch: [16][320/412]	Loss 0.2286 (0.2485)	InvT  23.20 ( 23.12)	Acc@1  97.27 ( 96.20)	Acc@3 100.00 ( 99.90)
Epoch: [16][340/412]	Loss 0.3005 (0.2505)	InvT  23.21 ( 23.13)	Acc@1  95.31 ( 96.15)	Acc@3 100.00 ( 99.90)
Epoch: [16][360/412]	Loss 0.2526 (0.2508)	InvT  23.22 ( 23.13)	Acc@1  94.92 ( 96.13)	Acc@3 100.00 ( 99.90)
Epoch: [16][380/412]	Loss 0.1851 (0.2521)	InvT  23.23 ( 23.14)	Acc@1  98.05 ( 96.11)	Acc@3 100.00 ( 99.90)
Epoch: [16][400/412]	Loss 0.2407 (0.2525)	InvT  23.24 ( 23.14)	Acc@1  96.88 ( 96.10)	Acc@3 100.00 ( 99.90)
Learning rate: 2.515433155603385e-05
Epoch 16, valid metric: {"Acc@1": 37.9, "Acc@3": 53.3, "loss": 3.508}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch16.mdl
Epoch: [17][  0/412]	Loss 0.1846 (0.1846)	InvT  23.25 ( 23.25)	Acc@1  97.66 ( 97.66)	Acc@3 100.00 (100.00)
Epoch: [17][ 20/412]	Loss 0.231 (0.2302)	InvT  23.25 ( 23.25)	Acc@1  95.70 ( 96.22)	Acc@3 100.00 ( 99.96)
Epoch: [17][ 40/412]	Loss 0.1996 (0.2251)	InvT  23.26 ( 23.25)	Acc@1  97.27 ( 96.62)	Acc@3  99.61 ( 99.93)
Epoch: [17][ 60/412]	Loss 0.1756 (0.219)	InvT  23.27 ( 23.26)	Acc@1  97.66 ( 96.68)	Acc@3 100.00 ( 99.94)
Epoch: [17][ 80/412]	Loss 0.1886 (0.2179)	InvT  23.28 ( 23.26)	Acc@1  97.66 ( 96.72)	Acc@3 100.00 ( 99.93)
Epoch: [17][100/412]	Loss 0.2114 (0.2157)	InvT  23.29 ( 23.27)	Acc@1  97.27 ( 96.76)	Acc@3 100.00 ( 99.93)
Epoch: [17][120/412]	Loss 0.151 (0.2135)	InvT  23.30 ( 23.27)	Acc@1  98.44 ( 96.79)	Acc@3 100.00 ( 99.94)
Epoch: [17][140/412]	Loss 0.2227 (0.2139)	InvT  23.31 ( 23.28)	Acc@1  96.48 ( 96.77)	Acc@3 100.00 ( 99.94)
Epoch: [17][160/412]	Loss 0.2531 (0.2153)	InvT  23.32 ( 23.28)	Acc@1  96.48 ( 96.74)	Acc@3 100.00 ( 99.94)
Epoch: [17][180/412]	Loss 0.2132 (0.2165)	InvT  23.33 ( 23.29)	Acc@1  95.70 ( 96.69)	Acc@3  99.61 ( 99.93)
Epoch: [17][200/412]	Loss 0.2322 (0.2188)	InvT  23.34 ( 23.29)	Acc@1  96.88 ( 96.66)	Acc@3  99.61 ( 99.93)
Epoch: [17][220/412]	Loss 0.2052 (0.2194)	InvT  23.35 ( 23.30)	Acc@1  98.05 ( 96.64)	Acc@3 100.00 ( 99.93)
Epoch: [17][240/412]	Loss 0.1784 (0.2215)	InvT  23.36 ( 23.30)	Acc@1  96.88 ( 96.60)	Acc@3  99.61 ( 99.92)
Epoch: [17][260/412]	Loss 0.2323 (0.2224)	InvT  23.37 ( 23.31)	Acc@1  97.27 ( 96.56)	Acc@3 100.00 ( 99.92)
Epoch: [17][280/412]	Loss 0.207 (0.2238)	InvT  23.38 ( 23.31)	Acc@1  97.66 ( 96.54)	Acc@3 100.00 ( 99.91)
Epoch: [17][300/412]	Loss 0.2493 (0.2251)	InvT  23.39 ( 23.32)	Acc@1  96.09 ( 96.50)	Acc@3  99.61 ( 99.91)
Epoch: [17][320/412]	Loss 0.233 (0.2257)	InvT  23.40 ( 23.32)	Acc@1  96.88 ( 96.51)	Acc@3  99.61 ( 99.91)
Epoch: [17][340/412]	Loss 0.2077 (0.2265)	InvT  23.41 ( 23.32)	Acc@1  97.27 ( 96.51)	Acc@3 100.00 ( 99.91)
Epoch: [17][360/412]	Loss 0.2609 (0.2275)	InvT  23.41 ( 23.33)	Acc@1  96.48 ( 96.49)	Acc@3  99.61 ( 99.91)
Epoch: [17][380/412]	Loss 0.1846 (0.229)	InvT  23.42 ( 23.33)	Acc@1  98.83 ( 96.47)	Acc@3 100.00 ( 99.90)
Epoch: [17][400/412]	Loss 0.2382 (0.2303)	InvT  23.43 ( 23.34)	Acc@1  94.53 ( 96.44)	Acc@3 100.00 ( 99.90)
Learning rate: 2.4852027588905737e-05
Epoch 17, valid metric: {"Acc@1": 36.9, "Acc@3": 51.7, "loss": 3.584}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch17.mdl
Epoch: [18][  0/412]	Loss 0.2583 (0.2583)	InvT  23.44 ( 23.44)	Acc@1  96.09 ( 96.09)	Acc@3 100.00 (100.00)
Epoch: [18][ 20/412]	Loss 0.2342 (0.1988)	InvT  23.45 ( 23.45)	Acc@1  97.27 ( 97.01)	Acc@3 100.00 ( 99.94)
Epoch: [18][ 40/412]	Loss 0.2044 (0.1941)	InvT  23.46 ( 23.45)	Acc@1  97.27 ( 97.04)	Acc@3 100.00 ( 99.95)
Epoch: [18][ 60/412]	Loss 0.2068 (0.2018)	InvT  23.47 ( 23.45)	Acc@1  98.05 ( 96.91)	Acc@3  99.61 ( 99.94)
Epoch: [18][ 80/412]	Loss 0.2319 (0.1995)	InvT  23.48 ( 23.46)	Acc@1  96.09 ( 97.02)	Acc@3 100.00 ( 99.94)
Epoch: [18][100/412]	Loss 0.1623 (0.1982)	InvT  23.49 ( 23.46)	Acc@1  97.66 ( 97.06)	Acc@3 100.00 ( 99.94)
Epoch: [18][120/412]	Loss 0.1566 (0.1985)	InvT  23.50 ( 23.47)	Acc@1  98.44 ( 97.08)	Acc@3 100.00 ( 99.94)
Epoch: [18][140/412]	Loss 0.3153 (0.1999)	InvT  23.50 ( 23.47)	Acc@1  96.09 ( 97.06)	Acc@3 100.00 ( 99.94)
Epoch: [18][160/412]	Loss 0.09999 (0.1997)	InvT  23.51 ( 23.48)	Acc@1  99.61 ( 97.05)	Acc@3 100.00 ( 99.94)
Epoch: [18][180/412]	Loss 0.2139 (0.1993)	InvT  23.52 ( 23.48)	Acc@1  96.88 ( 97.02)	Acc@3  99.61 ( 99.94)
Epoch: [18][200/412]	Loss 0.1707 (0.2011)	InvT  23.53 ( 23.49)	Acc@1  98.05 ( 96.98)	Acc@3 100.00 ( 99.93)
Epoch: [18][220/412]	Loss 0.169 (0.2026)	InvT  23.54 ( 23.49)	Acc@1  98.44 ( 96.97)	Acc@3 100.00 ( 99.93)
Epoch: [18][240/412]	Loss 0.1976 (0.2011)	InvT  23.55 ( 23.50)	Acc@1  98.05 ( 97.01)	Acc@3 100.00 ( 99.93)
Epoch: [18][260/412]	Loss 0.1316 (0.2021)	InvT  23.56 ( 23.50)	Acc@1  97.27 ( 96.98)	Acc@3 100.00 ( 99.93)
Epoch: [18][280/412]	Loss 0.2133 (0.2025)	InvT  23.57 ( 23.51)	Acc@1  96.88 ( 96.96)	Acc@3 100.00 ( 99.93)
Epoch: [18][300/412]	Loss 0.2304 (0.2036)	InvT  23.58 ( 23.51)	Acc@1  95.31 ( 96.92)	Acc@3  99.22 ( 99.93)
Epoch: [18][320/412]	Loss 0.2279 (0.2041)	InvT  23.59 ( 23.52)	Acc@1  96.09 ( 96.91)	Acc@3  99.61 ( 99.93)
Epoch: [18][340/412]	Loss 0.2456 (0.2044)	InvT  23.60 ( 23.52)	Acc@1  96.48 ( 96.91)	Acc@3  99.22 ( 99.93)
Epoch: [18][360/412]	Loss 0.2392 (0.2048)	InvT  23.61 ( 23.52)	Acc@1  96.88 ( 96.91)	Acc@3  99.61 ( 99.92)
Epoch: [18][380/412]	Loss 0.2724 (0.2048)	InvT  23.62 ( 23.53)	Acc@1  95.31 ( 96.90)	Acc@3  99.61 ( 99.93)
Epoch: [18][400/412]	Loss 0.1768 (0.2054)	InvT  23.63 ( 23.53)	Acc@1  97.27 ( 96.90)	Acc@3 100.00 ( 99.92)
Learning rate: 2.4549723621777626e-05
Epoch 18, valid metric: {"Acc@1": 37.3, "Acc@3": 51.8, "loss": 3.565}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch18.mdl
Epoch: [19][  0/412]	Loss 0.1663 (0.1663)	InvT  23.64 ( 23.64)	Acc@1  96.88 ( 96.88)	Acc@3 100.00 (100.00)
Epoch: [19][ 20/412]	Loss 0.1603 (0.1751)	InvT  23.65 ( 23.64)	Acc@1  96.88 ( 97.38)	Acc@3 100.00 (100.00)
Epoch: [19][ 40/412]	Loss 0.1273 (0.1737)	InvT  23.65 ( 23.65)	Acc@1  99.22 ( 97.49)	Acc@3 100.00 ( 99.97)
Epoch: [19][ 60/412]	Loss 0.1917 (0.1745)	InvT  23.66 ( 23.65)	Acc@1  97.66 ( 97.42)	Acc@3 100.00 ( 99.96)
Epoch: [19][ 80/412]	Loss 0.1668 (0.1744)	InvT  23.67 ( 23.65)	Acc@1  97.27 ( 97.44)	Acc@3 100.00 ( 99.97)
Epoch: [19][100/412]	Loss 0.1405 (0.1752)	InvT  23.68 ( 23.66)	Acc@1  98.44 ( 97.44)	Acc@3  99.61 ( 99.97)
Epoch: [19][120/412]	Loss 0.1635 (0.1755)	InvT  23.69 ( 23.66)	Acc@1  97.27 ( 97.44)	Acc@3 100.00 ( 99.97)
Epoch: [19][140/412]	Loss 0.2461 (0.1781)	InvT  23.70 ( 23.67)	Acc@1  95.70 ( 97.40)	Acc@3  99.61 ( 99.96)
Epoch: [19][160/412]	Loss 0.25 (0.1791)	InvT  23.71 ( 23.67)	Acc@1  94.92 ( 97.34)	Acc@3 100.00 ( 99.96)
Epoch: [19][180/412]	Loss 0.1804 (0.1795)	InvT  23.72 ( 23.68)	Acc@1  96.48 ( 97.33)	Acc@3 100.00 ( 99.97)
Epoch: [19][200/412]	Loss 0.1351 (0.1807)	InvT  23.73 ( 23.68)	Acc@1  98.05 ( 97.29)	Acc@3  99.61 ( 99.96)
Epoch: [19][220/412]	Loss 0.2222 (0.1816)	InvT  23.74 ( 23.69)	Acc@1  96.48 ( 97.27)	Acc@3  99.61 ( 99.95)
Epoch: [19][240/412]	Loss 0.136 (0.1821)	InvT  23.75 ( 23.69)	Acc@1  98.05 ( 97.28)	Acc@3 100.00 ( 99.95)
Epoch: [19][260/412]	Loss 0.1724 (0.1833)	InvT  23.76 ( 23.70)	Acc@1  96.48 ( 97.23)	Acc@3 100.00 ( 99.95)
Epoch: [19][280/412]	Loss 0.1417 (0.1854)	InvT  23.76 ( 23.70)	Acc@1  98.05 ( 97.19)	Acc@3 100.00 ( 99.94)
Epoch: [19][300/412]	Loss 0.2153 (0.1865)	InvT  23.77 ( 23.70)	Acc@1  96.09 ( 97.17)	Acc@3 100.00 ( 99.94)
Epoch: [19][320/412]	Loss 0.1612 (0.1873)	InvT  23.78 ( 23.71)	Acc@1  97.27 ( 97.16)	Acc@3 100.00 ( 99.94)
Epoch: [19][340/412]	Loss 0.1593 (0.1874)	InvT  23.79 ( 23.71)	Acc@1  98.44 ( 97.15)	Acc@3 100.00 ( 99.94)
Epoch: [19][360/412]	Loss 0.1942 (0.1879)	InvT  23.80 ( 23.72)	Acc@1  97.27 ( 97.14)	Acc@3 100.00 ( 99.94)
Epoch: [19][380/412]	Loss 0.1503 (0.1888)	InvT  23.81 ( 23.72)	Acc@1  98.44 ( 97.12)	Acc@3 100.00 ( 99.93)
Epoch: [19][400/412]	Loss 0.175 (0.1894)	InvT  23.82 ( 23.73)	Acc@1  97.66 ( 97.11)	Acc@3 100.00 ( 99.93)
Learning rate: 2.4247419654649515e-05
Epoch 19, valid metric: {"Acc@1": 37.4, "Acc@3": 51.5, "loss": 3.497}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch19.mdl
Epoch: [20][  0/412]	Loss 0.1639 (0.1639)	InvT  23.83 ( 23.83)	Acc@1  98.05 ( 98.05)	Acc@3 100.00 (100.00)
Epoch: [20][ 20/412]	Loss 0.146 (0.1593)	InvT  23.84 ( 23.83)	Acc@1  98.05 ( 97.54)	Acc@3 100.00 ( 99.98)
Epoch: [20][ 40/412]	Loss 0.2022 (0.1679)	InvT  23.85 ( 23.84)	Acc@1  96.88 ( 97.47)	Acc@3 100.00 ( 99.96)
Epoch: [20][ 60/412]	Loss 0.1397 (0.165)	InvT  23.86 ( 23.84)	Acc@1  98.83 ( 97.57)	Acc@3 100.00 ( 99.96)
Epoch: [20][ 80/412]	Loss 0.2358 (0.1673)	InvT  23.86 ( 23.85)	Acc@1  96.48 ( 97.47)	Acc@3 100.00 ( 99.96)
Epoch: [20][100/412]	Loss 0.1871 (0.1672)	InvT  23.87 ( 23.85)	Acc@1  97.27 ( 97.48)	Acc@3 100.00 ( 99.95)
Epoch: [20][120/412]	Loss 0.1709 (0.169)	InvT  23.88 ( 23.86)	Acc@1  96.48 ( 97.45)	Acc@3 100.00 ( 99.96)
Epoch: [20][140/412]	Loss 0.1517 (0.1677)	InvT  23.89 ( 23.86)	Acc@1  97.66 ( 97.46)	Acc@3 100.00 ( 99.96)
Epoch: [20][160/412]	Loss 0.1905 (0.1682)	InvT  23.90 ( 23.86)	Acc@1  97.66 ( 97.44)	Acc@3 100.00 ( 99.96)
Epoch: [20][180/412]	Loss 0.182 (0.1704)	InvT  23.91 ( 23.87)	Acc@1  97.27 ( 97.38)	Acc@3 100.00 ( 99.96)
Epoch: [20][200/412]	Loss 0.1664 (0.169)	InvT  23.92 ( 23.87)	Acc@1  96.09 ( 97.39)	Acc@3 100.00 ( 99.96)
Epoch: [20][220/412]	Loss 0.1636 (0.1687)	InvT  23.93 ( 23.88)	Acc@1  96.48 ( 97.39)	Acc@3 100.00 ( 99.96)
Epoch: [20][240/412]	Loss 0.1978 (0.1697)	InvT  23.94 ( 23.88)	Acc@1  96.88 ( 97.38)	Acc@3  99.61 ( 99.96)
Epoch: [20][260/412]	Loss 0.2037 (0.1699)	InvT  23.95 ( 23.89)	Acc@1  96.88 ( 97.36)	Acc@3 100.00 ( 99.95)
Epoch: [20][280/412]	Loss 0.187 (0.1705)	InvT  23.96 ( 23.89)	Acc@1  96.48 ( 97.34)	Acc@3 100.00 ( 99.95)
Epoch: [20][300/412]	Loss 0.1527 (0.1713)	InvT  23.97 ( 23.90)	Acc@1  97.66 ( 97.32)	Acc@3 100.00 ( 99.96)
Epoch: [20][320/412]	Loss 0.1345 (0.1718)	InvT  23.98 ( 23.90)	Acc@1  98.05 ( 97.32)	Acc@3 100.00 ( 99.95)
Epoch: [20][340/412]	Loss 0.1697 (0.1714)	InvT  23.98 ( 23.91)	Acc@1  98.05 ( 97.33)	Acc@3 100.00 ( 99.96)
Epoch: [20][360/412]	Loss 0.2009 (0.1722)	InvT  23.99 ( 23.91)	Acc@1  96.48 ( 97.33)	Acc@3 100.00 ( 99.96)
Epoch: [20][380/412]	Loss 0.1875 (0.1724)	InvT  24.00 ( 23.92)	Acc@1  97.66 ( 97.32)	Acc@3 100.00 ( 99.96)
Epoch: [20][400/412]	Loss 0.2028 (0.1734)	InvT  24.01 ( 23.92)	Acc@1  97.27 ( 97.32)	Acc@3 100.00 ( 99.95)
Learning rate: 2.39451156875214e-05
Epoch 20, valid metric: {"Acc@1": 36.7, "Acc@3": 51.8, "loss": 3.51}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch20.mdl
Epoch: [21][  0/412]	Loss 0.1858 (0.1858)	InvT  24.02 ( 24.02)	Acc@1  96.09 ( 96.09)	Acc@3 100.00 (100.00)
Epoch: [21][ 20/412]	Loss 0.2179 (0.1672)	InvT  24.03 ( 24.02)	Acc@1  97.27 ( 97.75)	Acc@3 100.00 ( 99.94)
Epoch: [21][ 40/412]	Loss 0.1331 (0.16)	InvT  24.04 ( 24.03)	Acc@1  97.27 ( 97.81)	Acc@3 100.00 ( 99.95)
Epoch: [21][ 60/412]	Loss 0.2155 (0.1624)	InvT  24.05 ( 24.03)	Acc@1  96.48 ( 97.77)	Acc@3 100.00 ( 99.94)
Epoch: [21][ 80/412]	Loss 0.1545 (0.1616)	InvT  24.05 ( 24.04)	Acc@1  96.88 ( 97.79)	Acc@3 100.00 ( 99.95)
Epoch: [21][100/412]	Loss 0.1719 (0.163)	InvT  24.06 ( 24.04)	Acc@1  96.88 ( 97.68)	Acc@3 100.00 ( 99.95)
Epoch: [21][120/412]	Loss 0.1546 (0.1617)	InvT  24.07 ( 24.05)	Acc@1  98.83 ( 97.69)	Acc@3  99.61 ( 99.95)
Epoch: [21][140/412]	Loss 0.2758 (0.1612)	InvT  24.08 ( 24.05)	Acc@1  94.53 ( 97.67)	Acc@3 100.00 ( 99.95)
Epoch: [21][160/412]	Loss 0.2238 (0.1626)	InvT  24.09 ( 24.05)	Acc@1  97.27 ( 97.65)	Acc@3 100.00 ( 99.94)
Epoch: [21][180/412]	Loss 0.1763 (0.1633)	InvT  24.10 ( 24.06)	Acc@1  97.66 ( 97.63)	Acc@3 100.00 ( 99.94)
Epoch: [21][200/412]	Loss 0.2031 (0.1642)	InvT  24.11 ( 24.06)	Acc@1  96.88 ( 97.60)	Acc@3 100.00 ( 99.94)
Epoch: [21][220/412]	Loss 0.1132 (0.1642)	InvT  24.12 ( 24.07)	Acc@1  98.44 ( 97.60)	Acc@3 100.00 ( 99.95)
Epoch: [21][240/412]	Loss 0.1299 (0.164)	InvT  24.13 ( 24.07)	Acc@1  97.27 ( 97.60)	Acc@3 100.00 ( 99.95)
Epoch: [21][260/412]	Loss 0.1281 (0.1638)	InvT  24.14 ( 24.08)	Acc@1  98.44 ( 97.59)	Acc@3 100.00 ( 99.95)
Epoch: [21][280/412]	Loss 0.2047 (0.1643)	InvT  24.15 ( 24.08)	Acc@1  96.09 ( 97.59)	Acc@3 100.00 ( 99.95)
Epoch: [21][300/412]	Loss 0.1843 (0.1639)	InvT  24.16 ( 24.09)	Acc@1  97.27 ( 97.59)	Acc@3  99.61 ( 99.94)
Epoch: [21][320/412]	Loss 0.1412 (0.1635)	InvT  24.17 ( 24.09)	Acc@1  98.44 ( 97.59)	Acc@3 100.00 ( 99.94)
Epoch: [21][340/412]	Loss 0.2151 (0.165)	InvT  24.17 ( 24.10)	Acc@1  97.66 ( 97.54)	Acc@3 100.00 ( 99.94)
Epoch: [21][360/412]	Loss 0.201 (0.1659)	InvT  24.18 ( 24.10)	Acc@1  96.88 ( 97.50)	Acc@3 100.00 ( 99.94)
Epoch: [21][380/412]	Loss 0.2077 (0.167)	InvT  24.19 ( 24.10)	Acc@1  97.27 ( 97.48)	Acc@3 100.00 ( 99.94)
Epoch: [21][400/412]	Loss 0.2392 (0.1673)	InvT  24.20 ( 24.11)	Acc@1  95.70 ( 97.47)	Acc@3 100.00 ( 99.94)
Learning rate: 2.364281172039329e-05
Epoch 21, valid metric: {"Acc@1": 37.9, "Acc@3": 52.5, "loss": 3.562}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch21.mdl
Epoch: [22][  0/412]	Loss 0.1859 (0.1859)	InvT  24.21 ( 24.21)	Acc@1  96.88 ( 96.88)	Acc@3 100.00 (100.00)
Epoch: [22][ 20/412]	Loss 0.1234 (0.1503)	InvT  24.22 ( 24.21)	Acc@1  97.66 ( 97.47)	Acc@3 100.00 ( 99.94)
Epoch: [22][ 40/412]	Loss 0.1313 (0.1475)	InvT  24.23 ( 24.22)	Acc@1  98.44 ( 97.71)	Acc@3 100.00 ( 99.95)
Epoch: [22][ 60/412]	Loss 0.219 (0.1467)	InvT  24.24 ( 24.22)	Acc@1  96.48 ( 97.72)	Acc@3 100.00 ( 99.97)
Epoch: [22][ 80/412]	Loss 0.1214 (0.1457)	InvT  24.24 ( 24.23)	Acc@1  98.44 ( 97.79)	Acc@3 100.00 ( 99.97)
Epoch: [22][100/412]	Loss 0.1268 (0.1465)	InvT  24.25 ( 24.23)	Acc@1  96.88 ( 97.71)	Acc@3 100.00 ( 99.97)
Epoch: [22][120/412]	Loss 0.1705 (0.1456)	InvT  24.26 ( 24.24)	Acc@1  98.05 ( 97.72)	Acc@3 100.00 ( 99.96)
Epoch: [22][140/412]	Loss 0.1342 (0.1452)	InvT  24.27 ( 24.24)	Acc@1  98.05 ( 97.78)	Acc@3 100.00 ( 99.96)
Epoch: [22][160/412]	Loss 0.1679 (0.1467)	InvT  24.28 ( 24.24)	Acc@1  97.27 ( 97.78)	Acc@3 100.00 ( 99.95)
Epoch: [22][180/412]	Loss 0.1183 (0.1458)	InvT  24.29 ( 24.25)	Acc@1  98.83 ( 97.78)	Acc@3 100.00 ( 99.95)
Epoch: [22][200/412]	Loss 0.1333 (0.1451)	InvT  24.30 ( 24.25)	Acc@1  98.44 ( 97.80)	Acc@3 100.00 ( 99.96)
Epoch: [22][220/412]	Loss 0.1709 (0.1459)	InvT  24.31 ( 24.26)	Acc@1  98.05 ( 97.78)	Acc@3 100.00 ( 99.95)
Epoch: [22][240/412]	Loss 0.09945 (0.1465)	InvT  24.32 ( 24.26)	Acc@1  98.44 ( 97.77)	Acc@3 100.00 ( 99.95)
Epoch: [22][260/412]	Loss 0.1165 (0.1479)	InvT  24.33 ( 24.27)	Acc@1  98.83 ( 97.76)	Acc@3  99.61 ( 99.95)
Epoch: [22][280/412]	Loss 0.2189 (0.1491)	InvT  24.34 ( 24.27)	Acc@1  96.09 ( 97.72)	Acc@3 100.00 ( 99.95)
Epoch: [22][300/412]	Loss 0.1762 (0.1501)	InvT  24.35 ( 24.28)	Acc@1  97.66 ( 97.69)	Acc@3 100.00 ( 99.95)
Epoch: [22][320/412]	Loss 0.1413 (0.1503)	InvT  24.35 ( 24.28)	Acc@1  97.66 ( 97.69)	Acc@3 100.00 ( 99.95)
Epoch: [22][340/412]	Loss 0.1747 (0.1512)	InvT  24.36 ( 24.29)	Acc@1  97.27 ( 97.67)	Acc@3 100.00 ( 99.95)
Epoch: [22][360/412]	Loss 0.1593 (0.1519)	InvT  24.37 ( 24.29)	Acc@1  98.05 ( 97.67)	Acc@3 100.00 ( 99.95)
Epoch: [22][380/412]	Loss 0.1681 (0.1526)	InvT  24.38 ( 24.29)	Acc@1  96.88 ( 97.66)	Acc@3 100.00 ( 99.95)
Epoch: [22][400/412]	Loss 0.2289 (0.1531)	InvT  24.39 ( 24.30)	Acc@1  96.48 ( 97.65)	Acc@3  99.61 ( 99.95)
Learning rate: 2.3340507753265177e-05
Epoch 22, valid metric: {"Acc@1": 36.5, "Acc@3": 51.9, "loss": 3.608}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch22.mdl
Epoch: [23][  0/412]	Loss 0.1764 (0.1764)	InvT  24.40 ( 24.40)	Acc@1  97.27 ( 97.27)	Acc@3  99.61 ( 99.61)
Epoch: [23][ 20/412]	Loss 0.1083 (0.139)	InvT  24.41 ( 24.40)	Acc@1  98.05 ( 97.77)	Acc@3 100.00 ( 99.96)
Epoch: [23][ 40/412]	Loss 0.1553 (0.1409)	InvT  24.42 ( 24.41)	Acc@1  98.05 ( 97.85)	Acc@3 100.00 ( 99.96)
Epoch: [23][ 60/412]	Loss 0.07477 (0.1423)	InvT  24.42 ( 24.41)	Acc@1  98.44 ( 97.82)	Acc@3 100.00 ( 99.97)
Epoch: [23][ 80/412]	Loss 0.1443 (0.139)	InvT  24.43 ( 24.42)	Acc@1  97.66 ( 97.83)	Acc@3 100.00 ( 99.97)
Epoch: [23][100/412]	Loss 0.09091 (0.1374)	InvT  24.44 ( 24.42)	Acc@1  98.44 ( 97.89)	Acc@3 100.00 ( 99.97)
Epoch: [23][120/412]	Loss 0.1679 (0.1384)	InvT  24.45 ( 24.42)	Acc@1  96.88 ( 97.88)	Acc@3 100.00 ( 99.97)
Epoch: [23][140/412]	Loss 0.119 (0.138)	InvT  24.46 ( 24.43)	Acc@1  98.83 ( 97.93)	Acc@3 100.00 ( 99.97)
Epoch: [23][160/412]	Loss 0.1413 (0.1396)	InvT  24.47 ( 24.43)	Acc@1  98.05 ( 97.89)	Acc@3 100.00 ( 99.96)
Epoch: [23][180/412]	Loss 0.1417 (0.1405)	InvT  24.48 ( 24.44)	Acc@1  97.66 ( 97.87)	Acc@3 100.00 ( 99.97)
Epoch: [23][200/412]	Loss 0.1366 (0.1394)	InvT  24.49 ( 24.44)	Acc@1  98.05 ( 97.89)	Acc@3 100.00 ( 99.97)
Epoch: [23][220/412]	Loss 0.1337 (0.1398)	InvT  24.50 ( 24.45)	Acc@1  97.27 ( 97.87)	Acc@3 100.00 ( 99.97)
Epoch: [23][240/412]	Loss 0.1299 (0.141)	InvT  24.51 ( 24.45)	Acc@1  98.05 ( 97.84)	Acc@3 100.00 ( 99.97)
Epoch: [23][260/412]	Loss 0.1102 (0.1421)	InvT  24.51 ( 24.46)	Acc@1  96.88 ( 97.80)	Acc@3 100.00 ( 99.97)
Epoch: [23][280/412]	Loss 0.2022 (0.143)	InvT  24.52 ( 24.46)	Acc@1  94.92 ( 97.79)	Acc@3 100.00 ( 99.97)
Epoch: [23][300/412]	Loss 0.2503 (0.1428)	InvT  24.53 ( 24.46)	Acc@1  97.27 ( 97.80)	Acc@3 100.00 ( 99.97)
Epoch: [23][320/412]	Loss 0.1256 (0.1434)	InvT  24.54 ( 24.47)	Acc@1  98.05 ( 97.78)	Acc@3 100.00 ( 99.97)
Epoch: [23][340/412]	Loss 0.1258 (0.143)	InvT  24.55 ( 24.47)	Acc@1  99.22 ( 97.80)	Acc@3 100.00 ( 99.97)
Epoch: [23][360/412]	Loss 0.1329 (0.1429)	InvT  24.56 ( 24.48)	Acc@1  97.27 ( 97.80)	Acc@3 100.00 ( 99.97)
Epoch: [23][380/412]	Loss 0.1155 (0.1435)	InvT  24.57 ( 24.48)	Acc@1  98.83 ( 97.80)	Acc@3 100.00 ( 99.97)
Epoch: [23][400/412]	Loss 0.1141 (0.1438)	InvT  24.58 ( 24.49)	Acc@1  97.66 ( 97.81)	Acc@3 100.00 ( 99.97)
Learning rate: 2.3038203786137063e-05
Epoch 23, valid metric: {"Acc@1": 36.9, "Acc@3": 50.6, "loss": 3.629}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch23.mdl
Epoch: [24][  0/412]	Loss 0.1741 (0.1741)	InvT  24.59 ( 24.59)	Acc@1  97.66 ( 97.66)	Acc@3 100.00 (100.00)
Epoch: [24][ 20/412]	Loss 0.08695 (0.1315)	InvT  24.60 ( 24.59)	Acc@1  98.05 ( 98.07)	Acc@3 100.00 ( 99.94)
Epoch: [24][ 40/412]	Loss 0.1024 (0.1286)	InvT  24.60 ( 24.60)	Acc@1  98.44 ( 98.05)	Acc@3 100.00 ( 99.96)
Epoch: [24][ 60/412]	Loss 0.1245 (0.1237)	InvT  24.61 ( 24.60)	Acc@1  98.44 ( 98.21)	Acc@3 100.00 ( 99.96)
Epoch: [24][ 80/412]	Loss 0.1368 (0.1262)	InvT  24.62 ( 24.60)	Acc@1  97.66 ( 98.11)	Acc@3 100.00 ( 99.96)
Epoch: [24][100/412]	Loss 0.1253 (0.1285)	InvT  24.63 ( 24.61)	Acc@1  98.05 ( 98.03)	Acc@3 100.00 ( 99.96)
Epoch: [24][120/412]	Loss 0.09919 (0.128)	InvT  24.64 ( 24.61)	Acc@1  98.44 ( 98.05)	Acc@3 100.00 ( 99.96)
Epoch: [24][140/412]	Loss 0.07567 (0.1279)	InvT  24.65 ( 24.62)	Acc@1  99.22 ( 98.05)	Acc@3 100.00 ( 99.97)
Epoch: [24][160/412]	Loss 0.117 (0.1282)	InvT  24.66 ( 24.62)	Acc@1  97.27 ( 98.05)	Acc@3 100.00 ( 99.97)
Epoch: [24][180/412]	Loss 0.1084 (0.1291)	InvT  24.67 ( 24.63)	Acc@1  98.83 ( 98.04)	Acc@3 100.00 ( 99.96)
Epoch: [24][200/412]	Loss 0.1241 (0.131)	InvT  24.67 ( 24.63)	Acc@1  98.05 ( 98.00)	Acc@3 100.00 ( 99.96)
Epoch: [24][220/412]	Loss 0.1726 (0.1319)	InvT  24.68 ( 24.63)	Acc@1  98.05 ( 98.00)	Acc@3 100.00 ( 99.96)
Epoch: [24][240/412]	Loss 0.1724 (0.1319)	InvT  24.69 ( 24.64)	Acc@1  97.66 ( 97.97)	Acc@3 100.00 ( 99.96)
Epoch: [24][260/412]	Loss 0.175 (0.1326)	InvT  24.70 ( 24.64)	Acc@1  96.88 ( 97.96)	Acc@3 100.00 ( 99.96)
Epoch: [24][280/412]	Loss 0.163 (0.1327)	InvT  24.71 ( 24.65)	Acc@1  97.27 ( 97.97)	Acc@3 100.00 ( 99.96)
Epoch: [24][300/412]	Loss 0.1517 (0.1331)	InvT  24.72 ( 24.65)	Acc@1  98.44 ( 97.97)	Acc@3 100.00 ( 99.96)
Epoch: [24][320/412]	Loss 0.09123 (0.1339)	InvT  24.73 ( 24.66)	Acc@1  98.83 ( 97.94)	Acc@3 100.00 ( 99.96)
Epoch: [24][340/412]	Loss 0.1262 (0.1345)	InvT  24.74 ( 24.66)	Acc@1  98.05 ( 97.92)	Acc@3 100.00 ( 99.96)
Epoch: [24][360/412]	Loss 0.1423 (0.1343)	InvT  24.75 ( 24.67)	Acc@1  98.05 ( 97.93)	Acc@3 100.00 ( 99.97)
Epoch: [24][380/412]	Loss 0.1989 (0.1349)	InvT  24.76 ( 24.67)	Acc@1  95.31 ( 97.92)	Acc@3 100.00 ( 99.97)
Epoch: [24][400/412]	Loss 0.129 (0.135)	InvT  24.77 ( 24.68)	Acc@1  98.83 ( 97.92)	Acc@3  99.61 ( 99.96)
Learning rate: 2.273589981900895e-05
Epoch 24, valid metric: {"Acc@1": 37.3, "Acc@3": 51.1, "loss": 3.634}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch24.mdl
Epoch: [25][  0/412]	Loss 0.1639 (0.1639)	InvT  24.78 ( 24.78)	Acc@1  96.88 ( 96.88)	Acc@3  99.61 ( 99.61)
Epoch: [25][ 20/412]	Loss 0.1086 (0.1319)	InvT  24.79 ( 24.78)	Acc@1  97.66 ( 97.79)	Acc@3 100.00 ( 99.94)
Epoch: [25][ 40/412]	Loss 0.1298 (0.1296)	InvT  24.79 ( 24.79)	Acc@1  98.44 ( 97.99)	Acc@3 100.00 ( 99.95)
Epoch: [25][ 60/412]	Loss 0.08912 (0.1269)	InvT  24.80 ( 24.79)	Acc@1  99.61 ( 98.04)	Acc@3 100.00 ( 99.96)
Epoch: [25][ 80/412]	Loss 0.1589 (0.1256)	InvT  24.81 ( 24.79)	Acc@1  97.27 ( 98.10)	Acc@3 100.00 ( 99.97)
Epoch: [25][100/412]	Loss 0.2383 (0.1266)	InvT  24.82 ( 24.80)	Acc@1  95.70 ( 98.02)	Acc@3 100.00 ( 99.97)
Epoch: [25][120/412]	Loss 0.1432 (0.125)	InvT  24.83 ( 24.80)	Acc@1  97.66 ( 98.07)	Acc@3 100.00 ( 99.97)
Epoch: [25][140/412]	Loss 0.1351 (0.125)	InvT  24.84 ( 24.81)	Acc@1  98.44 ( 98.11)	Acc@3 100.00 ( 99.98)
Epoch: [25][160/412]	Loss 0.1279 (0.1251)	InvT  24.85 ( 24.81)	Acc@1  97.66 ( 98.12)	Acc@3 100.00 ( 99.98)
Epoch: [25][180/412]	Loss 0.1301 (0.1252)	InvT  24.86 ( 24.82)	Acc@1  98.83 ( 98.10)	Acc@3 100.00 ( 99.98)
Epoch: [25][200/412]	Loss 0.08945 (0.1255)	InvT  24.87 ( 24.82)	Acc@1  98.05 ( 98.08)	Acc@3 100.00 ( 99.98)
Epoch: [25][220/412]	Loss 0.0967 (0.1261)	InvT  24.88 ( 24.83)	Acc@1  98.05 ( 98.06)	Acc@3 100.00 ( 99.98)
Epoch: [25][240/412]	Loss 0.1309 (0.1254)	InvT  24.89 ( 24.83)	Acc@1  99.22 ( 98.08)	Acc@3 100.00 ( 99.98)
Epoch: [25][260/412]	Loss 0.1341 (0.126)	InvT  24.90 ( 24.84)	Acc@1  97.66 ( 98.07)	Acc@3 100.00 ( 99.98)
Epoch: [25][280/412]	Loss 0.09979 (0.1256)	InvT  24.90 ( 24.84)	Acc@1  98.83 ( 98.10)	Acc@3 100.00 ( 99.98)
Epoch: [25][300/412]	Loss 0.1818 (0.1264)	InvT  24.91 ( 24.84)	Acc@1  95.70 ( 98.08)	Acc@3 100.00 ( 99.98)
Epoch: [25][320/412]	Loss 0.1261 (0.1262)	InvT  24.92 ( 24.85)	Acc@1  97.27 ( 98.08)	Acc@3  99.61 ( 99.98)
Epoch: [25][340/412]	Loss 0.09902 (0.1256)	InvT  24.93 ( 24.85)	Acc@1  98.05 ( 98.09)	Acc@3 100.00 ( 99.98)
Epoch: [25][360/412]	Loss 0.1289 (0.1257)	InvT  24.94 ( 24.86)	Acc@1  97.66 ( 98.09)	Acc@3 100.00 ( 99.98)
Epoch: [25][380/412]	Loss 0.1685 (0.1258)	InvT  24.95 ( 24.86)	Acc@1  96.88 ( 98.09)	Acc@3 100.00 ( 99.97)
Epoch: [25][400/412]	Loss 0.1112 (0.1256)	InvT  24.96 ( 24.87)	Acc@1  98.44 ( 98.08)	Acc@3 100.00 ( 99.98)
Learning rate: 2.243359585188084e-05
Epoch 25, valid metric: {"Acc@1": 38.4, "Acc@3": 52.4, "loss": 3.573}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch25.mdl
Epoch: [26][  0/412]	Loss 0.1607 (0.1607)	InvT  24.97 ( 24.97)	Acc@1  97.66 ( 97.66)	Acc@3 100.00 (100.00)
Epoch: [26][ 20/412]	Loss 0.09419 (0.117)	InvT  24.98 ( 24.97)	Acc@1  98.44 ( 98.34)	Acc@3 100.00 ( 99.96)
Epoch: [26][ 40/412]	Loss 0.1095 (0.1146)	InvT  24.99 ( 24.98)	Acc@1  99.22 ( 98.40)	Acc@3 100.00 ( 99.98)
Epoch: [26][ 60/412]	Loss 0.09792 (0.1143)	InvT  24.99 ( 24.98)	Acc@1  98.83 ( 98.37)	Acc@3 100.00 ( 99.99)
Epoch: [26][ 80/412]	Loss 0.1359 (0.1142)	InvT  25.00 ( 24.99)	Acc@1  97.27 ( 98.37)	Acc@3 100.00 ( 99.99)
Epoch: [26][100/412]	Loss 0.1147 (0.1142)	InvT  25.01 ( 24.99)	Acc@1  99.22 ( 98.29)	Acc@3 100.00 ( 99.98)
Epoch: [26][120/412]	Loss 0.07629 (0.1151)	InvT  25.02 ( 24.99)	Acc@1  98.44 ( 98.25)	Acc@3 100.00 ( 99.98)
Epoch: [26][140/412]	Loss 0.07653 (0.1158)	InvT  25.03 ( 25.00)	Acc@1  98.83 ( 98.24)	Acc@3 100.00 ( 99.98)
Epoch: [26][160/412]	Loss 0.0747 (0.1159)	InvT  25.04 ( 25.00)	Acc@1  98.83 ( 98.22)	Acc@3 100.00 ( 99.98)
Epoch: [26][180/412]	Loss 0.1003 (0.1161)	InvT  25.05 ( 25.01)	Acc@1  98.44 ( 98.21)	Acc@3 100.00 ( 99.98)
Epoch: [26][200/412]	Loss 0.07102 (0.1158)	InvT  25.06 ( 25.01)	Acc@1 100.00 ( 98.20)	Acc@3 100.00 ( 99.98)
Epoch: [26][220/412]	Loss 0.09184 (0.1156)	InvT  25.07 ( 25.02)	Acc@1  98.83 ( 98.21)	Acc@3 100.00 ( 99.98)
Epoch: [26][240/412]	Loss 0.148 (0.1169)	InvT  25.07 ( 25.02)	Acc@1  98.05 ( 98.17)	Acc@3 100.00 ( 99.98)
Epoch: [26][260/412]	Loss 0.1174 (0.1169)	InvT  25.08 ( 25.03)	Acc@1  99.22 ( 98.17)	Acc@3 100.00 ( 99.98)
Epoch: [26][280/412]	Loss 0.1052 (0.1168)	InvT  25.09 ( 25.03)	Acc@1  98.44 ( 98.19)	Acc@3 100.00 ( 99.98)
Epoch: [26][300/412]	Loss 0.08717 (0.117)	InvT  25.10 ( 25.03)	Acc@1  98.83 ( 98.18)	Acc@3 100.00 ( 99.97)
Epoch: [26][320/412]	Loss 0.09807 (0.1177)	InvT  25.11 ( 25.04)	Acc@1  97.66 ( 98.17)	Acc@3 100.00 ( 99.97)
Epoch: [26][340/412]	Loss 0.1849 (0.118)	InvT  25.12 ( 25.04)	Acc@1  96.88 ( 98.17)	Acc@3 100.00 ( 99.97)
Epoch: [26][360/412]	Loss 0.1364 (0.1183)	InvT  25.13 ( 25.05)	Acc@1  98.05 ( 98.17)	Acc@3  99.61 ( 99.97)
Epoch: [26][380/412]	Loss 0.1572 (0.1184)	InvT  25.14 ( 25.05)	Acc@1  96.88 ( 98.17)	Acc@3 100.00 ( 99.97)
Epoch: [26][400/412]	Loss 0.09078 (0.1187)	InvT  25.15 ( 25.06)	Acc@1  98.83 ( 98.15)	Acc@3 100.00 ( 99.97)
Learning rate: 2.213129188475273e-05
Epoch 26, valid metric: {"Acc@1": 37.6, "Acc@3": 52.3, "loss": 3.66}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch26.mdl
Epoch: [27][  0/412]	Loss 0.121 (0.121)	InvT  25.15 ( 25.15)	Acc@1  98.05 ( 98.05)	Acc@3 100.00 (100.00)
Epoch: [27][ 20/412]	Loss 0.1102 (0.09945)	InvT  25.16 ( 25.16)	Acc@1  98.05 ( 98.33)	Acc@3 100.00 ( 99.96)
Epoch: [27][ 40/412]	Loss 0.1106 (0.09934)	InvT  25.17 ( 25.16)	Acc@1  98.44 ( 98.43)	Acc@3 100.00 ( 99.98)
Epoch: [27][ 60/412]	Loss 0.0849 (0.1034)	InvT  25.18 ( 25.17)	Acc@1  98.83 ( 98.46)	Acc@3 100.00 ( 99.98)
Epoch: [27][ 80/412]	Loss 0.09092 (0.1049)	InvT  25.19 ( 25.17)	Acc@1  99.22 ( 98.41)	Acc@3 100.00 ( 99.98)
Epoch: [27][100/412]	Loss 0.08087 (0.1064)	InvT  25.20 ( 25.18)	Acc@1  99.22 ( 98.38)	Acc@3 100.00 ( 99.97)
Epoch: [27][120/412]	Loss 0.1233 (0.1083)	InvT  25.21 ( 25.18)	Acc@1  98.83 ( 98.37)	Acc@3 100.00 ( 99.97)
Epoch: [27][140/412]	Loss 0.09497 (0.1075)	InvT  25.22 ( 25.18)	Acc@1  98.83 ( 98.39)	Acc@3 100.00 ( 99.98)
Epoch: [27][160/412]	Loss 0.1174 (0.1094)	InvT  25.22 ( 25.19)	Acc@1  99.22 ( 98.36)	Acc@3 100.00 ( 99.97)
Epoch: [27][180/412]	Loss 0.1268 (0.1092)	InvT  25.23 ( 25.19)	Acc@1  97.66 ( 98.36)	Acc@3 100.00 ( 99.97)
Epoch: [27][200/412]	Loss 0.1083 (0.1104)	InvT  25.24 ( 25.20)	Acc@1  98.05 ( 98.33)	Acc@3 100.00 ( 99.97)
Epoch: [27][220/412]	Loss 0.09022 (0.1104)	InvT  25.25 ( 25.20)	Acc@1  98.44 ( 98.32)	Acc@3 100.00 ( 99.97)
Epoch: [27][240/412]	Loss 0.08544 (0.111)	InvT  25.26 ( 25.21)	Acc@1  99.61 ( 98.30)	Acc@3 100.00 ( 99.97)
Epoch: [27][260/412]	Loss 0.1111 (0.1109)	InvT  25.27 ( 25.21)	Acc@1  97.66 ( 98.29)	Acc@3 100.00 ( 99.98)
Epoch: [27][280/412]	Loss 0.1087 (0.1113)	InvT  25.28 ( 25.22)	Acc@1  97.66 ( 98.28)	Acc@3 100.00 ( 99.98)
Epoch: [27][300/412]	Loss 0.09209 (0.1112)	InvT  25.29 ( 25.22)	Acc@1  98.44 ( 98.28)	Acc@3 100.00 ( 99.98)
Epoch: [27][320/412]	Loss 0.1267 (0.1114)	InvT  25.30 ( 25.22)	Acc@1  98.05 ( 98.28)	Acc@3 100.00 ( 99.97)
Epoch: [27][340/412]	Loss 0.09814 (0.1115)	InvT  25.31 ( 25.23)	Acc@1  98.83 ( 98.27)	Acc@3 100.00 ( 99.98)
Epoch: [27][360/412]	Loss 0.1195 (0.1116)	InvT  25.32 ( 25.23)	Acc@1  98.44 ( 98.29)	Acc@3 100.00 ( 99.98)
Epoch: [27][380/412]	Loss 0.1632 (0.1124)	InvT  25.33 ( 25.24)	Acc@1  96.88 ( 98.26)	Acc@3 100.00 ( 99.98)
Epoch: [27][400/412]	Loss 0.09122 (0.1123)	InvT  25.34 ( 25.24)	Acc@1  98.44 ( 98.26)	Acc@3 100.00 ( 99.98)
Learning rate: 2.1828987917624614e-05
Epoch 27, valid metric: {"Acc@1": 38.3, "Acc@3": 52.2, "loss": 3.616}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch27.mdl
Epoch: [28][  0/412]	Loss 0.07902 (0.07902)	InvT  25.34 ( 25.34)	Acc@1  98.83 ( 98.83)	Acc@3 100.00 (100.00)
Epoch: [28][ 20/412]	Loss 0.09452 (0.1009)	InvT  25.35 ( 25.35)	Acc@1  97.27 ( 98.07)	Acc@3 100.00 (100.00)
Epoch: [28][ 40/412]	Loss 0.07895 (0.093)	InvT  25.36 ( 25.35)	Acc@1  98.83 ( 98.35)	Acc@3 100.00 (100.00)
Epoch: [28][ 60/412]	Loss 0.1874 (0.09687)	InvT  25.37 ( 25.36)	Acc@1  97.27 ( 98.35)	Acc@3 100.00 (100.00)
Epoch: [28][ 80/412]	Loss 0.09167 (0.09687)	InvT  25.38 ( 25.36)	Acc@1  98.05 ( 98.38)	Acc@3 100.00 (100.00)
Epoch: [28][100/412]	Loss 0.08913 (0.09848)	InvT  25.39 ( 25.36)	Acc@1  98.44 ( 98.34)	Acc@3 100.00 (100.00)
Epoch: [28][120/412]	Loss 0.1106 (0.09778)	InvT  25.39 ( 25.37)	Acc@1  96.88 ( 98.38)	Acc@3 100.00 ( 99.99)
Epoch: [28][140/412]	Loss 0.05698 (0.09847)	InvT  25.40 ( 25.37)	Acc@1  99.22 ( 98.36)	Acc@3 100.00 ( 99.99)
Epoch: [28][160/412]	Loss 0.1182 (0.0993)	InvT  25.41 ( 25.38)	Acc@1  98.05 ( 98.36)	Acc@3 100.00 ( 99.99)
Epoch: [28][180/412]	Loss 0.142 (0.09976)	InvT  25.42 ( 25.38)	Acc@1  97.66 ( 98.38)	Acc@3 100.00 ( 99.99)
Epoch: [28][200/412]	Loss 0.08817 (0.1005)	InvT  25.43 ( 25.39)	Acc@1  98.05 ( 98.36)	Acc@3 100.00 ( 99.99)
Epoch: [28][220/412]	Loss 0.1208 (0.1009)	InvT  25.44 ( 25.39)	Acc@1  97.27 ( 98.35)	Acc@3 100.00 ( 99.99)
Epoch: [28][240/412]	Loss 0.1182 (0.1012)	InvT  25.45 ( 25.39)	Acc@1  98.44 ( 98.35)	Acc@3 100.00 ( 99.99)
Epoch: [28][260/412]	Loss 0.09084 (0.101)	InvT  25.46 ( 25.40)	Acc@1  98.83 ( 98.37)	Acc@3 100.00 ( 99.98)
Epoch: [28][280/412]	Loss 0.06162 (0.1019)	InvT  25.47 ( 25.40)	Acc@1  99.22 ( 98.37)	Acc@3 100.00 ( 99.98)
Epoch: [28][300/412]	Loss 0.138 (0.1016)	InvT  25.48 ( 25.41)	Acc@1  97.66 ( 98.38)	Acc@3 100.00 ( 99.98)
Epoch: [28][320/412]	Loss 0.07685 (0.1017)	InvT  25.49 ( 25.41)	Acc@1  98.83 ( 98.39)	Acc@3 100.00 ( 99.98)
Epoch: [28][340/412]	Loss 0.1598 (0.1017)	InvT  25.49 ( 25.42)	Acc@1  97.66 ( 98.38)	Acc@3 100.00 ( 99.98)
Epoch: [28][360/412]	Loss 0.1248 (0.1025)	InvT  25.50 ( 25.42)	Acc@1  98.44 ( 98.37)	Acc@3  99.61 ( 99.97)
Epoch: [28][380/412]	Loss 0.1236 (0.1026)	InvT  25.51 ( 25.43)	Acc@1  98.83 ( 98.38)	Acc@3 100.00 ( 99.98)
Epoch: [28][400/412]	Loss 0.08209 (0.1031)	InvT  25.52 ( 25.43)	Acc@1  98.44 ( 98.38)	Acc@3 100.00 ( 99.98)
Learning rate: 2.1526683950496502e-05
Epoch 28, valid metric: {"Acc@1": 37.6, "Acc@3": 52.0, "loss": 3.693}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch28.mdl
Epoch: [29][  0/412]	Loss 0.09881 (0.09881)	InvT  25.53 ( 25.53)	Acc@1  98.05 ( 98.05)	Acc@3 100.00 (100.00)
Epoch: [29][ 20/412]	Loss 0.1238 (0.09648)	InvT  25.54 ( 25.53)	Acc@1  98.83 ( 98.18)	Acc@3 100.00 (100.00)
Epoch: [29][ 40/412]	Loss 0.07387 (0.08957)	InvT  25.55 ( 25.54)	Acc@1  98.83 ( 98.63)	Acc@3 100.00 (100.00)
Epoch: [29][ 60/412]	Loss 0.1181 (0.09082)	InvT  25.56 ( 25.54)	Acc@1  98.44 ( 98.62)	Acc@3 100.00 ( 99.97)
Epoch: [29][ 80/412]	Loss 0.129 (0.09182)	InvT  25.56 ( 25.55)	Acc@1  97.66 ( 98.60)	Acc@3 100.00 ( 99.98)
Epoch: [29][100/412]	Loss 0.1012 (0.09268)	InvT  25.57 ( 25.55)	Acc@1  97.66 ( 98.59)	Acc@3 100.00 ( 99.98)
Epoch: [29][120/412]	Loss 0.07818 (0.09136)	InvT  25.58 ( 25.56)	Acc@1  99.22 ( 98.65)	Acc@3 100.00 ( 99.99)
Epoch: [29][140/412]	Loss 0.1389 (0.09195)	InvT  25.59 ( 25.56)	Acc@1  98.05 ( 98.63)	Acc@3 100.00 ( 99.99)
Epoch: [29][160/412]	Loss 0.1254 (0.09231)	InvT  25.60 ( 25.56)	Acc@1  98.05 ( 98.60)	Acc@3 100.00 ( 99.99)
Epoch: [29][180/412]	Loss 0.138 (0.09286)	InvT  25.61 ( 25.57)	Acc@1  97.27 ( 98.58)	Acc@3 100.00 ( 99.99)
Epoch: [29][200/412]	Loss 0.08411 (0.09308)	InvT  25.62 ( 25.57)	Acc@1  98.05 ( 98.57)	Acc@3 100.00 ( 99.99)
Epoch: [29][220/412]	Loss 0.1164 (0.09343)	InvT  25.63 ( 25.58)	Acc@1  98.05 ( 98.56)	Acc@3 100.00 ( 99.99)
Epoch: [29][240/412]	Loss 0.1047 (0.09339)	InvT  25.64 ( 25.58)	Acc@1  98.05 ( 98.55)	Acc@3 100.00 ( 99.99)
Epoch: [29][260/412]	Loss 0.1545 (0.09451)	InvT  25.64 ( 25.59)	Acc@1  97.27 ( 98.54)	Acc@3 100.00 ( 99.99)
Epoch: [29][280/412]	Loss 0.1442 (0.09567)	InvT  25.65 ( 25.59)	Acc@1  97.66 ( 98.51)	Acc@3 100.00 ( 99.99)
Epoch: [29][300/412]	Loss 0.1212 (0.09594)	InvT  25.66 ( 25.60)	Acc@1  98.83 ( 98.50)	Acc@3 100.00 ( 99.99)
Epoch: [29][320/412]	Loss 0.1754 (0.0967)	InvT  25.67 ( 25.60)	Acc@1  97.66 ( 98.49)	Acc@3  99.61 ( 99.99)
Epoch: [29][340/412]	Loss 0.1014 (0.09713)	InvT  25.68 ( 25.60)	Acc@1  98.05 ( 98.49)	Acc@3 100.00 ( 99.99)
Epoch: [29][360/412]	Loss 0.1893 (0.09802)	InvT  25.69 ( 25.61)	Acc@1  96.48 ( 98.48)	Acc@3 100.00 ( 99.99)
Epoch: [29][380/412]	Loss 0.2079 (0.09854)	InvT  25.70 ( 25.61)	Acc@1  95.70 ( 98.47)	Acc@3 100.00 ( 99.99)
Epoch: [29][400/412]	Loss 0.1018 (0.09839)	InvT  25.71 ( 25.62)	Acc@1  99.22 ( 98.48)	Acc@3 100.00 ( 99.99)
Learning rate: 2.122437998336839e-05
Epoch 29, valid metric: {"Acc@1": 38.5, "Acc@3": 52.3, "loss": 3.634}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch29.mdl
Epoch: [30][  0/412]	Loss 0.101 (0.101)	InvT  25.71 ( 25.71)	Acc@1  98.83 ( 98.83)	Acc@3 100.00 (100.00)
Epoch: [30][ 20/412]	Loss 0.07189 (0.08499)	InvT  25.72 ( 25.72)	Acc@1 100.00 ( 98.96)	Acc@3 100.00 ( 99.98)
Epoch: [30][ 40/412]	Loss 0.06844 (0.08514)	InvT  25.73 ( 25.72)	Acc@1  98.44 ( 98.89)	Acc@3 100.00 ( 99.98)
Epoch: [30][ 60/412]	Loss 0.07315 (0.09157)	InvT  25.74 ( 25.73)	Acc@1  98.05 ( 98.67)	Acc@3 100.00 ( 99.99)
Epoch: [30][ 80/412]	Loss 0.09678 (0.09165)	InvT  25.75 ( 25.73)	Acc@1  98.44 ( 98.65)	Acc@3 100.00 ( 99.99)
Epoch: [30][100/412]	Loss 0.09171 (0.09346)	InvT  25.76 ( 25.74)	Acc@1  98.05 ( 98.55)	Acc@3 100.00 ( 99.98)
Epoch: [30][120/412]	Loss 0.07418 (0.09288)	InvT  25.77 ( 25.74)	Acc@1  99.22 ( 98.59)	Acc@3 100.00 ( 99.98)
Epoch: [30][140/412]	Loss 0.1306 (0.09371)	InvT  25.78 ( 25.75)	Acc@1  97.66 ( 98.56)	Acc@3 100.00 ( 99.98)
Epoch: [30][160/412]	Loss 0.08767 (0.09302)	InvT  25.79 ( 25.75)	Acc@1  98.44 ( 98.60)	Acc@3 100.00 ( 99.98)
Epoch: [30][180/412]	Loss 0.08468 (0.09364)	InvT  25.79 ( 25.75)	Acc@1  98.83 ( 98.59)	Acc@3 100.00 ( 99.98)
Epoch: [30][200/412]	Loss 0.07434 (0.09246)	InvT  25.80 ( 25.76)	Acc@1  98.83 ( 98.61)	Acc@3 100.00 ( 99.98)
Epoch: [30][220/412]	Loss 0.1572 (0.09276)	InvT  25.81 ( 25.76)	Acc@1  96.48 ( 98.61)	Acc@3  99.61 ( 99.98)
Epoch: [30][240/412]	Loss 0.0674 (0.09291)	InvT  25.82 ( 25.77)	Acc@1  99.22 ( 98.60)	Acc@3 100.00 ( 99.98)
Epoch: [30][260/412]	Loss 0.07604 (0.09281)	InvT  25.83 ( 25.77)	Acc@1  99.22 ( 98.59)	Acc@3 100.00 ( 99.98)
Epoch: [30][280/412]	Loss 0.1278 (0.09323)	InvT  25.84 ( 25.78)	Acc@1  98.05 ( 98.58)	Acc@3 100.00 ( 99.98)
Epoch: [30][300/412]	Loss 0.0613 (0.09268)	InvT  25.85 ( 25.78)	Acc@1  98.83 ( 98.60)	Acc@3 100.00 ( 99.98)
Epoch: [30][320/412]	Loss 0.1412 (0.09253)	InvT  25.86 ( 25.79)	Acc@1  98.05 ( 98.59)	Acc@3 100.00 ( 99.98)
Epoch: [30][340/412]	Loss 0.1017 (0.0929)	InvT  25.87 ( 25.79)	Acc@1  98.83 ( 98.59)	Acc@3 100.00 ( 99.98)
Epoch: [30][360/412]	Loss 0.09077 (0.09312)	InvT  25.88 ( 25.80)	Acc@1  98.83 ( 98.59)	Acc@3 100.00 ( 99.98)
Epoch: [30][380/412]	Loss 0.123 (0.09394)	InvT  25.89 ( 25.80)	Acc@1  98.05 ( 98.58)	Acc@3 100.00 ( 99.98)
Epoch: [30][400/412]	Loss 0.05903 (0.09439)	InvT  25.90 ( 25.81)	Acc@1  99.22 ( 98.57)	Acc@3 100.00 ( 99.98)
Learning rate: 2.0922076016240276e-05
Epoch 30, valid metric: {"Acc@1": 38.9, "Acc@3": 52.7, "loss": 3.695}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch30.mdl
Epoch: [31][  0/412]	Loss 0.1551 (0.1551)	InvT  25.91 ( 25.91)	Acc@1  96.88 ( 96.88)	Acc@3 100.00 (100.00)
Epoch: [31][ 20/412]	Loss 0.05597 (0.08462)	InvT  25.91 ( 25.91)	Acc@1  99.22 ( 98.70)	Acc@3 100.00 (100.00)
Epoch: [31][ 40/412]	Loss 0.1262 (0.08563)	InvT  25.92 ( 25.91)	Acc@1  98.05 ( 98.68)	Acc@3  99.61 ( 99.98)
Epoch: [31][ 60/412]	Loss 0.06336 (0.0851)	InvT  25.93 ( 25.92)	Acc@1  98.83 ( 98.67)	Acc@3 100.00 ( 99.99)
Epoch: [31][ 80/412]	Loss 0.05689 (0.08654)	InvT  25.94 ( 25.92)	Acc@1  99.61 ( 98.60)	Acc@3 100.00 ( 99.99)
Epoch: [31][100/412]	Loss 0.08507 (0.08663)	InvT  25.95 ( 25.93)	Acc@1  98.83 ( 98.61)	Acc@3 100.00 ( 99.99)
Epoch: [31][120/412]	Loss 0.07673 (0.08741)	InvT  25.96 ( 25.93)	Acc@1  99.22 ( 98.59)	Acc@3 100.00 ( 99.99)
Epoch: [31][140/412]	Loss 0.1222 (0.08849)	InvT  25.97 ( 25.94)	Acc@1  98.05 ( 98.56)	Acc@3 100.00 ( 99.99)
Epoch: [31][160/412]	Loss 0.08253 (0.08841)	InvT  25.98 ( 25.94)	Acc@1  99.22 ( 98.57)	Acc@3 100.00 ( 99.99)
Epoch: [31][180/412]	Loss 0.1475 (0.08965)	InvT  25.99 ( 25.95)	Acc@1  98.05 ( 98.56)	Acc@3 100.00 ( 99.98)
Epoch: [31][200/412]	Loss 0.07879 (0.08798)	InvT  26.00 ( 25.95)	Acc@1  98.83 ( 98.59)	Acc@3 100.00 ( 99.99)
Epoch: [31][220/412]	Loss 0.06929 (0.08795)	InvT  26.00 ( 25.95)	Acc@1  99.22 ( 98.61)	Acc@3 100.00 ( 99.99)
Epoch: [31][240/412]	Loss 0.1157 (0.08908)	InvT  26.01 ( 25.96)	Acc@1  98.44 ( 98.61)	Acc@3 100.00 ( 99.99)
Epoch: [31][260/412]	Loss 0.05421 (0.08936)	InvT  26.02 ( 25.96)	Acc@1  99.61 ( 98.60)	Acc@3 100.00 ( 99.99)
Epoch: [31][280/412]	Loss 0.07865 (0.08957)	InvT  26.03 ( 25.97)	Acc@1  98.83 ( 98.59)	Acc@3 100.00 ( 99.99)
Epoch: [31][300/412]	Loss 0.1239 (0.08968)	InvT  26.04 ( 25.97)	Acc@1  98.44 ( 98.59)	Acc@3 100.00 ( 99.98)
Epoch: [31][320/412]	Loss 0.1025 (0.08939)	InvT  26.05 ( 25.98)	Acc@1  98.05 ( 98.60)	Acc@3 100.00 ( 99.98)
Epoch: [31][340/412]	Loss 0.1471 (0.0894)	InvT  26.06 ( 25.98)	Acc@1  97.27 ( 98.61)	Acc@3 100.00 ( 99.98)
Epoch: [31][360/412]	Loss 0.1223 (0.08958)	InvT  26.07 ( 25.99)	Acc@1  98.05 ( 98.61)	Acc@3  99.61 ( 99.98)
Epoch: [31][380/412]	Loss 0.057 (0.08929)	InvT  26.08 ( 25.99)	Acc@1  99.61 ( 98.62)	Acc@3 100.00 ( 99.98)
Epoch: [31][400/412]	Loss 0.06635 (0.08933)	InvT  26.09 ( 25.99)	Acc@1  99.22 ( 98.62)	Acc@3 100.00 ( 99.98)
Learning rate: 2.0619772049112165e-05
Epoch 31, valid metric: {"Acc@1": 38.2, "Acc@3": 50.2, "loss": 3.725}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch31.mdl
Epoch: [32][  0/412]	Loss 0.08328 (0.08328)	InvT  26.09 ( 26.09)	Acc@1  98.83 ( 98.83)	Acc@3 100.00 (100.00)
Epoch: [32][ 20/412]	Loss 0.09705 (0.08179)	InvT  26.10 ( 26.10)	Acc@1  99.22 ( 98.88)	Acc@3 100.00 (100.00)
Epoch: [32][ 40/412]	Loss 0.1008 (0.08331)	InvT  26.11 ( 26.10)	Acc@1  98.05 ( 98.87)	Acc@3 100.00 (100.00)
Epoch: [32][ 60/412]	Loss 0.1174 (0.08196)	InvT  26.12 ( 26.11)	Acc@1  98.05 ( 98.85)	Acc@3 100.00 ( 99.99)
Epoch: [32][ 80/412]	Loss 0.1117 (0.08311)	InvT  26.13 ( 26.11)	Acc@1  98.44 ( 98.74)	Acc@3 100.00 ( 99.99)
Epoch: [32][100/412]	Loss 0.04636 (0.08276)	InvT  26.14 ( 26.11)	Acc@1  99.61 ( 98.74)	Acc@3 100.00 ( 99.99)
Epoch: [32][120/412]	Loss 0.1229 (0.08395)	InvT  26.14 ( 26.12)	Acc@1  98.83 ( 98.72)	Acc@3 100.00 ( 99.98)
Epoch: [32][140/412]	Loss 0.1142 (0.08463)	InvT  26.15 ( 26.12)	Acc@1  98.44 ( 98.70)	Acc@3 100.00 ( 99.99)
Epoch: [32][160/412]	Loss 0.05717 (0.084)	InvT  26.16 ( 26.13)	Acc@1  99.61 ( 98.69)	Acc@3 100.00 ( 99.99)
Epoch: [32][180/412]	Loss 0.09703 (0.08528)	InvT  26.17 ( 26.13)	Acc@1  98.83 ( 98.66)	Acc@3 100.00 ( 99.98)
Epoch: [32][200/412]	Loss 0.08738 (0.08529)	InvT  26.18 ( 26.14)	Acc@1  98.83 ( 98.66)	Acc@3 100.00 ( 99.98)
Epoch: [32][220/412]	Loss 0.051 (0.08536)	InvT  26.19 ( 26.14)	Acc@1  99.61 ( 98.66)	Acc@3 100.00 ( 99.98)
Epoch: [32][240/412]	Loss 0.06695 (0.08565)	InvT  26.20 ( 26.14)	Acc@1  98.83 ( 98.66)	Acc@3 100.00 ( 99.98)
Epoch: [32][260/412]	Loss 0.08942 (0.08564)	InvT  26.21 ( 26.15)	Acc@1  99.22 ( 98.66)	Acc@3 100.00 ( 99.98)
Epoch: [32][280/412]	Loss 0.07481 (0.08569)	InvT  26.22 ( 26.15)	Acc@1  98.83 ( 98.66)	Acc@3 100.00 ( 99.98)
Epoch: [32][300/412]	Loss 0.08814 (0.08515)	InvT  26.23 ( 26.16)	Acc@1  99.22 ( 98.68)	Acc@3 100.00 ( 99.98)
Epoch: [32][320/412]	Loss 0.08695 (0.08502)	InvT  26.23 ( 26.16)	Acc@1  99.22 ( 98.68)	Acc@3 100.00 ( 99.98)
Epoch: [32][340/412]	Loss 0.08139 (0.08511)	InvT  26.24 ( 26.17)	Acc@1  98.83 ( 98.68)	Acc@3 100.00 ( 99.98)
Epoch: [32][360/412]	Loss 0.09801 (0.08622)	InvT  26.25 ( 26.17)	Acc@1  97.66 ( 98.66)	Acc@3 100.00 ( 99.98)
Epoch: [32][380/412]	Loss 0.07022 (0.08663)	InvT  26.26 ( 26.18)	Acc@1  98.83 ( 98.65)	Acc@3 100.00 ( 99.98)
Epoch: [32][400/412]	Loss 0.07551 (0.08655)	InvT  26.27 ( 26.18)	Acc@1  99.61 ( 98.65)	Acc@3 100.00 ( 99.98)
Learning rate: 2.0317468081984054e-05
Epoch 32, valid metric: {"Acc@1": 37.1, "Acc@3": 52.3, "loss": 3.735}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch32.mdl
Epoch: [33][  0/412]	Loss 0.06286 (0.06286)	InvT  26.28 ( 26.28)	Acc@1  98.83 ( 98.83)	Acc@3 100.00 (100.00)
Epoch: [33][ 20/412]	Loss 0.04396 (0.07949)	InvT  26.29 ( 26.28)	Acc@1  99.22 ( 98.83)	Acc@3 100.00 ( 99.96)
Epoch: [33][ 40/412]	Loss 0.0604 (0.0741)	InvT  26.29 ( 26.29)	Acc@1  99.61 ( 98.84)	Acc@3 100.00 ( 99.97)
Epoch: [33][ 60/412]	Loss 0.07031 (0.0765)	InvT  26.30 ( 26.29)	Acc@1  98.83 ( 98.85)	Acc@3 100.00 ( 99.98)
Epoch: [33][ 80/412]	Loss 0.106 (0.07839)	InvT  26.31 ( 26.29)	Acc@1  98.05 ( 98.79)	Acc@3 100.00 ( 99.98)
Epoch: [33][100/412]	Loss 0.07328 (0.0786)	InvT  26.32 ( 26.30)	Acc@1  99.22 ( 98.82)	Acc@3 100.00 ( 99.98)
Epoch: [33][120/412]	Loss 0.06964 (0.07779)	InvT  26.33 ( 26.30)	Acc@1  99.61 ( 98.82)	Acc@3 100.00 ( 99.98)
Epoch: [33][140/412]	Loss 0.1242 (0.07909)	InvT  26.34 ( 26.31)	Acc@1  97.66 ( 98.80)	Acc@3 100.00 ( 99.98)
Epoch: [33][160/412]	Loss 0.06128 (0.07904)	InvT  26.35 ( 26.31)	Acc@1  98.83 ( 98.78)	Acc@3 100.00 ( 99.98)
Epoch: [33][180/412]	Loss 0.08818 (0.0793)	InvT  26.36 ( 26.32)	Acc@1  98.05 ( 98.77)	Acc@3 100.00 ( 99.98)
Epoch: [33][200/412]	Loss 0.05393 (0.08001)	InvT  26.36 ( 26.32)	Acc@1  99.61 ( 98.75)	Acc@3 100.00 ( 99.98)
Epoch: [33][220/412]	Loss 0.06571 (0.07975)	InvT  26.37 ( 26.32)	Acc@1  99.61 ( 98.76)	Acc@3 100.00 ( 99.98)
Epoch: [33][240/412]	Loss 0.0776 (0.0791)	InvT  26.38 ( 26.33)	Acc@1  98.83 ( 98.78)	Acc@3 100.00 ( 99.98)
Epoch: [33][260/412]	Loss 0.1187 (0.07979)	InvT  26.39 ( 26.33)	Acc@1  98.83 ( 98.76)	Acc@3 100.00 ( 99.98)
Epoch: [33][280/412]	Loss 0.08981 (0.07941)	InvT  26.40 ( 26.34)	Acc@1  98.83 ( 98.76)	Acc@3 100.00 ( 99.98)
Epoch: [33][300/412]	Loss 0.04274 (0.07926)	InvT  26.41 ( 26.34)	Acc@1  99.61 ( 98.75)	Acc@3 100.00 ( 99.98)
Epoch: [33][320/412]	Loss 0.04482 (0.07962)	InvT  26.42 ( 26.35)	Acc@1  99.22 ( 98.74)	Acc@3 100.00 ( 99.98)
Epoch: [33][340/412]	Loss 0.1153 (0.07969)	InvT  26.43 ( 26.35)	Acc@1  98.83 ( 98.73)	Acc@3  99.61 ( 99.98)
Epoch: [33][360/412]	Loss 0.08302 (0.08039)	InvT  26.44 ( 26.36)	Acc@1  99.22 ( 98.72)	Acc@3 100.00 ( 99.98)
Epoch: [33][380/412]	Loss 0.09398 (0.08029)	InvT  26.44 ( 26.36)	Acc@1  99.22 ( 98.72)	Acc@3  99.61 ( 99.98)
Epoch: [33][400/412]	Loss 0.05913 (0.08025)	InvT  26.45 ( 26.36)	Acc@1  99.22 ( 98.72)	Acc@3 100.00 ( 99.98)
Learning rate: 2.0015164114855942e-05
Epoch 33, valid metric: {"Acc@1": 37.9, "Acc@3": 51.9, "loss": 3.826}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch33.mdl
Epoch: [34][  0/412]	Loss 0.03087 (0.03087)	InvT  26.46 ( 26.46)	Acc@1 100.00 (100.00)	Acc@3 100.00 (100.00)
Epoch: [34][ 20/412]	Loss 0.0653 (0.07011)	InvT  26.47 ( 26.46)	Acc@1  99.22 ( 98.96)	Acc@3 100.00 (100.00)
Epoch: [34][ 40/412]	Loss 0.09998 (0.07281)	InvT  26.48 ( 26.47)	Acc@1  97.66 ( 98.80)	Acc@3 100.00 ( 99.99)
Epoch: [34][ 60/412]	Loss 0.06815 (0.07186)	InvT  26.49 ( 26.47)	Acc@1  99.22 ( 98.82)	Acc@3 100.00 ( 99.99)
Epoch: [34][ 80/412]	Loss 0.07484 (0.071)	InvT  26.49 ( 26.48)	Acc@1  98.83 ( 98.84)	Acc@3 100.00 (100.00)
Epoch: [34][100/412]	Loss 0.04712 (0.07284)	InvT  26.50 ( 26.48)	Acc@1 100.00 ( 98.81)	Acc@3 100.00 (100.00)
Epoch: [34][120/412]	Loss 0.06277 (0.07576)	InvT  26.51 ( 26.49)	Acc@1  98.83 ( 98.77)	Acc@3 100.00 ( 99.99)
Epoch: [34][140/412]	Loss 0.06991 (0.07707)	InvT  26.52 ( 26.49)	Acc@1  99.22 ( 98.72)	Acc@3 100.00 ( 99.99)
Epoch: [34][160/412]	Loss 0.05836 (0.07703)	InvT  26.53 ( 26.49)	Acc@1  99.61 ( 98.73)	Acc@3 100.00 ( 99.99)
Epoch: [34][180/412]	Loss 0.04367 (0.07758)	InvT  26.54 ( 26.50)	Acc@1  99.61 ( 98.73)	Acc@3 100.00 ( 99.99)
Epoch: [34][200/412]	Loss 0.0941 (0.07824)	InvT  26.55 ( 26.50)	Acc@1  98.44 ( 98.73)	Acc@3 100.00 ( 99.99)
Epoch: [34][220/412]	Loss 0.1093 (0.07773)	InvT  26.56 ( 26.51)	Acc@1  98.05 ( 98.75)	Acc@3 100.00 ( 99.99)
Epoch: [34][240/412]	Loss 0.05385 (0.07799)	InvT  26.56 ( 26.51)	Acc@1  99.22 ( 98.77)	Acc@3 100.00 ( 99.99)
Epoch: [34][260/412]	Loss 0.05267 (0.07845)	InvT  26.57 ( 26.52)	Acc@1  98.83 ( 98.77)	Acc@3 100.00 ( 99.99)
Epoch: [34][280/412]	Loss 0.06055 (0.07909)	InvT  26.58 ( 26.52)	Acc@1  99.22 ( 98.75)	Acc@3 100.00 ( 99.99)
Epoch: [34][300/412]	Loss 0.07026 (0.07873)	InvT  26.59 ( 26.52)	Acc@1 100.00 ( 98.77)	Acc@3 100.00 ( 99.99)
Epoch: [34][320/412]	Loss 0.09425 (0.07845)	InvT  26.60 ( 26.53)	Acc@1  97.66 ( 98.77)	Acc@3 100.00 ( 99.99)
Epoch: [34][340/412]	Loss 0.07944 (0.07852)	InvT  26.61 ( 26.53)	Acc@1  98.44 ( 98.77)	Acc@3 100.00 ( 99.99)
Epoch: [34][360/412]	Loss 0.07471 (0.079)	InvT  26.62 ( 26.54)	Acc@1  99.22 ( 98.76)	Acc@3 100.00 ( 99.99)
Epoch: [34][380/412]	Loss 0.06323 (0.07928)	InvT  26.63 ( 26.54)	Acc@1  98.44 ( 98.75)	Acc@3 100.00 ( 99.99)
Epoch: [34][400/412]	Loss 0.08671 (0.07939)	InvT  26.64 ( 26.55)	Acc@1  99.22 ( 98.75)	Acc@3 100.00 ( 99.99)
Learning rate: 1.9712860147727828e-05
Epoch 34, valid metric: {"Acc@1": 37.6, "Acc@3": 51.6, "loss": 3.823}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch34.mdl
Epoch: [35][  0/412]	Loss 0.1231 (0.1231)	InvT  26.64 ( 26.64)	Acc@1  97.66 ( 97.66)	Acc@3 100.00 (100.00)
Epoch: [35][ 20/412]	Loss 0.07097 (0.07244)	InvT  26.65 ( 26.65)	Acc@1  97.66 ( 98.77)	Acc@3 100.00 ( 99.98)
Epoch: [35][ 40/412]	Loss 0.159 (0.07146)	InvT  26.66 ( 26.65)	Acc@1  96.88 ( 98.88)	Acc@3 100.00 ( 99.99)
Epoch: [35][ 60/412]	Loss 0.0648 (0.07094)	InvT  26.67 ( 26.66)	Acc@1  99.22 ( 98.90)	Acc@3 100.00 ( 99.99)
Epoch: [35][ 80/412]	Loss 0.08742 (0.07138)	InvT  26.68 ( 26.66)	Acc@1  99.22 ( 98.90)	Acc@3 100.00 ( 99.99)
Epoch: [35][100/412]	Loss 0.08658 (0.07091)	InvT  26.69 ( 26.67)	Acc@1  98.83 ( 98.95)	Acc@3  99.61 ( 99.99)
Epoch: [35][120/412]	Loss 0.05554 (0.0711)	InvT  26.70 ( 26.67)	Acc@1  98.83 ( 98.95)	Acc@3 100.00 ( 99.99)
Epoch: [35][140/412]	Loss 0.09815 (0.07212)	InvT  26.70 ( 26.67)	Acc@1  98.83 ( 98.94)	Acc@3 100.00 ( 99.99)
Epoch: [35][160/412]	Loss 0.1162 (0.07282)	InvT  26.71 ( 26.68)	Acc@1  97.27 ( 98.92)	Acc@3 100.00 ( 99.99)
Epoch: [35][180/412]	Loss 0.06371 (0.0736)	InvT  26.72 ( 26.68)	Acc@1  99.61 ( 98.88)	Acc@3 100.00 ( 99.99)
Epoch: [35][200/412]	Loss 0.09268 (0.07372)	InvT  26.73 ( 26.69)	Acc@1  98.44 ( 98.88)	Acc@3 100.00 ( 99.99)
Epoch: [35][220/412]	Loss 0.08109 (0.07361)	InvT  26.74 ( 26.69)	Acc@1  98.44 ( 98.89)	Acc@3 100.00 ( 99.99)
Epoch: [35][240/412]	Loss 0.1016 (0.07397)	InvT  26.75 ( 26.70)	Acc@1  98.44 ( 98.89)	Acc@3  99.61 ( 99.99)
Epoch: [35][260/412]	Loss 0.0974 (0.07452)	InvT  26.76 ( 26.70)	Acc@1  98.44 ( 98.88)	Acc@3 100.00 ( 99.99)
Epoch: [35][280/412]	Loss 0.06262 (0.07434)	InvT  26.77 ( 26.70)	Acc@1  99.61 ( 98.89)	Acc@3 100.00 ( 99.99)
Epoch: [35][300/412]	Loss 0.08617 (0.07425)	InvT  26.77 ( 26.71)	Acc@1  98.05 ( 98.89)	Acc@3 100.00 ( 99.99)
Epoch: [35][320/412]	Loss 0.07236 (0.07408)	InvT  26.78 ( 26.71)	Acc@1  98.44 ( 98.89)	Acc@3  99.61 ( 99.99)
Epoch: [35][340/412]	Loss 0.05833 (0.07388)	InvT  26.79 ( 26.72)	Acc@1  99.22 ( 98.89)	Acc@3 100.00 ( 99.99)
Epoch: [35][360/412]	Loss 0.07859 (0.07421)	InvT  26.80 ( 26.72)	Acc@1  98.44 ( 98.89)	Acc@3 100.00 ( 99.99)
Epoch: [35][380/412]	Loss 0.06485 (0.07441)	InvT  26.81 ( 26.73)	Acc@1  99.22 ( 98.88)	Acc@3 100.00 ( 99.99)
Epoch: [35][400/412]	Loss 0.05878 (0.07445)	InvT  26.82 ( 26.73)	Acc@1  99.61 ( 98.89)	Acc@3 100.00 ( 99.99)
Learning rate: 1.9410556180599716e-05
Epoch 35, valid metric: {"Acc@1": 37.0, "Acc@3": 51.3, "loss": 3.761}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch35.mdl
Epoch: [36][  0/412]	Loss 0.05412 (0.05412)	InvT  26.82 ( 26.82)	Acc@1  98.05 ( 98.05)	Acc@3 100.00 (100.00)
Epoch: [36][ 20/412]	Loss 0.1022 (0.06758)	InvT  26.83 ( 26.83)	Acc@1  98.83 ( 99.11)	Acc@3 100.00 ( 99.98)
Epoch: [36][ 40/412]	Loss 0.08495 (0.06674)	InvT  26.84 ( 26.83)	Acc@1  98.44 ( 99.14)	Acc@3 100.00 ( 99.98)
Epoch: [36][ 60/412]	Loss 0.07626 (0.06611)	InvT  26.85 ( 26.84)	Acc@1  99.22 ( 99.07)	Acc@3 100.00 ( 99.99)
Epoch: [36][ 80/412]	Loss 0.0743 (0.06713)	InvT  26.86 ( 26.84)	Acc@1  98.83 ( 99.06)	Acc@3 100.00 ( 99.98)
Epoch: [36][100/412]	Loss 0.06321 (0.0681)	InvT  26.87 ( 26.85)	Acc@1  98.83 ( 98.99)	Acc@3 100.00 ( 99.97)
Epoch: [36][120/412]	Loss 0.08754 (0.06899)	InvT  26.88 ( 26.85)	Acc@1  98.05 ( 98.97)	Acc@3 100.00 ( 99.98)
Epoch: [36][140/412]	Loss 0.08923 (0.06956)	InvT  26.89 ( 26.86)	Acc@1  98.44 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [36][160/412]	Loss 0.04175 (0.06989)	InvT  26.89 ( 26.86)	Acc@1  99.61 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [36][180/412]	Loss 0.07064 (0.07022)	InvT  26.90 ( 26.86)	Acc@1  99.61 ( 98.92)	Acc@3 100.00 ( 99.98)
Epoch: [36][200/412]	Loss 0.03979 (0.06982)	InvT  26.91 ( 26.87)	Acc@1  99.61 ( 98.92)	Acc@3 100.00 ( 99.98)
Epoch: [36][220/412]	Loss 0.04852 (0.06924)	InvT  26.92 ( 26.87)	Acc@1  98.83 ( 98.92)	Acc@3 100.00 ( 99.98)
Epoch: [36][240/412]	Loss 0.08138 (0.06994)	InvT  26.93 ( 26.88)	Acc@1  99.22 ( 98.91)	Acc@3  99.61 ( 99.98)
Epoch: [36][260/412]	Loss 0.06503 (0.06907)	InvT  26.94 ( 26.88)	Acc@1  99.61 ( 98.93)	Acc@3 100.00 ( 99.98)
Epoch: [36][280/412]	Loss 0.06853 (0.0695)	InvT  26.95 ( 26.89)	Acc@1  99.22 ( 98.93)	Acc@3 100.00 ( 99.98)
Epoch: [36][300/412]	Loss 0.05636 (0.06988)	InvT  26.96 ( 26.89)	Acc@1  99.22 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [36][320/412]	Loss 0.09999 (0.07023)	InvT  26.96 ( 26.89)	Acc@1  98.05 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [36][340/412]	Loss 0.09717 (0.0706)	InvT  26.97 ( 26.90)	Acc@1  97.66 ( 98.93)	Acc@3 100.00 ( 99.99)
Epoch: [36][360/412]	Loss 0.09758 (0.07129)	InvT  26.98 ( 26.90)	Acc@1  98.83 ( 98.91)	Acc@3 100.00 ( 99.99)
Epoch: [36][380/412]	Loss 0.04143 (0.07164)	InvT  26.99 ( 26.91)	Acc@1  99.22 ( 98.90)	Acc@3 100.00 ( 99.99)
Epoch: [36][400/412]	Loss 0.07469 (0.07171)	InvT  27.00 ( 26.91)	Acc@1  99.22 ( 98.90)	Acc@3 100.00 ( 99.99)
Learning rate: 1.9108252213471605e-05
Epoch 36, valid metric: {"Acc@1": 37.4, "Acc@3": 51.8, "loss": 3.862}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch36.mdl
Epoch: [37][  0/412]	Loss 0.08872 (0.08872)	InvT  27.00 ( 27.00)	Acc@1  98.05 ( 98.05)	Acc@3 100.00 (100.00)
Epoch: [37][ 20/412]	Loss 0.08943 (0.06722)	InvT  27.01 ( 27.01)	Acc@1  98.05 ( 99.05)	Acc@3 100.00 (100.00)
Epoch: [37][ 40/412]	Loss 0.02614 (0.066)	InvT  27.02 ( 27.01)	Acc@1 100.00 ( 98.98)	Acc@3 100.00 ( 99.99)
Epoch: [37][ 60/412]	Loss 0.1039 (0.06617)	InvT  27.03 ( 27.02)	Acc@1  98.44 ( 98.99)	Acc@3  99.61 ( 99.99)
Epoch: [37][ 80/412]	Loss 0.05374 (0.06451)	InvT  27.04 ( 27.02)	Acc@1  99.22 ( 99.06)	Acc@3 100.00 ( 99.99)
Epoch: [37][100/412]	Loss 0.04423 (0.06432)	InvT  27.05 ( 27.03)	Acc@1  99.22 ( 99.02)	Acc@3 100.00 ( 99.98)
Epoch: [37][120/412]	Loss 0.06469 (0.06493)	InvT  27.06 ( 27.03)	Acc@1  98.83 ( 99.02)	Acc@3 100.00 ( 99.98)
Epoch: [37][140/412]	Loss 0.09433 (0.06547)	InvT  27.07 ( 27.04)	Acc@1  97.66 ( 99.00)	Acc@3 100.00 ( 99.98)
Epoch: [37][160/412]	Loss 0.06747 (0.06611)	InvT  27.07 ( 27.04)	Acc@1  99.22 ( 98.98)	Acc@3  99.61 ( 99.98)
Epoch: [37][180/412]	Loss 0.0727 (0.0669)	InvT  27.08 ( 27.04)	Acc@1  98.83 ( 98.96)	Acc@3 100.00 ( 99.98)
Epoch: [37][200/412]	Loss 0.06817 (0.06701)	InvT  27.09 ( 27.05)	Acc@1  99.61 ( 98.97)	Acc@3 100.00 ( 99.98)
Epoch: [37][220/412]	Loss 0.0784 (0.06755)	InvT  27.10 ( 27.05)	Acc@1  99.22 ( 98.96)	Acc@3 100.00 ( 99.99)
Epoch: [37][240/412]	Loss 0.02993 (0.06762)	InvT  27.11 ( 27.06)	Acc@1 100.00 ( 98.96)	Acc@3 100.00 ( 99.99)
Epoch: [37][260/412]	Loss 0.0722 (0.06859)	InvT  27.12 ( 27.06)	Acc@1  99.22 ( 98.94)	Acc@3 100.00 ( 99.99)
Epoch: [37][280/412]	Loss 0.09865 (0.06911)	InvT  27.12 ( 27.06)	Acc@1  98.05 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [37][300/412]	Loss 0.07517 (0.06894)	InvT  27.13 ( 27.07)	Acc@1  98.44 ( 98.94)	Acc@3 100.00 ( 99.99)
Epoch: [37][320/412]	Loss 0.07364 (0.0687)	InvT  27.14 ( 27.07)	Acc@1  98.05 ( 98.94)	Acc@3 100.00 ( 99.99)
Epoch: [37][340/412]	Loss 0.05036 (0.0685)	InvT  27.15 ( 27.08)	Acc@1  99.22 ( 98.95)	Acc@3 100.00 ( 99.99)
Epoch: [37][360/412]	Loss 0.06878 (0.06867)	InvT  27.16 ( 27.08)	Acc@1  98.05 ( 98.94)	Acc@3 100.00 ( 99.98)
Epoch: [37][380/412]	Loss 0.0454 (0.06844)	InvT  27.17 ( 27.09)	Acc@1  99.22 ( 98.94)	Acc@3 100.00 ( 99.99)
Epoch: [37][400/412]	Loss 0.0944 (0.06872)	InvT  27.18 ( 27.09)	Acc@1  98.05 ( 98.94)	Acc@3 100.00 ( 99.98)
Learning rate: 1.880594824634349e-05
Epoch 37, valid metric: {"Acc@1": 38.1, "Acc@3": 53.1, "loss": 3.842}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch37.mdl
Epoch: [38][  0/412]	Loss 0.04696 (0.04696)	InvT  27.18 ( 27.18)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [38][ 20/412]	Loss 0.07142 (0.0594)	InvT  27.19 ( 27.19)	Acc@1  99.22 ( 99.18)	Acc@3 100.00 (100.00)
Epoch: [38][ 40/412]	Loss 0.05138 (0.06097)	InvT  27.20 ( 27.19)	Acc@1  99.22 ( 99.19)	Acc@3 100.00 (100.00)
Epoch: [38][ 60/412]	Loss 0.1068 (0.0634)	InvT  27.21 ( 27.20)	Acc@1  98.05 ( 99.10)	Acc@3 100.00 (100.00)
Epoch: [38][ 80/412]	Loss 0.0421 (0.06629)	InvT  27.22 ( 27.20)	Acc@1  99.22 ( 99.00)	Acc@3 100.00 (100.00)
Epoch: [38][100/412]	Loss 0.04384 (0.06595)	InvT  27.22 ( 27.20)	Acc@1  98.83 ( 98.99)	Acc@3 100.00 (100.00)
Epoch: [38][120/412]	Loss 0.05578 (0.06496)	InvT  27.23 ( 27.21)	Acc@1  98.83 ( 99.00)	Acc@3 100.00 (100.00)
Epoch: [38][140/412]	Loss 0.0734 (0.06405)	InvT  27.24 ( 27.21)	Acc@1  98.44 ( 99.01)	Acc@3 100.00 (100.00)
Epoch: [38][160/412]	Loss 0.04684 (0.06441)	InvT  27.25 ( 27.22)	Acc@1  99.22 ( 99.01)	Acc@3 100.00 (100.00)
Epoch: [38][180/412]	Loss 0.04286 (0.06423)	InvT  27.26 ( 27.22)	Acc@1  98.83 ( 99.01)	Acc@3 100.00 (100.00)
Epoch: [38][200/412]	Loss 0.04268 (0.06398)	InvT  27.27 ( 27.22)	Acc@1  99.61 ( 99.02)	Acc@3 100.00 (100.00)
Epoch: [38][220/412]	Loss 0.09046 (0.06399)	InvT  27.28 ( 27.23)	Acc@1  98.44 ( 99.04)	Acc@3 100.00 (100.00)
Epoch: [38][240/412]	Loss 0.08457 (0.06422)	InvT  27.29 ( 27.23)	Acc@1  99.22 ( 99.05)	Acc@3 100.00 ( 99.99)
Epoch: [38][260/412]	Loss 0.05313 (0.06395)	InvT  27.29 ( 27.24)	Acc@1  98.05 ( 99.04)	Acc@3 100.00 ( 99.99)
Epoch: [38][280/412]	Loss 0.09129 (0.0638)	InvT  27.30 ( 27.24)	Acc@1  98.83 ( 99.05)	Acc@3 100.00 ( 99.99)
Epoch: [38][300/412]	Loss 0.0663 (0.06365)	InvT  27.31 ( 27.25)	Acc@1  99.61 ( 99.03)	Acc@3 100.00 ( 99.99)
Epoch: [38][320/412]	Loss 0.04365 (0.06445)	InvT  27.32 ( 27.25)	Acc@1  99.61 ( 99.01)	Acc@3 100.00 ( 99.99)
Epoch: [38][340/412]	Loss 0.106 (0.06462)	InvT  27.33 ( 27.26)	Acc@1  98.05 ( 99.00)	Acc@3 100.00 ( 99.99)
Epoch: [38][360/412]	Loss 0.06495 (0.06446)	InvT  27.34 ( 27.26)	Acc@1  99.61 ( 99.00)	Acc@3 100.00 ( 99.99)
Epoch: [38][380/412]	Loss 0.07794 (0.06469)	InvT  27.35 ( 27.26)	Acc@1  99.22 ( 98.99)	Acc@3 100.00 ( 99.99)
Epoch: [38][400/412]	Loss 0.07212 (0.06498)	InvT  27.35 ( 27.27)	Acc@1  98.44 ( 98.99)	Acc@3 100.00 ( 99.99)
Learning rate: 1.850364427921538e-05
Epoch 38, valid metric: {"Acc@1": 38.7, "Acc@3": 51.6, "loss": 3.871}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch38.mdl
Epoch: [39][  0/412]	Loss 0.05244 (0.05244)	InvT  27.36 ( 27.36)	Acc@1  99.22 ( 99.22)	Acc@3 100.00 (100.00)
Epoch: [39][ 20/412]	Loss 0.09009 (0.05815)	InvT  27.37 ( 27.36)	Acc@1  98.05 ( 99.05)	Acc@3 100.00 (100.00)
Epoch: [39][ 40/412]	Loss 0.06302 (0.0602)	InvT  27.38 ( 27.37)	Acc@1  98.05 ( 99.03)	Acc@3 100.00 (100.00)
Epoch: [39][ 60/412]	Loss 0.05418 (0.0614)	InvT  27.38 ( 27.37)	Acc@1  99.61 ( 99.00)	Acc@3 100.00 ( 99.99)
Epoch: [39][ 80/412]	Loss 0.02965 (0.06062)	InvT  27.39 ( 27.38)	Acc@1  99.61 ( 98.99)	Acc@3 100.00 ( 99.99)
Epoch: [39][100/412]	Loss 0.07832 (0.06108)	InvT  27.40 ( 27.38)	Acc@1  98.05 ( 99.01)	Acc@3 100.00 ( 99.99)
Epoch: [39][120/412]	Loss 0.07531 (0.06159)	InvT  27.41 ( 27.38)	Acc@1  98.44 ( 99.02)	Acc@3 100.00 ( 99.99)
Epoch: [39][140/412]	Loss 0.07717 (0.06137)	InvT  27.42 ( 27.39)	Acc@1  97.66 ( 99.02)	Acc@3 100.00 ( 99.99)
Epoch: [39][160/412]	Loss 0.08057 (0.06086)	InvT  27.43 ( 27.39)	Acc@1  98.44 ( 99.04)	Acc@3 100.00 ( 99.99)
Epoch: [39][180/412]	Loss 0.06618 (0.06073)	InvT  27.43 ( 27.40)	Acc@1  98.05 ( 99.04)	Acc@3 100.00 ( 99.99)
Epoch: [39][200/412]	Loss 0.04946 (0.06148)	InvT  27.44 ( 27.40)	Acc@1  99.22 ( 99.04)	Acc@3 100.00 ( 99.99)
Epoch: [39][220/412]	Loss 0.0438 (0.06224)	InvT  27.45 ( 27.40)	Acc@1  99.61 ( 99.03)	Acc@3 100.00 ( 99.99)
Epoch: [39][240/412]	Loss 0.07029 (0.06202)	InvT  27.46 ( 27.41)	Acc@1  98.83 ( 99.03)	Acc@3 100.00 ( 99.99)
Epoch: [39][260/412]	Loss 0.0725 (0.06262)	InvT  27.47 ( 27.41)	Acc@1  98.83 ( 99.02)	Acc@3 100.00 ( 99.99)
Epoch: [39][280/412]	Loss 0.05858 (0.0639)	InvT  27.48 ( 27.42)	Acc@1  99.22 ( 98.99)	Acc@3 100.00 ( 99.99)
Epoch: [39][300/412]	Loss 0.05661 (0.0639)	InvT  27.48 ( 27.42)	Acc@1  99.22 ( 99.00)	Acc@3 100.00 ( 99.99)
Epoch: [39][320/412]	Loss 0.0757 (0.06379)	InvT  27.49 ( 27.43)	Acc@1  98.05 ( 99.01)	Acc@3 100.00 ( 99.99)
Epoch: [39][340/412]	Loss 0.04062 (0.06351)	InvT  27.50 ( 27.43)	Acc@1  99.22 ( 99.02)	Acc@3 100.00 ( 99.99)
Epoch: [39][360/412]	Loss 0.0495 (0.06327)	InvT  27.51 ( 27.43)	Acc@1  99.61 ( 99.03)	Acc@3 100.00 ( 99.99)
Epoch: [39][380/412]	Loss 0.05604 (0.0631)	InvT  27.52 ( 27.44)	Acc@1  99.61 ( 99.04)	Acc@3 100.00 ( 99.99)
Epoch: [39][400/412]	Loss 0.08041 (0.06302)	InvT  27.53 ( 27.44)	Acc@1  98.05 ( 99.04)	Acc@3 100.00 ( 99.99)
Learning rate: 1.8201340312087268e-05
Epoch 39, valid metric: {"Acc@1": 37.9, "Acc@3": 53.0, "loss": 3.909}
Delete old checkpoint ./checkpoint/union+trans_en2hi/checkpoint_epoch39.mdl
Lost patience at 39 epoch. Best epoch at 30
